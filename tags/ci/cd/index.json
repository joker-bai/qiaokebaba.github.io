[{"content":"\r\r爱生活，爱运维\r分享 DevOps、自动化运维等实践经验\r\r\r\r","description":"","id":0,"section":"","tags":null,"title":"友链","uri":"https://www.coolops.cn/friends/"},{"content":"前言 去年12月份，当Kubernetes社区宣布1.20版本之后会逐步弃用dockershim，当时也有很多自媒体在宣传Kubernetes弃用Docker。其实，我觉得这是一种误导，也许仅仅是为了蹭热度。\ndockershim是Kubernetes的一个组件，其作用是为了操作Docker。Docker是在2013年面世的，而Kubernetes是在2016年，所以Docker刚开始并没有想到编排，也不会知道会出现Kubernetes这个庞然大物（它要是知道，也不会败的那么快\u0026hellip;）。但是Kubernetes在创建的时候就是以Docker作为容器运行时，很多操作逻辑都是针对的Docker，随着社区越来越健壮，为了兼容更多的容器运行时，才将Docker的相关逻辑独立出来组成了dockershim。\n正因为这样，只要Kubernetes的任何变动或者Docker的任何变动，都必须维护dockershim，这样才能保证足够的支持，但是通过dockershim操作Docker，其本质还是操作Docker的底层运行时Containerd，而且Containerd自身也是支持CRI（Container Runtime Interface），那为什么还要绕一层Docker呢？是不是可以直接通过CRI和Containerd进行交互？这也是社区希望启动dockershim的原因之一吧。\n再来看看启动dockershim究竟对用户、对维护者有多少影响。\n对上层用户来说，其实并没有影响，因为上层已经屏蔽调了这些细节，只管用就可以了。更多的影响只是针对我们这些YAML工程师，因为我们主要是考虑用哪个容器运行时，如果继续用Docker，以后版本升级有没有影响？如果不用Docker，维护的成本、复杂度、学习成本会不会增加？其实我们是想多了，事情也远没那么复杂，喜欢用docker的依旧可以用docker，想用containerd的就用containerd，改动也不大，后面也有相关的部署文档。而且也只是kubernetes社区不再维护dockershim而已，Mirantis 和 Docker 已经决定之后共同合作维护 dockershim 组件，也就是说dockershim依然可以作为连接docker的桥梁，只是从kubernetes内置携带改成独立的而已。\n那什么是containerd呢？\nContainerd是从Docker中分离的一个项目，旨在为Kubernetes提供容器运行时，负责管理镜像和容器的生命周期。不过Containerd是可以抛开Docker独立工作的。它的特性如下：\n 支持OCI镜像规范，也就是runc 支持OCI运行时规范 支持镜像的pull 支持容器网络管理 存储支持多租户 支持容器运行时和容器的生命周期管理 支持管理网络名称空间  Containerd和Docker在命令使用上的一些区别主要如下：\n   功能 Docker Containerd     显示本地镜像列表 docker images crictl images   下载镜像 docker pull crictl pull   上传镜像 docker push 无   删除本地镜像 docker rmi crictl rmi   查看镜像详情 docker inspect IMAGE-ID crictl inspecti IMAGE-ID   显示容器列表 docker ps crictl ps   创建容器 docker create crictl create   启动容器 docker start crictl start   停止容器 docker stop crictl stop   删除容器 docker rm crictl rm   查看容器详情 docker inspect crictl inspect   attach docker attach crictl attach   exec docker exec crictl exec   logs docker logs crictl logs   stats docker stats crictl stats    可以看到使用方式大同小异。\n下面介绍一下使用kubeadm安装K8S集群，并使用containerd作为容器运行时的具体安装步骤。\n环境说明 主机节点    IP地址 系统 内核     192.168.0.5 CentOS7.6 3.10   192.168.0.125 CentOS7.6 3.10    软件说明    软件 版本     kubernetes 1.20.5   containerd 1.4.4    环境准备 （1）在每个节点上添加 hosts 信息：\n$ cat /etc/hosts\n192.168.0.5 k8s-master\r192.168.0.125 k8s-node01\r（2）禁用防火墙：\n$ systemctl stop firewalld\r$ systemctl disable firewalld\r（3）禁用SELINUX：\n$ setenforce 0\r$ cat /etc/selinux/config\rSELINUX=disabled\r（4）创建/etc/sysctl.d/k8s.conf文件，添加如下内容：\nnet.bridge.bridge-nf-call-ip6tables = 1\rnet.bridge.bridge-nf-call-iptables = 1\rnet.ipv4.ip_forward = 1\r（5）执行如下命令使修改生效：\n$ modprobe br_netfilter\r$ sysctl -p /etc/sysctl.d/k8s.conf\r（6）安装 ipvs\n$ cat \u0026gt; /etc/sysconfig/modules/ipvs.modules \u0026lt;\u0026lt;EOF\r#!/bin/bash\rmodprobe -- ip_vs\rmodprobe -- ip_vs_rr\rmodprobe -- ip_vs_wrr\rmodprobe -- ip_vs_sh\rmodprobe -- nf_conntrack_ipv4\rEOF\r$ chmod 755 /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; bash /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4\r上面脚本创建了的/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。使用lsmod | grep -e ip_vs -e nf_conntrack_ipv4命令查看是否已经正确加载所需的内核模块。\n（7）安装了 ipset 软件包：\n$ yum install ipset -y\r为了便于查看 ipvs 的代理规则，最好安装一下管理工具 ipvsadm：\n$ yum install ipvsadm -y\r（8）同步服务器时间\n$ yum install chrony -y\r$ systemctl enable chronyd\r$ systemctl start chronyd\r$ chronyc sources\r（9）关闭 swap 分区：\n$ swapoff -a\r（10）修改/etc/fstab文件，注释掉 SWAP 的自动挂载，使用free -m确认 swap 已经关闭。swappiness 参数调整，修改/etc/sysctl.d/k8s.conf添加下面一行：\nvm.swappiness=0\r执行sysctl -p /etc/sysctl.d/k8s.conf使修改生效。\n（11）接下来可以安装 Containerd\n$ yum install -y yum-utils \\\rdevice-mapper-persistent-data \\\rlvm2\r$ yum-config-manager \\\r--add-repo \\\rhttps://download.docker.com/linux/centos/docker-ce.repo\r$ yum list | grep containerd\r可以选择安装一个版本，比如我们这里安装最新版本：\n$ yum install containerd.io-1.4.4 -y\r（12）创建containerd配置文件：\nmkdir -p /etc/containerd\rcontainerd config default \u0026gt; /etc/containerd/config.toml\r# 替换配置文件\rsed -i \u0026quot;s#k8s.gcr.io#registry.cn-hangzhou.aliyuncs.com/google_containers#g\u0026quot; /etc/containerd/config.toml\rsed -i '/containerd.runtimes.runc.options/a\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ SystemdCgroup = true' /etc/containerd/config.toml\rsed -i \u0026quot;s#https://registry-1.docker.io#https://registry.cn-hangzhou.aliyuncs.com#g\u0026quot; /etc/containerd/config.toml\r（13）启动Containerd:\nsystemctl daemon-reload\rsystemctl enable containerd\rsystemctl restart containerd\r在确保 Containerd安装完成后，上面的相关环境配置也完成了，现在我们就可以来安装 Kubeadm 了，我们这里是通过指定yum 源的方式来进行安装，使用阿里云的源进行安装：\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo\r[kubernetes]\rname=Kubernetes\rbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\renabled=1\rgpgcheck=0\rrepo_gpgcheck=0\rgpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\rhttp://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\rEOF\r然后安装 kubeadm、kubelet、kubectl（我安装的是最新版，有版本要求自己设定版本）：\n$ yum install -y kubelet-1.20.5 kubeadm-1.20.5 kubectl-1.20.5\r设置运行时：\n$ crictl config runtime-endpoint /run/containerd/containerd.sock\r可以看到我们这里安装的是 v1.20.5版本，然后将 kubelet 设置成开机启动：\n$ systemctl daemon-reload\r$ systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet\r 到这里为止上面所有的操作都需要在所有节点执行配置。\n **\n**\n初始化集群 初始化Master 然后接下来在 master 节点配置 kubeadm 初始化文件，可以通过如下命令导出默认的初始化配置：\n$ kubeadm config print init-defaults \u0026gt; kubeadm.yaml\r然后根据我们自己的需求修改配置，比如修改 imageRepository 的值，kube-proxy 的模式为 ipvs，需要注意的是由于我们使用的containerd作为运行时，所以在初始化节点的时候需要指定cgroupDriver为systemd【1】\napiVersion: kubeadm.k8s.io/v1beta2\rbootstrapTokens:\r- groups:\r- system:bootstrappers:kubeadm:default-node-token\rtoken: abcdef.0123456789abcdef\rttl: 24h0m0s\rusages:\r- signing\r- authentication\rkind: InitConfiguration\rlocalAPIEndpoint:\radvertiseAddress: 192.168.0.5 bindPort: 6443\rnodeRegistration:\rcriSocket: /run/containerd/containerd.sock name: k8s-master\rtaints:\r- effect: NoSchedule\rkey: node-role.kubernetes.io/master\r---\rapiServer:\rtimeoutForControlPlane: 4m0s\rapiVersion: kubeadm.k8s.io/v1beta2\rcertificatesDir: /etc/kubernetes/pki\rclusterName: kubernetes\rcontrollerManager: {}\rdns:\rtype: CoreDNS\retcd:\rlocal:\rdataDir: /var/lib/etcd\rimageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers\rkind: ClusterConfiguration\rkubernetesVersion: v1.20.5\rnetworking:\rdnsDomain: cluster.local\rpodSubnet: 172.16.0.0/16\rserviceSubnet: 10.96.0.0/12\rscheduler: {}\r---\rapiVersion: kubeproxy.config.k8s.io/v1alpha1\rkind: KubeProxyConfiguration\rmode: ipvs\r---\rapiVersion: kubelet.config.k8s.io/v1beta1\rkind: KubeletConfiguration\rcgroupDriver: systemd\r然后使用上面的配置文件进行初始化：\n$ kubeadm init --config=kubeadm.yaml\r[init] Using Kubernetes version: v1.20.5\r[preflight] Running pre-flight checks\r[preflight] Pulling images required for setting up a Kubernetes cluster\r[preflight] This might take a minute or two, depending on the speed of your internet connection\r[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\r[certs] Using certificateDir folder \u0026quot;/etc/kubernetes/pki\u0026quot;\r[certs] Generating \u0026quot;ca\u0026quot; certificate and key\r[certs] Generating \u0026quot;apiserver\u0026quot; certificate and key\r[certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.0.5]\r[certs] Generating \u0026quot;apiserver-kubelet-client\u0026quot; certificate and key\r[certs] Generating \u0026quot;front-proxy-ca\u0026quot; certificate and key\r[certs] Generating \u0026quot;front-proxy-client\u0026quot; certificate and key\r[certs] Generating \u0026quot;etcd/ca\u0026quot; certificate and key\r[certs] Generating \u0026quot;etcd/server\u0026quot; certificate and key\r[certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.0.5 127.0.0.1 ::1]\r[certs] Generating \u0026quot;etcd/peer\u0026quot; certificate and key\r[certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.0.5 127.0.0.1 ::1]\r[certs] Generating \u0026quot;etcd/healthcheck-client\u0026quot; certificate and key\r[certs] Generating \u0026quot;apiserver-etcd-client\u0026quot; certificate and key\r[certs] Generating \u0026quot;sa\u0026quot; key and public key\r[kubeconfig] Using kubeconfig folder \u0026quot;/etc/kubernetes\u0026quot;\r[kubeconfig] Writing \u0026quot;admin.conf\u0026quot; kubeconfig file\r[kubeconfig] Writing \u0026quot;kubelet.conf\u0026quot; kubeconfig file\r[kubeconfig] Writing \u0026quot;controller-manager.conf\u0026quot; kubeconfig file\r[kubeconfig] Writing \u0026quot;scheduler.conf\u0026quot; kubeconfig file\r[kubelet-start] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot;\r[kubelet-start] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot;\r[kubelet-start] Starting the kubelet\r[control-plane] Using manifest folder \u0026quot;/etc/kubernetes/manifests\u0026quot;\r[control-plane] Creating static Pod manifest for \u0026quot;kube-apiserver\u0026quot;\r[control-plane] Creating static Pod manifest for \u0026quot;kube-controller-manager\u0026quot;\r[control-plane] Creating static Pod manifest for \u0026quot;kube-scheduler\u0026quot;\r[etcd] Creating static Pod manifest for local etcd in \u0026quot;/etc/kubernetes/manifests\u0026quot;\r[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026quot;/etc/kubernetes/manifests\u0026quot;. This can take up to 4m0s\r[kubelet-check] Initial timeout of 40s passed.\r[apiclient] All control plane components are healthy after 70.001862 seconds\r[upload-config] Storing the configuration used in ConfigMap \u0026quot;kubeadm-config\u0026quot; in the \u0026quot;kube-system\u0026quot; Namespace\r[kubelet] Creating a ConfigMap \u0026quot;kubelet-config-1.20\u0026quot; in namespace kube-system with the configuration for the kubelets in the cluster\r[upload-certs] Skipping phase. Please see --upload-certs\r[mark-control-plane] Marking the node k8s-master as control-plane by adding the labels \u0026quot;node-role.kubernetes.io/master=''\u0026quot; and \u0026quot;node-role.kubernetes.io/control-plane='' (deprecated)\u0026quot;\r[mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]\r[bootstrap-token] Using token: abcdef.0123456789abcdef\r[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\r[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes\r[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\r[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\r[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\r[bootstrap-token] Creating the \u0026quot;cluster-info\u0026quot; ConfigMap in the \u0026quot;kube-public\u0026quot; namespace\r[kubelet-finalize] Updating \u0026quot;/etc/kubernetes/kubelet.conf\u0026quot; to point to a rotatable kubelet client certificate and key\r[addons] Applied essential addon: CoreDNS\r[addons] Applied essential addon: kube-proxy\rYour Kubernetes control-plane has initialized successfully!\rTo start using your cluster, you need to run the following as a regular user:\rmkdir -p $HOME/.kube\rsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\rsudo chown $(id -u):$(id -g) $HOME/.kube/config\rAlternatively, if you are the root user, you can run:\rexport KUBECONFIG=/etc/kubernetes/admin.conf\rYou should now deploy a pod network to the cluster.\rRun \u0026quot;kubectl apply -f [podnetwork].yaml\u0026quot; with one of the options listed at:\rhttps://kubernetes.io/docs/concepts/cluster-administration/addons/\rThen you can join any number of worker nodes by running the following on each as root:\rkubeadm join 192.168.0.5:6443 --token abcdef.0123456789abcdef \\\r--discovery-token-ca-cert-hash sha256:446623b965cdb0289c687e74af53f9e9c2063e854a42ee36be9aa249d3f0ccec\r拷贝 kubeconfig 文件\n$ mkdir -p $HOME/.kube\r$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\r$ sudo chown $(id -u):$(id -g) $HOME/.kube/config\r**\n**\n添加节点 记住初始化集群上面的配置和操作要提前做好，将 master 节点上面的 $HOME/.kube/config 文件拷贝到 node 节点对应的文件中，安装 kubeadm、kubelet、kubectl，然后执行上面初始化完成后提示的 join 命令即可：\n# kubeadm join 192.168.0.5:6443 --token abcdef.0123456789abcdef \\\r\u0026gt; --discovery-token-ca-cert-hash sha256:446623b965cdb0289c687e74af53f9e9c2063e854a42ee36be9aa249d3f0ccec [preflight] Running pre-flight checks\r[preflight] Reading configuration from the cluster...\r[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\r[kubelet-start] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot;\r[kubelet-start] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot;\r[kubelet-start] Starting the kubelet\r[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...\rThis node has joined the cluster:\r* Certificate signing request was sent to apiserver and a response was received.\r* The Kubelet was informed of the new secure connection details.\rRun 'kubectl get nodes' on the control-plane to see this node join the cluster.\r 如果忘记了上面的 join 命令可以使用命令kubeadm token create \u0026ndash;print-join-command重新获取。\n 执行成功后运行 get nodes 命令：\n$ kubectl get no\rNAME STATUS ROLES AGE VERSION\rk8s-master NotReady control-plane,master 29m v1.20.5\rk8s-node01 NotReady \u0026lt;none\u0026gt; 28m v1.20.5\r可以看到是 NotReady 状态，这是因为还没有安装网络插件，接下来安装网络插件，可以在文档 https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/ 中选择我们自己的网络插件，这里我们安装 calio:\n$ wget https://docs.projectcalico.org/v3.8/manifests/calico.yaml\r# 因为有节点是多网卡，所以需要在资源清单文件中指定内网网卡\n$ vi calico.yaml\n......\rspec:\rcontainers:\r- env:\r- name: DATASTORE_TYPE\rvalue: kubernetes\r- name: IP_AUTODETECTION_METHOD # DaemonSet中添加该环境变量\rvalue: interface=eth0 # 指定内网网卡\r- name: WAIT_FOR_DATASTORE\rvalue: \u0026quot;true\u0026quot;\r- name: CALICO_IPV4POOL_CIDR # 由于在init的时候配置的172网段，所以这里需要修改\rvalue: \u0026quot;172.16.0.0/16\u0026quot;\r......\r安装calico网络插件\n$ kubectl apply -f calico.yaml\r隔一会儿查看 Pod 运行状态：\n# kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE\rcalico-kube-controllers-bcc6f659f-zmw8n 0/1 ContainerCreating 0 7m58s\rcalico-node-c4vv7 1/1 Running 0 7m58s\rcalico-node-dtw7g 0/1 PodInitializing 0 7m58s\rcoredns-54d67798b7-mrj2b 1/1 Running 0 46m\rcoredns-54d67798b7-p667d 1/1 Running 0 46m\retcd-k8s-master 1/1 Running 0 46m\rkube-apiserver-k8s-master 1/1 Running 0 46m\rkube-controller-manager-k8s-master 1/1 Running 0 46m\rkube-proxy-clf4s 1/1 Running 0 45m\rkube-proxy-mt7tt 1/1 Running 0 46m\rkube-scheduler-k8s-master 1/1 Running 0 46m\r网络插件运行成功了，node 状态也正常了：\n# kubectl get nodes NAME STATUS ROLES AGE VERSION\rk8s-master Ready control-plane,master 47m v1.20.5\rk8s-node01 Ready \u0026lt;none\u0026gt; 46m v1.20.5\r用同样的方法添加另外一个节点即可。**\n**\n配置命令自动补全\nyum install -y bash-completion\rsource /usr/share/bash-completion/bash_completion\rsource \u0026lt;(kubectl completion bash)\recho \u0026quot;source \u0026lt;(kubectl completion bash)\u0026quot; \u0026gt;\u0026gt; ~/.bashrc\r参考文档 【1】：https://github.com/containerd/containerd/issues/4857\n【2】：https://github.com/containerd/containerd\n","description":"本文主要介绍在使用containerd做容器运行时的时候如何使用Kubeadm安装集群","id":1,"section":"posts","tags":["kubernetes","kubeadm","containerd"],"title":"kubeadm部署K8S集群并使用containerd做容器运行时","uri":"https://www.coolops.cn/posts/kubeadm-containerd-k8s/"},{"content":"什么是Argo Workflows？ Argo Workflows是一个开源项目，为Kubernetes提供container-native工作流程，其主要通过Kubernetes CRD实现的。\n特点如下：\n 工作流的每一步都是一个容器 将多步骤工作流建模为一系列任务，或者使用有向无环图（DAG）描述任务之间的依赖关系 可以在短时间内轻松运行用于机器学习或数据处理的计算密集型作业 在Kubernetes上运行CI/CD Pipeline，无需复杂的软件配置  安装 安装控制器端 Argo Wordflows的安装非常简单，直接使用以下命令安装即可。\nkubectl create ns argo\rkubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo-workflows/stable/manifests/quick-start-postgres.yaml\r安装完成后，会生成以下4个pod。\n# kubectl get po -n argo\rNAME READY STATUS RESTARTS AGE\rargo-server-574ddc66b-62rjc 1/1 Running 4 4h25m\rminio 1/1 Running 0 4h25m\rpostgres-56fd897cf4-k8fwd 1/1 Running 0 4h25m\rworkflow-controller-77658c77cc-p25ll 1/1 Running 4 4h25m\r其中：\n argo-server是argo服务端 mino是进行制品仓库 postgres是数据库 workflow-controller是流程控制器  然后配置一个server端的ingress，即可访问UI，配置清单如下（我这里使用的是traefik）：\napiVersion: traefik.containo.us/v1alpha1\rkind: IngressRoute\rmetadata:\rname: argo-ui\rnamespace: argo\rspec:\rentryPoints:\r- web\rroutes:\r- match: Host(`argowork-test.coolops.cn`)\rkind: Rule\rservices:\r- name: argo-server\rport: 2746\rUI界面如下：\n再配置一个minio的ingress，配置清单如下：\napiVersion: traefik.containo.us/v1alpha1\rkind: IngressRoute\rmetadata:\rname: minio\rnamespace: argo\rspec:\rentryPoints:\r- web\rroutes:\r- match: Host(`minio-test.coolops.cn`)\rkind: Rule\rservices:\r- name: minio\rport: 9000\rUI界面如下（默认用户名密码是：admin:password）：\n安装Client端 Argo Workflows提供Argo CLI，其安装方式也非常简单，如下：\nLinux系统：\n# Download the binary\rcurl -sLO https://github.com/argoproj/argo/releases/download/v3.0.0-rc4/argo-linux-amd64.gz\r# Unzip\rgunzip argo-linux-amd64.gz\r# Make binary executable\rchmod +x argo-linux-amd64\r# Move binary to path\rmv ./argo-linux-amd64 /usr/local/bin/argo\r安装完成后，使用以下命令校验是否安装成功。\n# argo version\rargo: v3.0.0-rc4\rBuildDate: 2021-03-02T21:42:55Z\rGitCommit: ae5587e97dad0e4806f7a230672b998fe140a767\rGitTreeState: clean\rGitTag: v3.0.0-rc4\rGoVersion: go1.13\rCompiler: gc\rPlatform: linux/amd64\r其主要的命令有：\nlist 列出工作流\rlogs 查看工作流的日志\rsubmit 创建工作流\rwatch 实时监听工作流\rget 现实详细信息\rdelete 删除工作流\rstop 停止工作流\r更多命令可以使用argo --help进行查看。\n然后可以使用一个简单的hello world的WorkFlow，如下：\napiVersion: argoproj.io/v1alpha1\rkind: Workflow\rmetadata:\rgenerateName: hello-world-\rlabels:\rworkflows.argoproj.io/archive-strategy: \u0026quot;false\u0026quot;\rspec:\rentrypoint: whalesay\rtemplates:\r- name: whalesay\rcontainer:\rimage: docker/whalesay:latest\rcommand: [cowsay]\rargs: [\u0026quot;hello world\u0026quot;]\r使用如下命令创建并观察workflow。\n$ argo submit -n argo helloworld.yaml --watch\r然后可以看到以下输出。\nName: hello-world-9pw7v\rNamespace: argo\rServiceAccount: default\rStatus: Succeeded\rConditions: Completed True\rCreated: Mon Mar 08 14:51:35 +0800 (10 seconds ago)\rStarted: Mon Mar 08 14:51:35 +0800 (10 seconds ago)\rFinished: Mon Mar 08 14:51:45 +0800 (now)\rDuration: 10 seconds\rProgress: 1/1\rResourcesDuration: 4s*(1 cpu),4s*(100Mi memory)\rSTEP TEMPLATE PODNAME DURATION MESSAGE\r✔ hello-world-9pw7v whalesay hello-world-9pw7v 5s\r还可以通过argo list来查看状态，如下：\n# argo list -n argo\rNAME STATUS AGE DURATION PRIORITY\rhello-world-9pw7v Succeeded 1m 10s 0\r使用argo logs来查看具体的日志，如下：\n# argo logs -n argo hello-world-9pw7v\rhello-world-9pw7v: _____________ hello-world-9pw7v: \u0026lt; hello world \u0026gt;\rhello-world-9pw7v: ------------- hello-world-9pw7v: \\\rhello-world-9pw7v: \\\rhello-world-9pw7v: \\ hello-world-9pw7v: ## . hello-world-9pw7v: ## ## ## == hello-world-9pw7v: ## ## ## ## === hello-world-9pw7v: /\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;___/ === hello-world-9pw7v: ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ hello-world-9pw7v: \\______ o __/ hello-world-9pw7v: \\ \\ __/ hello-world-9pw7v: \\____\\______/ 核心概念 Workflow Workflow是Argo中最重要的资源，其主要有两个重要功能：\n 它定义要执行的工作流 它存储工作流程的状态  要执行的工作流定义在Workflow.spec字段中，其主要包括templates和entrypoint，如下：\napiVersion: argoproj.io/v1alpha1\rkind: Workflow\rmetadata:\rgenerateName: hello-world- # Workflow的配置名称\rspec:\rentrypoint: whalesay # 解析whalesay templates\rtemplates:\r- name: whalesay # 定义whalesay templates，和entrypoint保持一致\rcontainer: # 定义一个容器，输出\u0026quot;helloworld\u0026quot;\rimage: docker/whalesay\rcommand: [cowsay]\rargs: [\u0026quot;hello world\u0026quot;] Templates templates是列表结构，主要分为两类：\n 定义具体的工作流 调用其他模板提供并行控制  定义具体的工作流 定义具体的工作流有4种类别，如下：\n Container Script Resource Suspend  Container container是最常用的模板类型，它将调度一个container，其模板规范和K8S的容器规范相同，如下：\n - name: whalesay container: image: docker/whalesay\rcommand: [cowsay]\rargs: [\u0026quot;hello world\u0026quot;] Script Script是Container的另一种包装实现，其定义方式和Container相同，只是增加了source字段用于自定义脚本，如下：\n - name: gen-random-int\rscript:\rimage: python:alpine3.6\rcommand: [python]\rsource: |\rimport random\ri = random.randint(1, 100)\rprint(i)\r脚本的输出结果会根据调用方式自动导出到{{tasks.\u0026lt;NAME\u0026gt;.outputs.result}}或{{steps.\u0026lt;NAME\u0026gt;.outputs.result}}中。\nResource Resource主要用于直接在K8S集群上执行集群资源操作，可以 get, create, apply, delete, replace, patch集群资源。如下在集群中创建一个ConfigMap类型资源：\n - name: k8s-owner-reference\rresource:\raction: create\rmanifest: |\rapiVersion: v1\rkind: ConfigMap\rmetadata:\rgenerateName: owned-eg-\rdata:\rsome: value\rSuspend Suspend主要用于暂停，可以暂停一段时间，也可以手动恢复，命令使用argo resume进行恢复。定义格式如下：\n - name: delay\rsuspend:\rduration: \u0026quot;20s\u0026quot;\r调用其他模板提供并行控制 调用其他模板也有两种类别：\n Steps Dag  Steps Steps主要是通过定义一系列步骤来定义任务，其结构是\u0026quot;list of lists\u0026quot;，外部列表将顺序执行，内部列表将并行执行。如下：\n - name: hello-hello-hello\rsteps:\r- - name: step1\rtemplate: prepare-data\r- - name: step2a\rtemplate: run-data-first-half\r- name: step2b\rtemplate: run-data-second-half\r其中step1和step2a是顺序执行，而step2a和step2b是并行执行。\n还可以通过When来进行条件判断。如下：\napiVersion: argoproj.io/v1alpha1\rkind: Workflow\rmetadata:\rgenerateName: coinflip-\rspec:\rentrypoint: coinflip\rtemplates:\r- name: coinflip\rsteps:\r- - name: flip-coin\rtemplate: flip-coin\r- - name: heads\rtemplate: heads\rwhen: \u0026quot;{{steps.flip-coin.outputs.result}} == heads\u0026quot;\r- name: tails\rtemplate: tails\rwhen: \u0026quot;{{steps.flip-coin.outputs.result}} == tails\u0026quot;\r- name: flip-coin\rscript:\rimage: python:alpine3.6\rcommand: [python]\rsource: |\rimport random\rresult = \u0026quot;heads\u0026quot; if random.randint(0,1) == 0 else \u0026quot;tails\u0026quot;\rprint(result)\r- name: heads\rcontainer:\rimage: alpine:3.6\rcommand: [sh, -c]\rargs: [\u0026quot;echo \\\u0026quot;it was heads\\\u0026quot;\u0026quot;]\r- name: tails\rcontainer:\rimage: alpine:3.6\rcommand: [sh, -c]\rargs: [\u0026quot;echo \\\u0026quot;it was tails\\\u0026quot;\u0026quot;]\r提交这个Workflow，执行效果如下：\n除了使用When进行条件判断，还可以进行循环操作，示例代码如下：\napiVersion: argoproj.io/v1alpha1\rkind: Workflow\rmetadata:\rgenerateName: loops-\rspec:\rentrypoint: loop-example\rtemplates:\r- name: loop-example\rsteps:\r- - name: print-message\rtemplate: whalesay\rarguments:\rparameters:\r- name: message\rvalue: \u0026quot;{{item}}\u0026quot;\rwithItems:\r- hello world\r- goodbye world\r- name: whalesay\rinputs:\rparameters:\r- name: message\rcontainer:\rimage: docker/whalesay:latest\rcommand: [cowsay]\rargs: [\u0026quot;{{inputs.parameters.message}}\u0026quot;]\r提交Workflow，输出结果如下：\nDag Dag主要用于定义任务的依赖关系，可以设置开始特定任务之前必须完成其他任务，没有任何依赖关系的任务将立即执行。\n如下：\n - name: diamond\rdag:\rtasks:\r- name: A\rtemplate: echo\r- name: B\rdependencies: [A]\rtemplate: echo\r- name: C\rdependencies: [A]\rtemplate: echo\r- name: D\rdependencies: [B, C]\rtemplate: echo\r其中A会立即执行，B和C会依赖A，D依赖B和C。\n然后运行一个示例看看效果，示例如下：\napiVersion: argoproj.io/v1alpha1\rkind: Workflow\rmetadata:\rgenerateName: dag-diamond-\rspec:\rentrypoint: diamond\rtemplates:\r- name: diamond\rdag:\rtasks:\r- name: A\rtemplate: echo\rarguments:\rparameters: [{name: message, value: A}]\r- name: B\rdependencies: [A]\rtemplate: echo\rarguments:\rparameters: [{name: message, value: B}]\r- name: C\rdependencies: [A]\rtemplate: echo\rarguments:\rparameters: [{name: message, value: C}]\r- name: D\rdependencies: [B, C]\rtemplate: echo\rarguments:\rparameters: [{name: message, value: D}]\r- name: echo\rinputs:\rparameters:\r- name: message\rcontainer:\rimage: alpine:3.7\rcommand: [echo, \u0026quot;{{inputs.parameters.message}}\u0026quot;]\r提交workflow。\nargo submit -n argo dag.yam --watch\rVariables 在argo的Workflow中允许使用变量的，如下：\napiVersion: argoproj.io/v1alpha1\rkind: Workflow\rmetadata:\rgenerateName: hello-world-parameters-\rspec:\rentrypoint: whalesay\rarguments:\rparameters:\r- name: message\rvalue: hello world\rtemplates:\r- name: whalesay\rinputs:\rparameters:\r- name: message\rcontainer:\rimage: docker/whalesay\rcommand: [ cowsay ]\rargs: [ \u0026quot;{{inputs.parameters.message}}\u0026quot; ] 首先在spec字段定义arguments，定义变量message，其值是hello world，然后在templates字段中需要先定义一个inputs字段，用于templates的输入参数，然后在使用\u0026quot;{{}}\u0026quot;形式引用变量。\n变量还可以进行一些函数运算，主要有：\n filter：过滤 asInt：转换为Int asFloat：转换为Float string：转换为String toJson：转换为Json  例子：\nfilter([1, 2], { # \u0026gt; 1})\rasInt(inputs.parameters[\u0026quot;my-int-param\u0026quot;])\rasFloat(inputs.parameters[\u0026quot;my-float-param\u0026quot;])\rstring(1)\rtoJson([1, 2])\r更多语法可以访问https://github.com/antonmedv/expr/blob/master/docs/Language-Definition.md进行学习。\n制品库 在安装argo的时候，已经安装了mino作为制品库，那么到底该如何使用呢？\n先看一个官方的例子，如下：\napiVersion: argoproj.io/v1alpha1\rkind: Workflow\rmetadata:\rgenerateName: artifact-passing-\rspec:\rentrypoint: artifact-example\rtemplates:\r- name: artifact-example\rsteps:\r- - name: generate-artifact\rtemplate: whalesay\r- - name: consume-artifact\rtemplate: print-message\rarguments:\rartifacts:\r- name: message\rfrom: \u0026quot;{{steps.generate-artifact.outputs.artifacts.hello-art}}\u0026quot;\r- name: whalesay\rcontainer:\rimage: docker/whalesay:latest\rcommand: [sh, -c]\rargs: [\u0026quot;sleep 1; cowsay hello world | tee /tmp/hello_world.txt\u0026quot;]\routputs:\rartifacts:\r- name: hello-art\rpath: /tmp/hello_world.txt\r- name: print-message\rinputs:\rartifacts:\r- name: message\rpath: /tmp/message\rcontainer:\rimage: alpine:latest\rcommand: [sh, -c]\rargs: [\u0026quot;cat /tmp/message\u0026quot;]\r其分为两步：\n 首先生成制品 然后获取制品  提交Workflow，运行结果如下：\n然后在minio中可以看到生成的制品，制品经过了压缩，如下：\nWorkflowTemplate WorkflowTemplate是Workflow的模板，可以从WorkflowTemplate内部或者集群上其他Workflow和WorkflowTemplate引用它们。\nWorkflowTemplate和template的区别：\n template只是Workflow中templates下的一个任务，当我们定义一个Workflow时，至少需要定义一个template WorkflowTemplate是驻留在集群中的Workflow的定义，它是Workflow的定义，因为它包含模板，可以从WorkflowTemplate内部或者集群上其他Workflow和WorkflowTemplate引用它们。  在2.7版本后，WorkflowTemplate的定义和Workflow的定义一样，我们可以简单的将kind:Workflow改成kind:WorkflowTemplate。比如：\napiVersion: argoproj.io/v1alpha1\rkind: WorkflowTemplate\rmetadata:\rname: workflow-template-1\rspec:\rentrypoint: whalesay-template\rarguments:\rparameters:\r- name: message\rvalue: hello world\rtemplates:\r- name: whalesay-template\rinputs:\rparameters:\r- name: message\rcontainer:\rimage: docker/whalesay\rcommand: [cowsay]\rargs: [\u0026quot;{{inputs.parameters.message}}\u0026quot;]\r创建WorkflowTemplate，如下\nargo template create workflowtemplate.yaml\r然后在Workflow中引用，如下：\napiVersion: argoproj.io/v1alpha1\rkind: Workflow\rmetadata:\rgenerateName: workflow-template-hello-world-\rspec:\rentrypoint: whalesay\rtemplates:\r- name: whalesay\rsteps: # 引用模板必须在steps/dag/template下\r- - name: call-whalesay-template\rtemplateRef: # 应用模板字段\rname: workflow-template-1 # WorkflowTemplate名\rtemplate: whalesay-template # 具体的template名\rarguments: # 参数\rparameters:\r- name: message\rvalue: \u0026quot;hello world\u0026quot;\rClusterWorkflowTemplate ClusterWorkflowTemplate创建的是一个集群范围内的WorkflowTemplate，其他workflow可以引用它。\n如下定义一个ClusterWorkflow。\napiVersion: argoproj.io/v1alpha1\rkind: ClusterWorkflowTemplate\rmetadata:\rname: cluster-workflow-template-whalesay-template\rspec:\rtemplates:\r- name: whalesay-template\rinputs:\rparameters:\r- name: message\rcontainer:\rimage: docker/whalesay\rcommand: [cowsay]\rargs: [\u0026quot;{{inputs.parameters.message}}\u0026quot;]\r然后在workflow中使用templateRef去引用它，如下：\napiVersion: argoproj.io/v1alpha1\rkind: Workflow\rmetadata:\rgenerateName: workflow-template-hello-world-\rspec:\rentrypoint: whalesay\rtemplates:\r- name: whalesay\rsteps: - - name: call-whalesay-template\rtemplateRef: #引用模板\rname: cluster-workflow-template-whalesay-template # ClusterWorkflow名\rtemplate: whalesay-template # 具体的模板名\rclusterScope: true # 表示是ClusterWorkflow\rarguments: # 参数\rparameters:\r- name: message\rvalue: \u0026quot;hello world\u0026quot;\r实践 上面大概叙述了一下argo的基本理论知识，更多的理论知识可以到官网去学习。\n下面将使用一个简单的CI/CD实践，来了解一下用argo workflow应该如何做。\nCI/CD的整个流程很简单，即：拉代码-\u0026gt;编译-\u0026gt;构建镜像-\u0026gt;上传镜像-\u0026gt;部署。\n定义一个WorkflowTemplate，如下：\napiVersion: argoproj.io/v1alpha1\rkind: WorkflowTemplate\rmetadata:\rannotations:\rworkflows.argoproj.io/description: |\rCheckout out from Git, build and deploy application.\rworkflows.argoproj.io/maintainer: '@joker'\rworkflows.argoproj.io/tags: java, git\rworkflows.argoproj.io/version: '\u0026gt;= 2.9.0'\rname: devops-java spec:\rentrypoint: main\rarguments:\rparameters:\r- name: repo\rvalue: gitlab-test.coolops.cn:32080/root/springboot-helloworld.git\r- name: branch\rvalue: master\r- name: image\rvalue: registry.cn-hangzhou.aliyuncs.com/rookieops/myapp:202103101613\r- name: cache-image\rvalue: registry.cn-hangzhou.aliyuncs.com/rookieops/myapp\r- name: dockerfile\rvalue: Dockerfile\r- name: devops-cd-repo\rvalue: gitlab-test.coolops.cn:32080/root/devops-cd.git\r- name: gitlabUsername\rvalue: devops-argo\r- name: gitlabPassword\rvalue: devops123456\rtemplates:\r- name: main\rsteps:\r- - name: Checkout\rtemplate: Checkout\r- - name: Build\rtemplate: Build\r- - name: BuildImage\rtemplate: BuildImage\r- - name: Deploy\rtemplate: Deploy\r# 拉取代码\r- name: Checkout\rscript:\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/maven:3.5.0-alpine\rworkingDir: /work\rcommand:\r- sh\rsource: |\rgit clone --branch {{workflow.parameters.branch}} http://{{workflow.parameters.gitlabUsername}}:{{workflow.parameters.gitlabPassword}}@{{workflow.parameters.repo}} .\rvolumeMounts:\r- mountPath: /work\rname: work\r# 编译打包 - name: Build\rscript:\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/maven:3.5.0-alpine\rworkingDir: /work\rcommand:\r- sh\rsource: mvn -B clean package -Dmaven.test.skip=true -Dautoconfig.skip\rvolumeMounts:\r- mountPath: /work\rname: work\r# 构建镜像 - name: BuildImage\rvolumes:\r- name: docker-config\rsecret:\rsecretName: docker-config\rcontainer:\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/kaniko-executor:v1.5.0\rworkingDir: /work\rargs:\r- --context=.\r- --dockerfile={{workflow.parameters.dockerfile}}\r- --destination={{workflow.parameters.image}}\r- --skip-tls-verify\r- --reproducible\r- --cache=true\r- --cache-repo={{workflow.parameters.cache-image}}\rvolumeMounts:\r- mountPath: /work\rname: work\r- name: docker-config\rmountPath: /kaniko/.docker/\r# 部署 - name: Deploy\rscript:\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/kustomize:v3.8.1\rworkingDir: /work\rcommand:\r- sh\rsource: |\rgit remote set-url origin http://{{workflow.parameters.gitlabUsername}}:{{workflow.parameters.gitlabPassword}}@{{workflow.parameters.devops-cd-repo}}\rgit config --global user.name \u0026quot;Administrator\u0026quot;\rgit config --global user.email \u0026quot;coolops@163.com\u0026quot;\rgit clone http://{{workflow.parameters.gitlabUsername}}:{{workflow.parameters.gitlabPassword}}@{{workflow.parameters.devops-cd-repo}} /work/devops-cd\rcd /work/devops-cd\rgit pull\rcd /work/devops-cd/devops-simple-java\rkustomize edit set image {{workflow.parameters.image}}\rgit commit -am 'image update'\rgit push origin master\rvolumeMounts:\r- mountPath: /work\rname: work\rvolumeClaimTemplates:\r- name: work\rmetadata:\rname: work\rspec:\rstorageClassName: nfs-client-storageclass\raccessModes: [ \u0026quot;ReadWriteOnce\u0026quot; ]\rresources:\rrequests:\rstorage: 1Gi 说明：\n1、使用kaniko来创建镜像，不用挂载docker.sock，但是push镜像的时候需要config.json，所以首先需要创建一个secret，如下：\nkubectl create secret generic docker-config --from-file=.docker/config.json -n argo\r2、准备好storageClass，当然也可以不需要，直接使用empty，不过可以将缓存文件这些持久化，可以加速构建（我上面没有做）。\n3、创建WorkflowTemplate，命令如下：\nargo template create -n argo devops-java.yaml\r创建完成后，可以在UI界面看到刚创建的WorkflowTemplate，如下：\n4、创建Workflow，可以手动创建，如下：\napiVersion: argoproj.io/v1alpha1\rkind: Workflow\rmetadata:\rgenerateName: workflow-template-devops-java-\rspec:\rworkflowTemplateRef:\rname: devops-java\r也可以直接在UI界面点击创建，我这里直接在UI界面点击创建。选择刚创建的WorkflowTemplate，点击创建，如下：\n然后就会生成一条Workflow，如下：\n点进去，可以看到每个具体的步骤，如下\n点击每个具体的步骤，可以看日志，如下：\n也可以在命令行界面看到Workflow的执行结果，如下：\n初次使用到这里就结束了，后期会逐步去优化。\n参考文档  https://github.com/argoproj/argo-workflows/releases https://argoproj.github.io/argo-workflows https://github.com/antonmedv/expr/blob/master/docs/Language-Definition.md https://github.com/argoproj/argo-workflows/tree/master/examples  ","description":"本文主要介绍argo workflow以及CI/CD简单使用","id":2,"section":"posts","tags":["argo","devops"],"title":"Argo Workflows-Kubernetes的工作流引擎","uri":"https://www.coolops.cn/posts/argo-workflow/"},{"content":" 马上春节了，在这祝大家春节快乐。前阵子在使用K8S集群的时候，经常遇到Pod调度不均衡的问题，排查了许久也没找到所以然，最后认为的归结到Scheduler调度不够智能上（机智如我）。这不，为了使集群能够更均衡，充分利用节点，特地研究了一下Descheduler，下面内容主要来自官方文档，喜欢看官方文档的可以划到结尾处，喜欢看中文的，可以收藏慢慢看，对长期从事K8S集群管理的YAML工程师来说，还是有点用。\n 在Kubernetes中，kube-scheduler负责将Pod调度到合适的Node上，但是Kubernetes是一个非常动态的，高度弹性的环境，有时候会造成某一个或多个节点pod数分配不均，比如：\n 一些节点利用率低下或过度使用 添加删除标签或添加删除污点，pod或Node亲和性改变等造成原调度不再满足 一些节点故障，其上运行的Pod调度到其他节点 新节点加入集群  由于以上种种原因，可能导致多个Pod运行到不太理想的节点，而整个K8S集群也会处于一段时间不均衡的状态，这时候就需要重新平衡集群。Descheduler就是这样一个项目。\nDescheduler Descheduler可以根据一些规则配置来重新平衡集群状态，目前支持的策略有：\n RemoveDuplicates LowNodeUtilization RemovePodsViolatingInterPodAntiAffinity RemovePodsViolatingNodeAffinity RemovePodsViolatingNodeTaints RemovePodsViolatingTopologySpreadConstraint RemovePodsHavingTooManyRestarts PodLifeTime  这些策略可以启用，也可以关闭，默认情况下，所有策略都是启动的。\n另外，还有一些通用配置，如下：\n nodeSelector：限制要处理的节点 evictLocalStoragePods: 驱除使用LocalStorage的Pods ignorePvcPods: 是否忽略配置PVC的Pods，默认是False maxNoOfPodsToEvictPerNode：节点允许的最大驱逐Pods数  比如：\napiVersion: \u0026quot;descheduler/v1alpha1\u0026quot;\rkind: \u0026quot;DeschedulerPolicy\u0026quot;\rnodeSelector: prod=dev\revictLocalStoragePods: true\rmaxNoOfPodsToEvictPerNode: 40\rignorePvcPods: false\rstrategies:\r...\rRemoveDuplicates 该策略确保只有一个Pod与在同一节点上运行的副本集（RS），Replication Controller（RC），Deployment或Job相关联。 如果有更多的Pod，则将这些重复的Pod逐出，以更好地在群集中扩展。 如果某些节点由于任何原因而崩溃，并且它们上的Pod移至其他节点，导致多个与RS或RC关联的Pod（例如，在同一节点上运行），则可能发生此问题。 一旦发生故障的节点再次准备就绪，便可以启用此策略以驱逐这些重复的Pod。\n参数列表有：\n   参数名 类型     excludeOwnerKinds list(string)   namespaces list(string)   thresholdPriority int   thresholdPriorityClassName string    其中excludeOwnerKinds用于排除类型，这些类型下的Pod则不会被驱逐，比如：\napiVersion: \u0026quot;descheduler/v1alpha1\u0026quot;\rkind: \u0026quot;DeschedulerPolicy\u0026quot;\rstrategies:\r\u0026quot;RemoveDuplicates\u0026quot;:\renabled: true\rparams:\rremoveDuplicates:\rexcludeOwnerKinds:\r- \u0026quot;ReplicaSet\u0026quot;\rLowNodeUtilization 该策略主要是找到那些未充分利用的节点，将驱逐的Pod在这些节点上创建，该策略配置在nodeResourceUtilizationThresholds下。\n节点利用率低是由配置阈值决定的，配置在thresholds下，thresholds可以配置cpu、memory以及pods数量（百分比），如果节点利用率低于配置，则代表该节点未被充分利用。目前，Pod的请求资源需求被考虑用于计算节点资源利用率。\n另外一个参数targetThresholds，用于计算可能驱逐Pods的潜在节点，该参数也是配置cpu、memory以及Pods数量的百分比。如果超过该配置，则表示该节点被过度利用，上面的Pods就可能被驱逐。而在thresholds和targetThresholds之间的任何节点则认为是正常利用。\n参数有：\n   参数名 类型     thresholds map   targetThresholds map   numberOfNodes int   thresholdPriority int   thresholdPriorityClassName string    比如：\napiVersion: \u0026quot;descheduler/v1alpha1\u0026quot;\rkind: \u0026quot;DeschedulerPolicy\u0026quot;\rstrategies:\r\u0026quot;LowNodeUtilization\u0026quot;:\renabled: true\rparams:\rnodeResourceUtilizationThresholds:\rthresholds:\r\u0026quot;cpu\u0026quot; : 20\r\u0026quot;memory\u0026quot;: 20\r\u0026quot;pods\u0026quot;: 20\rtargetThresholds:\r\u0026quot;cpu\u0026quot; : 50\r\u0026quot;memory\u0026quot;: 50\r\u0026quot;pods\u0026quot;: 50\r需要注意的是：\n  仅支持以下三种资源类型\n   cpu memory pods    thresholds和targetThresholds必须配置相同的类型\n  参数值的访问是0-100（百分制）\n  相同的资源类型，thresholds的配置不能高于targetThresholds的配置\n  如果未指定任何资源类型，则默认是100%，已避免节点从未充分利用变为过度利用。\n与LowNodeUtilization策略关联的另一个参数称为numberOfNodes。 仅当未充分利用的节点数大于配置的值时，才可以配置此参数以激活策略。 这在大型群集中很有用，其中一些节点可能会频繁使用或短期使用不足。 默认情况下，numberOfNodes设置为零。\nRemovePodsViolatingInterPodAntiAffinity 该策略可确保从节点中删除违反Interpod反亲和关系的pod。例如，如果某个节点上有podA，并且podB和podC（在同一节点上运行）具有禁止它们在同一节点上运行的反亲和规则，则podA将被从该节点逐出，以便podB和podC正常运行。当 podB 和 podC 已经运行在节点上后，反亲和性规则被创建就会发送这样的问题。\n参数为：\n   参数名 类型     thresholdPriority int   thresholdPriorityClassName string   namespaces list(string)    如下开启该策略：\napiVersion: \u0026quot;descheduler/v1alpha1\u0026quot;\rkind: \u0026quot;DeschedulerPolicy\u0026quot;\rstrategies:\r\u0026quot;RemovePodsViolatingInterPodAntiAffinity\u0026quot;:\renabled: true\rRemovePodsViolatingNodeAffinity 启用后，该策略requiredDuringSchedulingRequiredDuringExecution将用作kubelet 的临时实现并逐出该kubelet，不再考虑节点亲和力。\n例如，在nodeA上调度了podA，该podA满足了调度时的节点亲缘性规则requiredDuringSchedulingIgnoredDuringExecution。随着时间的流逝，nodeA停止满足该规则。当执行该策略并且有另一个可用的节点满足该节点相似性规则时，podA被从nodeA中逐出。\n参数有：\n   参数名 类型     thresholdPriority int   thresholdPriorityClassName string   namspaces list(string)   nodeAffinityType list(string)    例如：\napiVersion: \u0026quot;descheduler/v1alpha1\u0026quot;\rkind: \u0026quot;DeschedulerPolicy\u0026quot;\rstrategies:\r\u0026quot;RemovePodsViolatingNodeAffinity\u0026quot;:\renabled: true\rparams:\rnodeAffinityType:\r- \u0026quot;requiredDuringSchedulingIgnoredDuringExecution\u0026quot;\rRemovePodsViolatingNodeTaints 该策略可以确保从节点中删除违反 NoSchedule 污点的 Pod。例如，有一个名为 podA 的 Pod，通过配置容忍 key=value:NoSchedule 允许被调度到有该污点配置的节点上，如果节点的污点随后被更新或者删除了，则污点将不再被 Pod 的容忍满足，然后将被驱逐。\n参数：\n   参数名 类型     thresholdPriority int   thresholdPriorityClassName string   namespaces list(string)    例如：\napiVersion: \u0026quot;descheduler/v1alpha1\u0026quot;\rkind: \u0026quot;DeschedulerPolicy\u0026quot;\rstrategies:\r\u0026quot;RemovePodsViolatingNodeTaints\u0026quot;:\renabled: true\rRemovePodsViolatingTopologySpreadConstraint 该策略确保从节点驱逐违反拓扑扩展约束的Pods，具体来说，它试图驱逐将拓扑域平衡到每个约束的maxSkew内所需的最小pod数，不过次策略需要k8s版本高于1.18才能使用。\n默认情况下，此策略仅处理硬约束，如果将参数includeSoftConstraints 设置为True，也将支持软约束。\n参数为：\n   参数名 类型     thresholdPriority int   thresholdPriorityClassName string   namespaces list(string)   includeSoftConstraints bool    例如：\napiVersion: \u0026quot;descheduler/v1alpha1\u0026quot;\rkind: \u0026quot;DeschedulerPolicy\u0026quot;\rstrategies:\r\u0026quot;RemovePodsViolatingTopologySpreadConstraint\u0026quot;:\renabled: true\rparams:\rincludeSoftConstraints: false\rRemovePodsHavingTooManyRestarts 该策略确保从节点中删除重启次数过多的Pods，例如，具有EBS / PD的Pod无法将卷/磁盘附加到实例，则应该将该Pod重新安排到其他节点。 它的参数包括podRestartThreshold（这是应将Pod逐出的重新启动次数），以及包括InitContainers，它确定在计算中是否应考虑初始化容器的重新启动。\n参数为：\n   参数名 类型     podRestartThreshold int   includingInitContainers bool   thresholdPriority int   thresholdPriorityClassName string   namespaces list(string)    例如：\napiVersion: \u0026quot;descheduler/v1alpha1\u0026quot;\rkind: \u0026quot;DeschedulerPolicy\u0026quot;\rstrategies:\r\u0026quot;RemovePodsHavingTooManyRestarts\u0026quot;:\renabled: true\rparams:\rpodsHavingTooManyRestarts:\rpodRestartThreshold: 100\rincludingInitContainers: true\rPodLifeTime 该策略用于驱逐比maxPodLifeTimeSeconds更旧的Pods，可以通过podStatusPhases 来配置哪类状态的Pods会被驱逐。\n参数有：\n   参数名 类型     maxPodLifeTimeSeconds int   podStatusPhases list(string)   thresholdPriority int (see priority filtering)   thresholdPriorityClassName string (see priority filtering)   namespaces (see namespace filtering)    例如：\napiVersion: \u0026quot;descheduler/v1alpha1\u0026quot;\rkind: \u0026quot;DeschedulerPolicy\u0026quot;\rstrategies:\r\u0026quot;PodLifeTime\u0026quot;:\renabled: true\rparams:\rpodLifeTime:\rmaxPodLifeTimeSeconds: 86400\rpodStatusPhases:\r- \u0026quot;Pending\u0026quot;\rFilter Pods 在驱逐Pods的时候，有时并不需要所有Pods都被驱逐，Descheduler提供一些过滤方式。\nNamespace filtering 该策略可以配置是包含还是排除某些名称空间。可以使用该策略的有：\n PodLifeTime RemovePodsHavingTooManyRestarts RemovePodsViolatingNodeTaints RemovePodsViolatingNodeAffinity RemovePodsViolatingInterPodAntiAffinity RemoveDuplicates RemovePodsViolatingTopologySpreadConstraint  （1）只驱逐某些命令空间下的Pods，使用include参数，如下：\napiVersion: \u0026quot;descheduler/v1alpha1\u0026quot;\rkind: \u0026quot;DeschedulerPolicy\u0026quot;\rstrategies:\r\u0026quot;PodLifeTime\u0026quot;:\renabled: true\rparams:\rpodLifeTime:\rmaxPodLifeTimeSeconds: 86400\rnamespaces:\rinclude:\r- \u0026quot;namespace1\u0026quot;\r- \u0026quot;namespace2\u0026quot;\r（2）排除掉某些命令空间下的Pods，使用exclude参数，如下：\napiVersion: \u0026quot;descheduler/v1alpha1\u0026quot;\rkind: \u0026quot;DeschedulerPolicy\u0026quot;\rstrategies:\r\u0026quot;PodLifeTime\u0026quot;:\renabled: true\rparams:\rpodLifeTime:\rmaxPodLifeTimeSeconds: 86400\rnamespaces:\rexclude:\r- \u0026quot;namespace1\u0026quot;\r- \u0026quot;namespace2\u0026quot;\rPriority filtering 所有策略都可以配置优先级阈值，只有在该阈值以下的Pod才能被驱逐。 您可以通过设置thresholdPriorityClassName（将阈值设置为给定优先级类别的值）或thresholdPriority（直接设置阈值）参数来指定此阈值。 默认情况下，此阈值设置为系统集群关键优先级类别的值。\n例如：\n（1）使用thresholdPriority\napiVersion: \u0026quot;descheduler/v1alpha1\u0026quot;\rkind: \u0026quot;DeschedulerPolicy\u0026quot;\rstrategies:\r\u0026quot;PodLifeTime\u0026quot;:\renabled: true\rparams:\rpodLifeTime:\rmaxPodLifeTimeSeconds: 86400\rthresholdPriority: 10000\r（2）使用thresholdPriorityClassName\napiVersion: \u0026quot;descheduler/v1alpha1\u0026quot;\rkind: \u0026quot;DeschedulerPolicy\u0026quot;\rstrategies:\r\u0026quot;PodLifeTime\u0026quot;:\renabled: true\rparams:\rpodLifeTime:\rmaxPodLifeTimeSeconds: 86400\rthresholdPriorityClassName: \u0026quot;priorityclass1\u0026quot;\r 注意：不能同时配置thresholdPriority和thresholdPriorityClassName，如果给定的优先级类不存在，则descheduler不会创建它，并且会引发错误。\n Pod Evictions 当使用descheduler驱除Pods的时候，需要注意以下几点：\n 关键性Pod不会为驱逐，比如priorityClassName设置为system-cluster-critical或system-node-critical的Pod 不属于RC、RS、Deployment或Job管理的Pod不会被驱逐 DS创建的Pods不会被驱逐 使用LocalStorage的Pod不会被驱逐，设置evictLocalStoragePods: true除外 除非设置ignorePvcPods: true，否正具有PVC的Pods会被驱逐 在LowNodeUtilization 和RemovePodsViolatingInterPodAntiAffinity策略下，Pods按优先级从低到高进行驱逐，如果优先级相同，Besteffort类型的Pod要先于Burstable和Guaranteed类型被驱逐 annotations中带有descheduler.alpha.kubernetes.io/evict字段的Pod都可以被驱逐，该注释用于覆盖阻止驱逐的检查，用户可以选择驱逐哪个Pods  如果Pods驱逐失败，可以设置--v=4或者从Descheduler日志中查找原因。\n如果驱逐违反PDB约束，则不会驱逐这类Pods。\n版本兼容    Descheduler Supported Kubernetes Version     v0.20 v1.20   v0.19 v1.19   v0.18 v1.18   v0.10 v1.17   v0.4-v0.9 v1.9+   v0.1-v0.3 v1.7-v1.8    实践  K8S集群版本：v1.18.9\n （1）下载对应版本的Descheduler\n$ wget https://github.com/kubernetes-sigs/descheduler/archive/v0.18.0.tar.gz\r（2）创建RBAC\n---\rkind: ClusterRole\rapiVersion: rbac.authorization.k8s.io/v1\rmetadata:\rname: descheduler-cluster-role\rnamespace: kube-system\rrules:\r- apiGroups: [\u0026quot;\u0026quot;]\rresources: [\u0026quot;events\u0026quot;]\rverbs: [\u0026quot;create\u0026quot;, \u0026quot;update\u0026quot;]\r- apiGroups: [\u0026quot;\u0026quot;]\rresources: [\u0026quot;nodes\u0026quot;]\rverbs: [\u0026quot;get\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;]\r- apiGroups: [\u0026quot;\u0026quot;]\rresources: [\u0026quot;pods\u0026quot;]\rverbs: [\u0026quot;get\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;delete\u0026quot;]\r- apiGroups: [\u0026quot;\u0026quot;]\rresources: [\u0026quot;pods/eviction\u0026quot;]\rverbs: [\u0026quot;create\u0026quot;]\r---\rapiVersion: v1\rkind: ServiceAccount\rmetadata:\rname: descheduler-sa\rnamespace: kube-system\r---\rapiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRoleBinding\rmetadata:\rname: descheduler-cluster-role-binding\rnamespace: kube-system\rroleRef:\rapiGroup: rbac.authorization.k8s.io\rkind: ClusterRole\rname: descheduler-cluster-role\rsubjects:\r- name: descheduler-sa\rkind: ServiceAccount\rnamespace: kube-system\r（3）创建ConfigMap，该配置文件主要配置驱逐策略，如下：\n---\rapiVersion: v1\rkind: ConfigMap\rmetadata:\rname: descheduler-policy-configmap\rnamespace: kube-system\rdata:\rpolicy.yaml: |\rapiVersion: \u0026quot;descheduler/v1alpha1\u0026quot;\rkind: \u0026quot;DeschedulerPolicy\u0026quot;\rstrategies:\r\u0026quot;RemoveDuplicates\u0026quot;:\renabled: true\r\u0026quot;RemovePodsViolatingInterPodAntiAffinity\u0026quot;:\renabled: true\r\u0026quot;LowNodeUtilization\u0026quot;:\renabled: true\rparams:\rnodeResourceUtilizationThresholds:\rthresholds:\r\u0026quot;cpu\u0026quot; : 20\r\u0026quot;memory\u0026quot;: 20\r\u0026quot;pods\u0026quot;: 20\rtargetThresholds:\r\u0026quot;cpu\u0026quot; : 50\r\u0026quot;memory\u0026quot;: 50\r\u0026quot;pods\u0026quot;: 50\r（4）使用Job来进行驱逐，配置文件如下：\n---\rapiVersion: batch/v1\rkind: Job\rmetadata:\rname: descheduler-job\rnamespace: kube-system\rspec:\rparallelism: 1\rcompletions: 1\rtemplate:\rmetadata:\rname: descheduler-pod\rspec:\rpriorityClassName: system-cluster-critical\rcontainers:\r- name: descheduler\rimage: us.gcr.io/k8s-artifacts-prod/descheduler/descheduler:v0.10.0\rvolumeMounts:\r- mountPath: /policy-dir\rname: policy-volume\rcommand:\r- \u0026quot;/bin/descheduler\u0026quot;\rargs:\r- \u0026quot;--policy-config-file\u0026quot;\r- \u0026quot;/policy-dir/policy.yaml\u0026quot;\r- \u0026quot;--v\u0026quot;\r- \u0026quot;3\u0026quot;\rrestartPolicy: \u0026quot;Never\u0026quot;\rserviceAccountName: descheduler-sa\rvolumes:\r- name: policy-volume\rconfigMap:\rname: descheduler-policy-configmap\r（5）如果需要配置定时任务进行驱逐，则使用CronJob，如下：\n---\rapiVersion: batch/v1beta1\rkind: CronJob\rmetadata:\rname: descheduler-cronjob\rnamespace: kube-system\rspec:\rschedule: \u0026quot;*/2 * * * *\u0026quot;\rconcurrencyPolicy: \u0026quot;Forbid\u0026quot;\rjobTemplate:\rspec:\rtemplate:\rmetadata:\rname: descheduler-pod\rspec:\rpriorityClassName: system-cluster-critical\rcontainers:\r- name: descheduler\rimage: us.gcr.io/k8s-artifacts-prod/descheduler/descheduler:v0.10.0\rvolumeMounts:\r- mountPath: /policy-dir\rname: policy-volume\rcommand:\r- \u0026quot;/bin/descheduler\u0026quot;\rargs:\r- \u0026quot;--policy-config-file\u0026quot;\r- \u0026quot;/policy-dir/policy.yaml\u0026quot;\r- \u0026quot;--v\u0026quot;\r- \u0026quot;3\u0026quot;\rrestartPolicy: \u0026quot;Never\u0026quot;\rserviceAccountName: descheduler-sa\rvolumes:\r- name: policy-volume\rconfigMap:\rname: descheduler-policy-configmap\r 参考：https://github.com/kubernetes-sigs/descheduler\n ","description":"本文主要介绍K8S中的调度均衡器Descheduler，它的配置以及使用注意事项","id":3,"section":"posts","tags":["kubernetes","descheduler"],"title":"Kubernetes中的调度均衡器Descheduler","uri":"https://www.coolops.cn/posts/kubernetes-descheduler/"},{"content":"CI/CD并不是陌生的东西，大部分企业都有自己的CI/CD，不过今天我要介绍的是使用Jenkins和GitOps实现CI/CD。\n整体架构如下：\n涉及的软件以及版本信息如下：\n   软件 版本     kubernetes 1.17.9   docker 19.03.13   jenkins 2.249.3   argocd 1.8.0   gitlab 社区版11.8.1   sonarqube 社区版8.5.1   traefik 2.3.3   代码仓库 阿里云仓库    涉及的技术：\n Jenkins shareLibrary Jenkins pipeline Jenkinsfile Argocd sonarqube api操作  软件安装 软件安装我这里不贴具体的安装代码了，所有的代码我都放在了github上，地址：https://github.com/cool-ops/kubernetes-software-yaml.git\n所以这里默认你已经安装好所以软件了。\n在Jenkins上安装如下插件  kubernetes AnsiColor HTTP Request SonarQube Scanner Utility Steps Email Extension Template Gitlab Hook Gitlab  在Jenkins上配置Kubernetes集群信息 在系统管理\u0026ndash;\u0026gt;系统配置\u0026ndash;\u0026gt;cloud\n在Jenkins上配置邮箱地址 系统设置\u0026ndash;\u0026gt;系统配置\u0026ndash;\u0026gt;Email\n（1）设置管理员邮箱\n配置SMTP服务\n在Gitlab上准备一个测试代码 我这里有一个简单的java测试代码，地址如下：https://gitee.com/jokerbai/springboot-helloworld.git\n可以将其导入到自己的gitlab仓库。\n在Gitlab上创建一个共享库 首先在gitlab上创建一个共享库，我这里取名叫shareLibrary，如下：\n然后创建src/org/devops目录，并在该目录下创建一下文件。\n它们的内容分别如下：\nbuild.groovy\npackage org.devops\r// docker容器直接build\rdef DockerBuild(buildShell){\rsh \u0026quot;\u0026quot;\u0026quot;\r${buildShell}\r\u0026quot;\u0026quot;\u0026quot;\r}\rsendEmail.groovy\npackage org.devops\r//定义邮件内容\rdef SendEmail(status,emailUser){\remailext body: \u0026quot;\u0026quot;\u0026quot;\r\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body leftmargin=\u0026quot;8\u0026quot; marginwidth=\u0026quot;0\u0026quot; topmargin=\u0026quot;8\u0026quot; marginheight=\u0026quot;4\u0026quot; offset=\u0026quot;0\u0026quot;\u0026gt; \u0026lt;table width=\u0026quot;95%\u0026quot; cellpadding=\u0026quot;0\u0026quot; cellspacing=\u0026quot;0\u0026quot; style=\u0026quot;font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sans-serif\u0026quot;\u0026gt; \u0026lt;tr\u0026gt;\r本邮件由系统自动发出，无需回复！\u0026lt;br/\u0026gt;\r各位同事，大家好，以下为${JOB_NAME}项目构建信息\u0026lt;/br\u0026gt;\r\u0026lt;td\u0026gt;\u0026lt;font color=\u0026quot;#CC0000\u0026quot;\u0026gt;构建结果 - ${status}\u0026lt;/font\u0026gt;\u0026lt;/td\u0026gt;\r\u0026lt;/tr\u0026gt;\r\u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;b\u0026gt;\u0026lt;font color=\u0026quot;#0B610B\u0026quot;\u0026gt;构建信息\u0026lt;/font\u0026gt;\u0026lt;/b\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;项目名称：${JOB_NAME}\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;构建编号：${BUILD_ID}\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;构建状态: ${status} \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;项目地址：\u0026lt;a href=\u0026quot;${BUILD_URL}\u0026quot;\u0026gt;${BUILD_URL}\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;构建日志：\u0026lt;a href=\u0026quot;${BUILD_URL}console\u0026quot;\u0026gt;${BUILD_URL}console\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026quot;\u0026quot;\u0026quot;,\rsubject: \u0026quot;Jenkins-${JOB_NAME}项目构建信息 \u0026quot;,\rto: emailUser\r}\rsonarAPI.groovy\npackage ore.devops\r// 封装HTTP请求\rdef HttpReq(requestType,requestUrl,requestBody){\r// 定义sonar api接口\rdef sonarServer = \u0026quot;http://sonar.devops.svc.cluster.local:9000/api\u0026quot;\rresult = httpRequest authentication: 'sonar-admin-user',\rhttpMode: requestType,\rcontentType: \u0026quot;APPLICATION_JSON\u0026quot;,\rconsoleLogResponseBody: true,\rignoreSslErrors: true,\rrequestBody: requestBody,\rurl: \u0026quot;${sonarServer}/${requestUrl}\u0026quot;\rreturn result\r}\r// 获取soanr项目的状态\rdef GetSonarStatus(projectName){\rdef apiUrl = \u0026quot;project_branches/list?project=${projectName}\u0026quot;\r// 发请求\rresponse = HttpReq(\u0026quot;GET\u0026quot;,apiUrl,\u0026quot;\u0026quot;)\r// 对返回的文本做JSON解析\rresponse = readJSON text: \u0026quot;\u0026quot;\u0026quot;${response.content}\u0026quot;\u0026quot;\u0026quot;\r// 获取状态值\rresult = response[\u0026quot;branches\u0026quot;][0][\u0026quot;status\u0026quot;][\u0026quot;qualityGateStatus\u0026quot;]\rreturn result\r}\r// 获取sonar项目，判断项目是否存在\rdef SearchProject(projectName){\rdef apiUrl = \u0026quot;projects/search?projects=${projectName}\u0026quot;\r// 发请求\rresponse = HttpReq(\u0026quot;GET\u0026quot;,apiUrl,\u0026quot;\u0026quot;)\rprintln \u0026quot;搜索的结果：${response}\u0026quot;\r// 对返回的文本做JSON解析\rresponse = readJSON text: \u0026quot;\u0026quot;\u0026quot;${response.content}\u0026quot;\u0026quot;\u0026quot;\r// 获取total字段，该字段如果是0则表示项目不存在,否则表示项目存在\rresult = response[\u0026quot;paging\u0026quot;][\u0026quot;total\u0026quot;]\r// 对result进行判断\rif (result.toString() == \u0026quot;0\u0026quot;){\rreturn \u0026quot;false\u0026quot;\r}else{\rreturn \u0026quot;true\u0026quot;\r}\r}\r// 创建sonar项目\rdef CreateProject(projectName){\rdef apiUrl = \u0026quot;projects/create?name=${projectName}\u0026amp;project=${projectName}\u0026quot;\r// 发请求\rresponse = HttpReq(\u0026quot;POST\u0026quot;,apiUrl,\u0026quot;\u0026quot;)\rprintln(response)\r}\r// 配置项目质量规则\rdef ConfigQualityProfiles(projectName,lang,qpname){\rdef apiUrl = \u0026quot;qualityprofiles/add_project?language=${lang}\u0026amp;project=${projectName}\u0026amp;qualityProfile=${qpname}\u0026quot;\r// 发请求\rresponse = HttpReq(\u0026quot;POST\u0026quot;,apiUrl,\u0026quot;\u0026quot;)\rprintln(response)\r}\r// 获取质量阈ID\rdef GetQualityGateId(gateName){\rdef apiUrl = \u0026quot;qualitygates/show?name=${gateName}\u0026quot;\r// 发请求\rresponse = HttpReq(\u0026quot;GET\u0026quot;,apiUrl,\u0026quot;\u0026quot;)\r// 对返回的文本做JSON解析\rresponse = readJSON text: \u0026quot;\u0026quot;\u0026quot;${response.content}\u0026quot;\u0026quot;\u0026quot;\r// 获取total字段，该字段如果是0则表示项目不存在,否则表示项目存在\rresult = response[\u0026quot;id\u0026quot;]\rreturn result\r}\r// 更新质量阈规则\rdef ConfigQualityGate(projectKey,gateName){\r// 获取质量阈id\rgateId = GetQualityGateId(gateName)\rapiUrl = \u0026quot;qualitygates/select?projectKey=${projectKey}\u0026amp;gateId=${gateId}\u0026quot;\r// 发请求\rresponse = HttpReq(\u0026quot;POST\u0026quot;,apiUrl,\u0026quot;\u0026quot;)\rprintln(response)\r}\r//获取Sonar质量阈状态\rdef GetProjectStatus(projectName){\rapiUrl = \u0026quot;project_branches/list?project=${projectName}\u0026quot;\rresponse = HttpReq(\u0026quot;GET\u0026quot;,apiUrl,'')\rresponse = readJSON text: \u0026quot;\u0026quot;\u0026quot;${response.content}\u0026quot;\u0026quot;\u0026quot;\rresult = response[\u0026quot;branches\u0026quot;][0][\u0026quot;status\u0026quot;][\u0026quot;qualityGateStatus\u0026quot;]\r//println(response)\rreturn result\r}\rsonarqube.groovy\npackage ore.devops\rdef SonarScan(projectName,projectDesc,projectPath){\r// sonarScanner安装地址\rdef sonarHome = \u0026quot;/opt/sonar-scanner\u0026quot;\r// sonarqube服务端地址\rdef sonarServer = \u0026quot;http://sonar.devops.svc.cluster.local:9000/\u0026quot;\r// 以时间戳为版本\rdef scanTime = sh returnStdout: true, script: 'date +%Y%m%d%H%m%S'\rscanTime = scanTime - \u0026quot;\\n\u0026quot;\rsh \u0026quot;\u0026quot;\u0026quot;\r${sonarHome}/bin/sonar-scanner -Dsonar.host.url=${sonarServer} \\\r-Dsonar.projectKey=${projectName} \\\r-Dsonar.projectName=${projectName} \\\r-Dsonar.projectVersion=${scanTime} \\\r-Dsonar.login=admin \\\r-Dsonar.password=admin \\\r-Dsonar.ws.timeout=30 \\\r-Dsonar.projectDescription=\u0026quot;${projectDesc}\u0026quot; \\\r-Dsonar.links.homepage=http://www.baidu.com \\\r-Dsonar.sources=${projectPath} \\\r-Dsonar.sourceEncoding=UTF-8 \\\r-Dsonar.java.binaries=target/classes \\\r-Dsonar.java.test.binaries=target/test-classes \\\r-Dsonar.java.surefire.report=target/surefire-reports -X echo \u0026quot;${projectName} scan success!\u0026quot;\r\u0026quot;\u0026quot;\u0026quot;\r}\rtools.groovy\npackage org.devops\r//格式化输出\rdef PrintMes(value,color){\rcolors = ['red' : \u0026quot;\\033[40;31m \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;${value}\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; \\033[0m\u0026quot;,\r'blue' : \u0026quot;\\033[47;34m ${value} \\033[0m\u0026quot;,\r'green' : \u0026quot;\u001b[1;32m\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;${value}\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u001b[m\u0026quot;,\r'green1' : \u0026quot;\\033[40;32m \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;${value}\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; \\033[0m\u0026quot; ]\ransiColor('xterm') {\rprintln(colors[color])\r}\r}\r// 获取镜像版本\rdef createVersion() {\r// 定义一个版本号作为当次构建的版本，输出结果 20191210175842_69\rreturn new Date().format('yyyyMMddHHmmss') + \u0026quot;_${env.BUILD_ID}\u0026quot;\r}\r// 获取时间\rdef getTime() {\r// 定义一个版本号作为当次构建的版本，输出结果 20191210175842\rreturn new Date().format('yyyyMMddHHmmss')\r}\r在Gitlab上创建一个YAML管理仓库 我这里创建了一个叫devops-cd的共享仓库，如下：\n然后以应用名创建一个目录，并在目录下创建以下几个文件。\n它们的内容分别如下。\nservice.yaml\nkind: Service\rapiVersion: v1\rmetadata:\rname: the-service\rnamespace: default\rspec:\rselector:\rdeployment: hello\rtype: NodePort\rports:\r- protocol: TCP\rport: 8080\rtargetPort: 8080\ringress.yaml\napiVersion: extensions/v1beta1\rkind: Ingress\rmetadata:\rname: the-ingress namespace: default\rspec:\rrules:\r- host: test.coolops.cn http:\rpaths:\r- backend:\rserviceName: the-service servicePort: 8080 path: /\rdeploymeny.yaml\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: the-deployment\rnamespace: default\rspec:\rreplicas: 3\rselector:\rmatchLabels:\rdeployment: hello\rtemplate:\rmetadata:\rlabels:\rdeployment: hello\rspec:\rcontainers:\r- args:\r- -jar\r- /opt/myapp.jar\r- --server.port=8080\rcommand:\r- java\renv:\r- name: HOST_IP\rvalueFrom:\rfieldRef:\rapiVersion: v1\rfieldPath: status.hostIP\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/myapp:latest\rimagePullPolicy: IfNotPresent\rlifecycle:\rpreStop:\rexec:\rcommand:\r- /bin/sh\r- -c\r- /bin/sleep 30\rlivenessProbe:\rfailureThreshold: 3\rhttpGet:\rpath: /hello\rport: 8080\rscheme: HTTP\rinitialDelaySeconds: 60\rperiodSeconds: 15\rsuccessThreshold: 1\rtimeoutSeconds: 1\rname: myapp\rports:\r- containerPort: 8080\rname: http\rprotocol: TCP\rreadinessProbe:\rfailureThreshold: 3\rhttpGet:\rpath: /hello\rport: 8080\rscheme: HTTP\rperiodSeconds: 15\rsuccessThreshold: 1\rtimeoutSeconds: 1\rresources:\rlimits:\rcpu: \u0026quot;1\u0026quot;\rmemory: 2Gi\rrequests:\rcpu: 100m\rmemory: 1Gi\rterminationMessagePath: /dev/termination-log\rterminationMessagePolicy: File\rdnsPolicy: ClusterFirstWithHostNet\rimagePullSecrets:\r- name: gitlab-registry\rkustomization.yaml\n# Example configuration for the webserver\r# at https://github.com/monopole/hello\rcommonLabels:\rapp: hello\rresources:\r- deployment.yaml\r- service.yaml\r- ingress.yaml\rapiVersion: kustomize.config.k8s.io/v1beta1\rkind: Kustomization\rimages:\r- name: registry.cn-hangzhou.aliyuncs.com/rookieops/myapp\rnewTag: \u0026quot;20201127150733_70\u0026quot;\rnamespace: dev\r在Jenkins上配置共享库 （1）需要在Jenkins上添加凭证\n（2）在Jenkins的系统配置里面配置共享库（系统管理\u0026ndash;\u0026gt;系统配置）\n然后点击应用并保存\n然后我们可以用一个简单的Jenkinsfile测试一下共享库，看配置是否正确。\n在Jenkins上创建一个项目，如下：\n然后在最地下的pipeline处贴入以下代码：\ndef labels = \u0026quot;slave-${UUID.randomUUID().toString()}\u0026quot;\r// 引用共享库\r@Library(\u0026quot;jenkins_shareLibrary\u0026quot;)\r// 应用共享库中的方法\rdef tools = new org.devops.tools()\rpipeline {\ragent {\rkubernetes {\rlabel labels\ryaml \u0026quot;\u0026quot;\u0026quot;\rapiVersion: v1\rkind: Pod\rmetadata:\rlabels:\rsome-label: some-label-value\rspec:\rvolumes:\r- name: docker-sock\rhostPath:\rpath: /var/run/docker.sock\rtype: ''\rcontainers:\r- name: jnlp\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/inbound-agent:4.3-4\r- name: maven\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/maven:3.5.0-alpine\rcommand:\r- cat\rtty: true\r- name: docker\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/docker:19.03.11\rcommand:\r- cat\rtty: true\rvolumeMounts:\r- name: docker-sock\rmountPath: /var/run/docker.sock\r\u0026quot;\u0026quot;\u0026quot;\r}\r}\rstages {\rstage('Checkout') {\rsteps {\rscript{\rtools.PrintMes(\u0026quot;拉代码\u0026quot;,\u0026quot;green\u0026quot;)\r}\r}\r}\rstage('Build') {\rsteps {\rcontainer('maven') {\rscript{\rtools.PrintMes(\u0026quot;编译打包\u0026quot;,\u0026quot;green\u0026quot;)\r}\r}\r}\r}\rstage('Make Image') {\rsteps {\rcontainer('docker') {\rscript{\rtools.PrintMes(\u0026quot;构建镜像\u0026quot;,\u0026quot;green\u0026quot;)\r}\r}\r}\r}\r}\r}\r然后点击保存并运行，如果看到输出有颜色，就代表共享库配置成功，如下：\n到此共享库配置完成。\n编写Jenkinsfile 整个java的Jenkinsfile如下：\ndef labels = \u0026quot;slave-${UUID.randomUUID().toString()}\u0026quot;\r// 引用共享库\r@Library(\u0026quot;jenkins_shareLibrary\u0026quot;)\r// 应用共享库中的方法\rdef tools = new org.devops.tools()\rdef sonarapi = new org.devops.sonarAPI()\rdef sendEmail = new org.devops.sendEmail()\rdef build = new org.devops.build()\rdef sonar = new org.devops.sonarqube()\r// 前端传来的变量\rdef gitBranch = env.branch\rdef gitUrl = env.git_url\rdef buildShell = env.build_shell\rdef image = env.image\rdef dockerRegistryUrl = env.dockerRegistryUrl\rdef devops_cd_git = env.devops_cd_git\rpipeline {\ragent {\rkubernetes {\rlabel labels\ryaml \u0026quot;\u0026quot;\u0026quot;\rapiVersion: v1\rkind: Pod\rmetadata:\rlabels:\rsome-label: some-label-value\rspec:\rvolumes:\r- name: docker-sock\rhostPath:\rpath: /var/run/docker.sock\rtype: ''\r- name: maven-cache\rpersistentVolumeClaim:\rclaimName: maven-cache-pvc\rcontainers:\r- name: jnlp\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/inbound-agent:4.3-4\r- name: maven\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/maven:3.5.0-alpine\rcommand:\r- cat\rtty: true\rvolumeMounts:\r- name: maven-cache\rmountPath: /root/.m2\r- name: docker\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/docker:19.03.11\rcommand:\r- cat\rtty: true\rvolumeMounts:\r- name: docker-sock\rmountPath: /var/run/docker.sock\r- name: sonar-scanner\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/sonar-scanner:latest\rcommand:\r- cat\rtty: true\r- name: kustomize\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/kustomize:v3.8.1\rcommand:\r- cat\rtty: true\r\u0026quot;\u0026quot;\u0026quot;\r}\r}\renvironment{\rauth = 'joker'\r}\roptions {\rtimestamps() // 日志会有时间\rskipDefaultCheckout() // 删除隐式checkout scm语句\rdisableConcurrentBuilds() //禁止并行\rtimeout(time:1,unit:'HOURS') //设置流水线超时时间\r}\rstages {\r// 拉取代码\rstage('GetCode') {\rsteps {\rcheckout([$class: 'GitSCM', branches: [[name: \u0026quot;${gitBranch}\u0026quot;]],\rdoGenerateSubmoduleConfigurations: false,\rextensions: [],\rsubmoduleCfg: [],\ruserRemoteConfigs: [[credentialsId: '83d2e934-75c9-48fe-9703-b48e2feff4d8', url: \u0026quot;${gitUrl}\u0026quot;]]])\r}\r}\r// 单元测试和编译打包\rstage('Build\u0026amp;Test') {\rsteps {\rcontainer('maven') {\rscript{\rtools.PrintMes(\u0026quot;编译打包\u0026quot;,\u0026quot;blue\u0026quot;)\rbuild.DockerBuild(\u0026quot;${buildShell}\u0026quot;)\r}\r}\r}\r}\r// 代码扫描\rstage('CodeScanner') {\rsteps {\rcontainer('sonar-scanner') {\rscript {\rtools.PrintMes(\u0026quot;代码扫描\u0026quot;,\u0026quot;green\u0026quot;)\rtools.PrintMes(\u0026quot;搜索项目\u0026quot;,\u0026quot;green\u0026quot;)\rresult = sonarapi.SearchProject(\u0026quot;${JOB_NAME}\u0026quot;)\rprintln(result)\rif (result == \u0026quot;false\u0026quot;){\rprintln(\u0026quot;${JOB_NAME}---项目不存在,准备创建项目---\u0026gt; ${JOB_NAME}！\u0026quot;)\rsonarapi.CreateProject(\u0026quot;${JOB_NAME}\u0026quot;)\r} else {\rprintln(\u0026quot;${JOB_NAME}---项目已存在！\u0026quot;)\r}\rtools.PrintMes(\u0026quot;代码扫描\u0026quot;,\u0026quot;green\u0026quot;)\rsonar.SonarScan(\u0026quot;${JOB_NAME}\u0026quot;,\u0026quot;${JOB_NAME}\u0026quot;,\u0026quot;src\u0026quot;)\rsleep 10\rtools.PrintMes(\u0026quot;获取扫描结果\u0026quot;,\u0026quot;green\u0026quot;)\rresult = sonarapi.GetProjectStatus(\u0026quot;${JOB_NAME}\u0026quot;)\rprintln(result)\rif (result.toString() == \u0026quot;ERROR\u0026quot;){\rtoemail.Email(\u0026quot;代码质量阈错误！请及时修复！\u0026quot;,userEmail)\rerror \u0026quot; 代码质量阈错误！请及时修复！\u0026quot;\r} else {\rprintln(result)\r}\r}\r}\r}\r}\r// 构建镜像\rstage('BuildImage') {\rsteps {\rwithCredentials([[$class: 'UsernamePasswordMultiBinding',\rcredentialsId: 'dockerhub',\rusernameVariable: 'DOCKER_HUB_USER',\rpasswordVariable: 'DOCKER_HUB_PASSWORD']]) {\rcontainer('docker') {\rscript{\rtools.PrintMes(\u0026quot;构建镜像\u0026quot;,\u0026quot;green\u0026quot;)\rimageTag = tools.createVersion()\rsh \u0026quot;\u0026quot;\u0026quot;\rdocker login ${dockerRegistryUrl} -u ${DOCKER_HUB_USER} -p ${DOCKER_HUB_PASSWORD}\rdocker build -t ${image}:${imageTag} .\rdocker push ${image}:${imageTag}\rdocker rmi ${image}:${imageTag}\r\u0026quot;\u0026quot;\u0026quot;\r}\r}\r}\r}\r}\r// 部署\rstage('Deploy') {\rsteps {\rwithCredentials([[$class: 'UsernamePasswordMultiBinding',\rcredentialsId: 'ci-devops',\rusernameVariable: 'DEVOPS_USER',\rpasswordVariable: 'DEVOPS_PASSWORD']]){\rcontainer('kustomize') {\rscript{\rAPP_DIR=\u0026quot;${JOB_NAME}\u0026quot;.split(\u0026quot;_\u0026quot;)[0]\rsh \u0026quot;\u0026quot;\u0026quot;\rgit remote set-url origin http://${DEVOPS_USER}:${DEVOPS_PASSWORD}@${devops_cd_git}\rgit config --global user.name \u0026quot;Administrator\u0026quot;\rgit config --global user.email \u0026quot;coolops@163.com\u0026quot;\rgit clone http://${DEVOPS_USER}:${DEVOPS_PASSWORD}@${devops_cd_git} /opt/devops-cd\rcd /opt/devops-cd\rgit pull\rcd /opt/devops-cd/${APP_DIR}\rkustomize edit set image ${image}:${imageTag}\rgit commit -am 'image update'\rgit push origin master\r\u0026quot;\u0026quot;\u0026quot;\r}\r}\r}\r}\r}\r// 接口测试\rstage('InterfaceTest') {\rsteps{\rsh 'echo \u0026quot;接口测试\u0026quot;'\r}\r}\r}\r// 构建后的操作\rpost {\rsuccess {\rscript{\rprintln(\u0026quot;success:只有构建成功才会执行\u0026quot;)\rcurrentBuild.description += \u0026quot;\\n构建成功！\u0026quot;\r// deploy.AnsibleDeploy(\u0026quot;${deployHosts}\u0026quot;,\u0026quot;-m ping\u0026quot;)\rsendEmail.SendEmail(\u0026quot;构建成功\u0026quot;,toEmailUser)\r// dingmes.SendDingTalk(\u0026quot;构建成功 ✅\u0026quot;)\r}\r}\rfailure {\rscript{\rprintln(\u0026quot;failure:只有构建失败才会执行\u0026quot;)\rcurrentBuild.description += \u0026quot;\\n构建失败!\u0026quot;\rsendEmail.SendEmail(\u0026quot;构建失败\u0026quot;,toEmailUser)\r// dingmes.SendDingTalk(\u0026quot;构建失败 ❌\u0026quot;)\r}\r}\raborted {\rscript{\rprintln(\u0026quot;aborted:只有取消构建才会执行\u0026quot;)\rcurrentBuild.description += \u0026quot;\\n构建取消!\u0026quot;\rsendEmail.SendEmail(\u0026quot;取消构建\u0026quot;,toEmailUser)\r// dingmes.SendDingTalk(\u0026quot;构建失败 ❌\u0026quot;,\u0026quot;暂停或中断\u0026quot;)\r}\r}\r}\r}\r需要在Jenkins上创建两个凭证，一个id叫dockerhub，一个叫ci-devops，还有一个叫sonar-admin-user。\ndockerhub是登录镜像仓库的用户名和密码。\nci-devops是管理YAML仓库的用户名和密码。\nsonar-admin-user是管理sonarqube的用户名和密码。\n然后将这个Jenkinsfile保存到shareLibrary的根目录下，命名为java.Jenkinsfile。\n在Jenkins上配置项目 在Jenkins上新建一个项目，如下：\n然后添加以下参数化构建。\n然后在流水线处配置Pipeline from SCM\n此处需要注意脚本名。\n然后点击应用保存，并运行。\n也可以在sonarqube上看到代码扫描的结果。\n在Argocd上配置CD流程 在argocd上添加代码仓库，如下：\n然后创建应用，如下：\n点击创建后，如下：\n点进去可以看到更多的详细信息。\n argocd有一个小bug，它ingress的健康检查必须要loadBalance有值，不然就不通过，但是并不影响使用。\n 然后可以正常访问应用了。\nnode项目的Jenkinsfile大同小异，由于我没有测试用例，所以并没有测试。\n集成Gitlab，通过Webhook触发Jenkins 在Jenkins中选择项目，在项目中配置gitlab触发，如下：\n生成token，如下\n在gitlab上配置集成。进入项目\u0026ndash;\u0026gt;项目设置\u0026ndash;\u0026gt;集成\n配置Jenkins上生成的回调URL和TOKEN\n到此配置完成，然后点击下方test，可以观察是否触发流水线。\n也可以通过修改仓库代码进行测试。\n写在最后 本片文章是纯操作步骤，大家在测试的时候可能会对Jenkinsfile做细微的调整，不过整体没什么问题。\n","description":"本文主要介绍如何将Jenkins和Argocd结合起来实现CI/CD","id":4,"section":"posts","tags":["devops","jenkins","argocd"],"title":"使用Jenkins和Argocd实现CI/CD","uri":"https://www.coolops.cn/posts/devops-jenkins-argocd/"},{"content":"Vault 是用于处理和加密整个基础架构秘钥的中心管理服务。Vault 通过 secret 引擎管理所有的秘钥，Vault 有一套 secret 引擎可以使用。\n其主要有以下功能：\n 安全密钥存储：任意的key/value Secret都可以存储到Vault中，Vault会对这些Secret进行加密并持久化存储。后端存储支持本地磁盘、cosul等； 动态密钥：Vault可以动态生成Secret，在租约到期后会自动撤销它们； 数据加密：Vault可以加密和解密数据，安全团队可以自定义加密参数； 租赁和续订：Vault 中的所有机密都有与其关联的租约。在租约结束时，Vault 将自动撤销该机密。客户端可以通过内置续订 API 续订租约； 吊销：Vault具有对秘密吊销的内置支持。Vault 可以撤消单个机密，还可以撤销一个机密树，例如由特定用户读取的所有机密或特定类型的所有机密。在发生入侵时，吊销有助于关键滚动和锁定系统；  安装 在Linux主机上安装 在Linux主机上安装比较简单，只需要下面三步：\n# 安装包管理工具\r$ sudo yum install -y yum-utils\r# 添加源\r$ sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo\r# 安装vault\r$ sudo yum -y install vault\r在K8S中安装 vault提供了helm包，可以使用helm进行安装。\n 版本说明：\n Helm 3.0+ Kubernetes 1.9+   # 添加repo仓库\r$ helm repo add hashicorp https://helm.releases.hashicorp.com\r# 更新本地仓库\r$ helm repo update\r# 安装vault\r$ helm install vault hashicorp/vault\r起服务端  这里仅针对主机上安装的vault，在K8S集群中使用helm安装的vault默认已经起了服务端了。\n 这里已经在主机上安装了vault。\n$ vault version\rVault v1.6.1 (6d2db3f033e02e70202bef9ec896360062b88b03)\r然后以开发默认运行一个Vault服务端，正式环境不用开发模式。\n$ vault server -dev -dev-listen-address=0.0.0.0:8200 \u0026amp;\r......\rWARNING! dev mode is enabled! In this mode, Vault runs entirely in-memory\rand starts unsealed with a single unseal key. The root token is already\rauthenticated to the CLI, so you can immediately begin using Vault.\rYou may need to set the following environment variable:\r$ export VAULT_ADDR='http://0.0.0.0:8200'\rThe unseal key and root token are displayed below in case you want to\rseal/unseal the Vault or re-authenticate.\rUnseal Key: killR+cPfTR7P7HoYRt5SsMySMDv2w9WD7ljcxpXB+Q=\rRoot Token: s.pd4FBsC1pamE21nLv3fszdI1\rDevelopment mode should NOT be used in production installations\r然后可以通过http://ip:8200/ui进行访问。\n填入生成的Token，即可登录。\n配置K8S与Vault通信 要使K8S能正常读取Vault中的Secret，则必须保证K8S和Vault能正常通信。\n PS：我这里是采用Kubeadm安装的K8S集群，版本1.18.9\n （1）添加环境变量，其中IP地址根据实际情况填写\n$ export VAULT_ADDR=http://192.168.0.153:8200\r（2）开启K8S认证方式\n$ vault auth enable kubernetes\rSuccess! Enabled kubernetes auth method at: kubernetes/\r（3）添加K8S集群配置信息\n$ vault write auth/kubernetes/config \\\rkubernetes_host=https://192.168.0.153:6443 \\\rkubernetes_ca_cert=@/etc/kubernetes/pki/ca.crt\rSuccess! Data written to: auth/kubernetes/config\r（4）创建权限策略\n$ cat \u0026lt;\u0026lt;EOF | vault policy write vault-demo-policy -\r\u0026gt; path \u0026quot;sys/mounts\u0026quot; { capabilities = [\u0026quot;read\u0026quot;] }\r\u0026gt; path \u0026quot;secret/data/demo/*\u0026quot; { capabilities = [\u0026quot;read\u0026quot;] }\r\u0026gt; path \u0026quot;secret/metadata/demo/*\u0026quot; { capabilities = [\u0026quot;list\u0026quot;] }\r\u0026gt; EOF\rSuccess! Uploaded policy: vault-demo-policy\r创建一个用于演示的demo策略。\n（5）创建一个认证角色\n$ vault write auth/kubernetes/role/vault-demo-role \\\r\u0026gt; bound_service_account_names=vault-serviceaccount \\\r\u0026gt; bound_service_account_namespaces=default \\\r\u0026gt; policies=vault-demo-policy \\\r\u0026gt; ttl=1h\rSuccess! Data written to: auth/kubernetes/role/vault-demo-role\r角色名是vault-demo-role，认证方式是RBAC认证，绑定的用户是vault-serviceaccount，策略是vault-demo-policy。\n（6）创建密钥\n$ vault kv put secret/demo/database username=\u0026quot;coolops\u0026quot; password=123456\rKey Value\r--- -----\rcreated_time 2021-01-25T08:22:35.134166877Z\rdeletion_time n/a\rdestroyed false\rversion 1\r# 查看\r$ vault kv get secret/demo/database\r====== Metadata ======\rKey Value\r--- -----\rcreated_time 2021-01-25T08:22:35.134166877Z\rdeletion_time n/a\rdestroyed false\rversion 1\r====== Data ======\rKey Value\r--- -----\rpassword 123456\rusername coolops\r（7）在K8S集群中创建RBAC权限\n---\rapiVersion: v1\rkind: ServiceAccount\rmetadata:\rname: vault-serviceaccount\r---\rapiVersion: rbac.authorization.k8s.io/v1beta1\rkind: ClusterRoleBinding\rmetadata:\rname: vault-clusterrolebinding\rroleRef:\rapiGroup: rbac.authorization.k8s.io\rkind: ClusterRole\rname: system:auth-delegator\rsubjects:\r- kind: ServiceAccount\rname: vault-serviceaccount\rnamespace: default\r---\rkind: Role\rapiVersion: rbac.authorization.k8s.io/v1\rmetadata:\rname: vault-secretadmin-role\rrules:\r- apiGroups: [\u0026quot;\u0026quot;]\rresources: [\u0026quot;secrets\u0026quot;]\rverbs: [\u0026quot;*\u0026quot;]\r---\rkind: RoleBinding\rapiVersion: rbac.authorization.k8s.io/v1\rmetadata:\rname: vault-secretadmin-rolebinding\rsubjects:\r- kind: ServiceAccount\rname: vault-serviceaccount\rroleRef:\rkind: Role\rname: vault-secretadmin-role\rapiGroup: rbac.authorization.k8s.io\r创建RBAC配置文件\n$ kubectl apply -f rbac.yaml serviceaccount/vault-serviceaccount created\rclusterrolebinding.rbac.authorization.k8s.io/vault-clusterrolebinding created\rrole.rbac.authorization.k8s.io/vault-secretadmin-role created\rrolebinding.rbac.authorization.k8s.io/vault-secretadmin-rolebinding created\r在K8S中使用Vault中的Secret 要获取到Vault中的Secret，有两种方式：\n 使用vault agent在initContainer中将secret取出来 使用vault SDK在程序中获取  使用initContainer方式  下面是官方的一个demo。\n 流程图如下：\n（1）创建ConfigMap\napiVersion: v1\rdata:\rvault-agent-config.hcl: |\r# Comment this out if running as sidecar instead of initContainer\rexit_after_auth = true\rpid_file = \u0026quot;/home/vault/pidfile\u0026quot;\rauto_auth {\rmethod \u0026quot;kubernetes\u0026quot; {\rmount_path = \u0026quot;auth/kubernetes\u0026quot;\rconfig = {\rrole = \u0026quot;vault-demo-role\u0026quot;\r}\r}\rsink \u0026quot;file\u0026quot; {\rconfig = {\rpath = \u0026quot;/home/vault/.vault-token\u0026quot;\r}\r}\r}\rtemplate {\rdestination = \u0026quot;/etc/secrets/index.html\u0026quot;\rcontents = \u0026lt;\u0026lt;EOT\r\u0026lt;html\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;Some secrets:\u0026lt;/p\u0026gt;\r{{- with secret \u0026quot;secret/demo/database\u0026quot; }}\r\u0026lt;ul\u0026gt;\r\u0026lt;li\u0026gt;\u0026lt;pre\u0026gt;username: {{ .Data.data.username }}\u0026lt;/pre\u0026gt;\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\u0026lt;pre\u0026gt;password: {{ .Data.data.password }}\u0026lt;/pre\u0026gt;\u0026lt;/li\u0026gt;\r\u0026lt;/ul\u0026gt;\r{{ end }}\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\rEOT\r}\rkind: ConfigMap\rmetadata:\rname: example-vault-agent-config\rnamespace: default\rtemplate允许将Vault里保存的Secret保存到文件。\n创建pod\napiVersion: v1\rkind: Pod\rmetadata:\rname: vault-agent-example\rnamespace: default\rspec:\rserviceAccountName: vault-serviceaccount volumes:\r- configMap:\ritems:\r- key: vault-agent-config.hcl\rpath: vault-agent-config.hcl\rname: example-vault-agent-config\rname: config\r- emptyDir: {}\rname: shared-data\rinitContainers:\r- args:\r- agent\r- -config=/etc/vault/vault-agent-config.hcl\r- -log-level=debug\renv:\r- name: VAULT_ADDR\rvalue: http://192.168.0.153:8200\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/vault:1.6.1 name: vault-agent\rvolumeMounts:\r- mountPath: /etc/vault\rname: config\r- mountPath: /etc/secrets\rname: shared-data\rcontainers:\r- image: nginx\rname: nginx-container\rports:\r- containerPort: 80\rvolumeMounts:\r- mountPath: /usr/share/nginx/html\rname: shared-data\r 注意serviceAccountName需和之前配置的保持一致\n 待pod运行后，可以正常获取到vault里的Secret，如下：\n$ kubectl get po -o wide\rNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES\rnfs-client-prosioner-598d477ff6-fmgwf 1/1 Running 8 65d 172.16.7.140 ecs-968f-0005 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rtraefik-5b8bb6787-dn96j 1/1 Running 0 65d 172.16.7.138 ecs-968f-0005 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rvault-agent-example 1/1 Running 0 106s 172.16.235.231 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\r$ curl 172.16.235.231\r\u0026lt;html\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;Some secrets:\u0026lt;/p\u0026gt;\r\u0026lt;ul\u0026gt;\r\u0026lt;li\u0026gt;\u0026lt;pre\u0026gt;username: coolops\u0026lt;/pre\u0026gt;\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\u0026lt;pre\u0026gt;password: 123456\u0026lt;/pre\u0026gt;\u0026lt;/li\u0026gt;\r\u0026lt;/ul\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r使用SDK方式 package main\rimport (\r\u0026quot;fmt\u0026quot;\r\u0026quot;io/ioutil\u0026quot;\rvaultApi \u0026quot;github.com/hashicorp/vault/api\u0026quot;\r)\rvar (\rvaultHost string\rvaultCAPath string\rvaultServiceAccount string\rvaultJWTPath string\r)\rfunc main() {\r// K8S的token\rvaultJWTPath = \u0026quot;/var/run/secrets/kubernetes.io/serviceaccount/token\u0026quot;\r// sa名字\rvaultServiceAccount = \u0026quot;vault-serviceaccount\u0026quot;\rtlsConfig := \u0026amp;vaultApi.TLSConfig{\rCACert: vaultCAPath,\rInsecure: false,\r}\rconfig := vaultApi.DefaultConfig()\r// vault地址\rconfig.Address = fmt.Sprintf(\u0026quot;https://%s\u0026quot;, vaultHost)\rconfig.ConfigureTLS(tlsConfig)\rclient, _ := vaultApi.NewClient(config)\rbuf, _ := ioutil.ReadFile(vaultJWTPath)\rjwt := string(buf)\roptions := map[string]interface{}{\r\u0026quot;jwt\u0026quot;: jwt,\r\u0026quot;role\u0026quot;: vaultServiceAccount,\r}\rloginSecret, _ := client.Logical().Write(\u0026quot;auth/kubernetes/login\u0026quot;, options)\rclient.SetToken(loginSecret.Auth.ClientToken)\r// secret地址\rsecret, _ := client.Logical().Read(\u0026quot;database/creds/tx\u0026quot;)\rfmt.Println(secret)\r}\r最后 Vault是一个很好的工具，可以相对安全的管理一些敏感信息，不过通过上面的步骤可以看到配置相对复杂，维护成本相对较高，不过Kubernetes和Vault集成依旧是一个不错的方案。\n参考：\n https://github.com/hashicorp/vault https://github.com/hashicorp/vault-helm https://www.vaultproject.io/docs/agent https://www.vaultproject.io/docs/agent/template https://learn.hashicorp.com/tutorials/vault/agent-kubernetes https://medium.com/getamis/vault-kubernetes-integration-63ce46d47550  ","description":"本文主要介绍在kubernetes中，如何和vault进行集成，安全管理vault","id":5,"section":"posts","tags":["kubernetes","vault"],"title":"kubernetes和vault进行集成，管理Secret","uri":"https://www.coolops.cn/posts/kubernetes-vault-secret/"},{"content":"为何要加密？ 在Kubernetes中，Secret是用来帮我们存储敏感信息的，比如密码、证书等，但是在默认的情况下，Secret只是做了简单的base64编码，任何人都可以非常容易的对其进行解密获取到原始数据。\n比如通过以下方法生成一个secret对象：\n$ echo -n \u0026quot;coolops\u0026quot; | kubectl create secret generic mysecret --dry-run --from-file=secret=/dev/stdin -o yaml \u0026gt; secret.yaml\r$ cat secret.yaml apiVersion: v1\rdata:\rsecret: Y29vbG9wcw==\rkind: Secret\rmetadata:\rcreationTimestamp: null\rname: mysecret\r其他人只要拿到secret的值，就可以对其进行解密获取到真实数据，如下：\n$ echo \u0026quot;Y29vbG9wcw==\u0026quot; | base64 -d\rcoolops\r这样就非常的不安全，有点\u0026quot;掩耳盗铃\u0026quot;的意思。\n在Kubernetes集群中，Etcd是集群数据库，存储着集群所以的资源数据，其中也包括Secrets，所以拿下了这个数据库就等于拿下了整个集群，所以在生产环境中对其进行加密是非常有必要的。\n如何进行加密？ 静态加密 kubernetes 1.13版本之后，提供静态加密方式，其主要是通过kube-apiserver来控制Secrets的加解密，而在Etcd中存储的是加密后的信息，所以攻击者拿下了etcd，也无法轻易的拿到Secrets保存的敏感数据。\n 当前集群是使用kubeadm安装，版本1.18.9\n （1）创建加密配置文件，保存到master节点/etc/kubernetes/pki/static-secret-encryption.yaml中\napiVersion: apiserver.config.k8s.io/v1\rkind: EncryptionConfiguration\rresources:\r- resources:\r- secrets\rproviders:\r- aescbc:\rkeys:\r- name: mysecret\rsecret: DJqYaMMpY2DNlHz+HYrFYOUSh5SXKWiVOwLf6nQX9ss=\r- identity: {}\r其中secret是加密密钥，使用如下命令生成：\n$ head -c 32 /dev/urandom | base64\r（2）修改kube-apiserver启动参数，位置/etc/kubernetes/manifests/kube-apiserver.yaml\n......\rspec:\rcontainers:\r- command:\r- kube-apiserver\r- --encryption-provider-config=/etc/kubernetes/pki/static-secret-encryption.yaml\r......  注意：kube-apiserver的加密参数，在1.14之后是\u0026ndash;encryption-provider-config\n （3）重启kube-apiserver\n（4）验证加密\n首先创建一个secret资源，如下：\n$ kubectl create secret generic secret1 -n default --from-literal=mykey=mydata\r然后查看etcd中的数据\n$ ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get /registry/secrets/default/secret1 | hexdump -C\r00000000 2f 72 65 67 69 73 74 72 79 2f 73 65 63 72 65 74 |/registry/secret|\r00000010 73 2f 64 65 66 61 75 6c 74 2f 73 65 63 72 65 74 |s/default/secret|\r00000020 31 0a 6b 38 73 3a 65 6e 63 3a 61 65 73 63 62 63 |1.k8s:enc:aescbc|\r00000030 3a 76 31 3a 6d 79 73 65 63 72 65 74 3a b6 90 7d |:v1:mysecret:..}|\r00000040 b3 40 9b 4d 63 eb 71 cf ca 16 52 a0 91 82 c3 69 |.@.Mc.q...R....i|\r00000050 2b 6f e5 1c ae 88 58 0d 4f 08 c9 29 57 69 f5 e6 |+o....X.O..)Wi..|\r00000060 e2 7c 42 79 bb 84 22 3a 90 54 5e d4 ac f6 a6 a6 |.|By..\u0026quot;:.T^.....|\r00000070 47 e2 b2 67 29 d8 c4 c6 61 9e 84 62 9d 3c 7c b0 |G..g)...a..b.\u0026lt;|.|\r00000080 6c 5f 5e f3 da 08 34 17 ef a3 a7 9c 76 31 02 98 |l_^...4.....v1..|\r00000090 54 5c 21 05 af 8d a8 dc 39 04 4d 84 bf a8 d1 f6 |T\\!.....9.M.....|\r000000a0 58 f4 90 30 22 46 14 a5 e6 19 3a 51 48 86 99 a7 |X..0\u0026quot;F....:QH...|\r000000b0 ed f1 5f 8e 4a 1c 30 cb 5f ec ba 3d e2 0a 1d 93 |.._.J.0._..=....|\r000000c0 7c 57 68 6b d2 01 51 49 fd 81 56 72 6d ca 98 e6 ||Whk..QI..Vrm...|\r000000d0 99 59 84 15 bc 5d 7d f7 95 75 b2 cb 4f ff 8d d1 |.Y...]}..u..O...|\r000000e0 ae 29 0d 27 df fa 59 b4 e2 37 2c 33 83 9e e4 73 |.).'..Y..7,3...s|\r000000f0 55 ce 89 cc c0 5f 3d e4 df 90 8d 70 91 f9 81 b1 |U...._=....p....|\r00000100 e7 0c ee 71 cf 81 22 6f 6c 45 74 51 0c f7 5f 4d |...q..\u0026quot;olEtQ.._M|\r00000110 1f 9a be 51 05 cd fd b2 74 0b 29 30 e2 24 ea 57 |...Q....t.)0.$.W|\r00000120 0e 8a cb 1f 55 7a 2e 4c 0e 1a 4e 09 c6 0a |....Uz.L..N...|\r0000012e\r注意在数据头部出现k8s:enc:aescbc:v1:,说明数据已经被正确加密，使用的是aescbc算法，使用的密钥为mysecret。\n接下来查看读取的时候能否正常被解密，如下：\n$ kubectl get secrets secret1 -o yaml\rapiVersion: v1\rdata:\rmykey: bXlkYXRh\rkind: Secret\rmetadata:\rcreationTimestamp: \u0026quot;2021-01-22T02:27:48Z\u0026quot;\rmanagedFields:\r- apiVersion: v1\rfieldsType: FieldsV1\rfieldsV1:\rf:data:\r.: {}\rf:mykey: {}\rf:type: {}\rmanager: kubectl\roperation: Update\rtime: \u0026quot;2021-01-22T02:27:48Z\u0026quot;\rname: secret1\rnamespace: default\rresourceVersion: \u0026quot;26907503\u0026quot;\rselfLink: /api/v1/namespaces/default/secrets/secret1\ruid: 9020c914-3785-404f-a7b2-0743ff49c19d\rtype: Opaque\r$ echo \u0026quot;bXlkYXRh\u0026quot; | base64 -d\rmydata\r可以发现能否正常解密。\n不知道你有没有发现，只有存在etcd里的数据被加密了，我们在集群使用kubectl get secrets secret1 -o yaml获取到的仍然只是简单的进行了base64转码，所以一旦我们的节点被攻破，secrets也就暴露在外面了。\n密钥管理服务KMS plugin KMS 加密驱动使用封套加密模型来加密 etcd 中的数据。 数据使用数据加密秘钥（DEK）加密；每次加密都生成一个新的 DEK。 这些 DEK 经一个秘钥加密秘钥（KEK）加密后在一个远端的 KMS 中存储和管理。 KMS 驱动使用 gRPC 与一个特定的 KMS 插件通信。这个 KMS 插件作为一个 gRPC 服务器被部署在 Kubernetes 主服务器的同一个主机上，负责与远端 KMS 的通信。\n现在基本云厂商都提供KMS服务。这里以阿里云为例。\n（1）开通密钥管理服务\n（2）创建密钥\n（3）然后部署kms-plugin\napiVersion: apps/v1\rkind: Deployment metadata:\rname: ack-kms-plugin\rnamespace: kube-system\rspec:\rselector:\rmatchLabels:\rname: ack-kms-plugin\rtemplate:\rmetadata:\rlabels:\rname: ack-kms-plugin\rspec:\raffinity:\rnodeAffinity:\rpreferredDuringSchedulingIgnoredDuringExecution:\r- preference: {}\rweight: 100\rrequiredDuringSchedulingIgnoredDuringExecution:\rnodeSelectorTerms:\r- matchExpressions:\r- key: node\roperator: In\rvalues:\r- master\rrestartPolicy: Always\rtolerations:\r- key: node-role.kubernetes.io/master\reffect: NoSchedule\rvolumes:\r- name: kmssocket\rhostPath:\rpath: /var/run/kmsplugin\rtype: DirectoryOrCreate\rcontainers:\r- name: ack-kms-plugin\rimage: registry.{{ .Region }}.aliyuncs.com/acs/ack-kms-plugin:v1.0.2\rimagePullPolicy: Always\rcommand:\r- ack-kms-plugin\r- --gloglevel=5\r- --key-id={{ .KeyId }}\r- --path-to-unix-socket=/var/run/kmsplugin/grpc.sock\rlivenessProbe:\rexec:\rcommand:\r- ack-kms-plugin\r- health\r- --path-to-unix-socket=/var/run/kmsplugin/grpc.sock\rinitialDelaySeconds: 30\rfailureThreshold: 3\rtimeoutSeconds: 5\rperiodSeconds: 300\renv:\r- name: ACCESS_KEY_ID #not required if you want plugin help to pull the sts credentials\rvalue: {{ .AK }}\r- name: ACCESS_KEY_SECRET #not required if you want plugin help to pull the sts credentials\rvalue: {{ .AK_Secret }}\r- name: CREDENTIAL_INTERVAL #not required if you want plugin help to pull the sts credentials\rvalue: {{ .Credential_Interval }}\rvolumeMounts:\r- name: kmssocket\rmountPath: /var/run/kmsplugin\rreadOnly: false\r其中需要更改的地方：\n {{ .Region }}：阿里巴巴云区域 ID， 如果您的群集部署在 ECS 上， 您可以通过curl http://100.100.100.200/latest/meta-data/region-id {{ .KeyId }}：Kms 服务列表中用于秘密加密的阿里云 Kms 密钥 ID {{ .AK }}：授权的角色ID {{ .AK_Secret }}：授权的角色密钥 {{ .Credential_Interval }}：凭据轮询间隔  （4）创建kms插件配置文件/etc/kubernetes/kmsplugin/encryptionconfig.yaml\napiVersion: apiserver.config.k8s.io/v1\rkind: EncryptionConfiguration\rresources:\r- resources:\r- secrets\rproviders:\r- kms:\rname: grpc-kms-provider\rendpoint: unix:///var/run/kmsplugin/grpc.sock\rcachesize: 1000\rtimeout: 3s\r- identity: {}\r（5）修改kube-apiserver配置清单文件/etc/kubernetes/manifests/kube-apiserver.yaml\n在启动参数里加入：\n......\rspec:\rcontainers:\r- command:\r- kube-apiserver\r- --encryption-provider-config=/etc/kubernetes/kmsplugin/encryptionconfig.yaml\r......\r然后再配置挂载，如下：\n...\rvolumeMounts:\r- name: kms-sock\rmountPath: /var/run/kmsplugin\r- name: kms-config\rmountPath: /etc/kubernetes/kmsplugin\r...\rvolumes:\r- name: kms-sock\rhostPath:\rpath: /var/run/kmsplugin\r- name: kms-config\rhostPath:\rpath: /etc/kubernetes/kmsplugin\r（6）重启kube-apiserver\n（7）验证\n现在，群集应使用信封加密方案，使用阿里云 KMS 的给定密钥加密密钥 （KEK） 对 中的秘密进行加密\n\\1. 创建新机密\n$ kubectl create secret generic secret1 -n default --from-literal=mykey=mydata\r\\2. 使用 etcdtl，读取主节点中的 etcd中的机密：\nps： [.local-ip] 应替换为主节点 IP 之一。\n$ ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.pem --cert=/etc/kubernetes/pki/etcd/etcd-client.pem --key=/etc/kubernetes/pki/etcd/etcd-client-key.pem --endpoints=https://{{.local-ip}}:2379 get /registry/secrets/default/secret1\r\\3. 验证存储的机密是否前缀，指示我们的 kms 提供商已加密生成的数据。k8s:enc:kms:v1:grpc-kms-provider\n\\4. 验证密钥是否已正确解密：\n$ kubectl get secrets secret1 -o yaml\r使用第三方插件 还可以使用其他插件，比如sealed-secrets和vault。\n参考文件：\n https://kubernetes.io/zh/docs/tasks/administer-cluster/encrypt-data/ https://kubernetes.io/zh/docs/tasks/administer-cluster/kms-provider/ https://github.com/AliyunContainerService/ack-kms-plugin https://github.com/hashicorp/vault https://github.com/bitnami-labs/sealed-secrets  ","description":"本文主要介绍如何在K8S中安全管理Secret","id":6,"section":"posts","tags":["kubernetes","secret","KMS"],"title":"在K8S中，如何安全管理Secrets?","uri":"https://www.coolops.cn/posts/kubernetes-secret-manager/"},{"content":"容器为我们提供应用运行环境，所以在制作镜像的时候应该重点关注镜像安全问题，而不是随意制作。\n这里总结以下几点：\n 以非root用户运行 保持镜像最小化 对镜像进行安全扫描  以非root用户运行镜像 在制作镜像的时候，默认都是以root用户在容器中运行，虽然此root用户和服务器上的root用户有区别，但是黑客可以利用这个用户来获取API密钥，令牌，密码等私密信息，或者通过提权等干扰服务器主机。\n所以我们在制作镜像的时候要使用非root用户，比如下面一个java服务：\nFROM openjdk:8-jdk-oraclelinux7\rRUN useradd -m -d /home/joker -u 1000 -U joker \u0026amp;\u0026amp; \\\ryum install -y procps\rUSER joker\rADD --chown=joker springboot-helloworld.jar /home/joker/app.jar\rEXPOSE 8080\rWORKDIR /home/joker\rCMD exec java -Djava.security.egd=file:/dev/./urandom -jar app.jar\r运行容器后，即可看到是以joker用户起的应用。保持镜像最小化 镜像并非越大越好，也并非越小越好，适合当前需求的镜像才是最好。不过在制作镜像的时候要坚持最小化原则，只安装需要的软件和包。\n可以通过docker images 查看镜像大小，如下：\nubuntu latest f643c72bc252 7 weeks ago 72.9MB\r对镜像进行安全扫描 把容器类比于主机，那么容器的安全和主机的安全同等重要，所以要镜像对每一个镜像进行安全扫描，发现高危漏洞及时打补丁，这样才能随时保持一个相对安全的运行环境。\n目前Harbor 2.0以上已经可以自定义镜像扫描规则，也可以定义拦截规则，可以有效的发现镜像漏洞，其他公有仓库也有类似的功能。当然我们还可以在做CI的阶段对制品镜像进行扫描，通过的才交付。具体怎么做根据不同的需求而定。\n","description":"本文主要介绍如果来保证Docker镜像的安全","id":7,"section":"posts","tags":["docker"],"title":"如何保证Docker镜像安全?","uri":"https://www.coolops.cn/posts/docker-image-secrut/"},{"content":"前提：\n 可用的K8S集群 安装好Helm3  （1）添加helm仓库 helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\rhelm repo update\r（2）下载prometheus-stack包 helm pull prometheus-community/kube-prometheus-stack\rtar xf kube-prometheus-stack.tar.gz\r（3）创建namespace kubectl create ns monitoring\r（4）创建storageclass  PS：这里采用的local storageclass，磁盘是ssd磁盘\n altermanager-storage.yaml\napiVersion: storage.k8s.io/v1\rkind: StorageClass\rmetadata:\rname: local-storage-alertmanager\rprovisioner: kubernetes.io/no-provisioner\rreclaimPolicy: Delete\rvolumeBindingMode: WaitForFirstConsumer\r---\rapiVersion: v1\rkind: PersistentVolume\rmetadata:\rfinalizers:\r- kubernetes.io/pv-protection\rname: local-pv-alertmanager\rspec:\raccessModes:\r- ReadWriteOnce\rcapacity:\rstorage: 50Gi\rlocal:\rpath: /opt/hipay/lib/altermanager\rnodeAffinity:\rrequired:\rnodeSelectorTerms:\r- matchExpressions:\r- key: category\roperator: In\rvalues:\r- monitoring\rpersistentVolumeReclaimPolicy: Delete\rstorageClassName: local-storage-alertmanager\rvolumeMode: Filesystem\rgrafana-storage.yaml\napiVersion: storage.k8s.io/v1\rkind: StorageClass\rmetadata:\rname: local-storage-grafana\rprovisioner: kubernetes.io/no-provisioner\rreclaimPolicy: Delete\rvolumeBindingMode: WaitForFirstConsumer\r---\rapiVersion: v1\rkind: PersistentVolume\rmetadata:\rfinalizers:\r- kubernetes.io/pv-protection\rname: local-grafana-pv\rspec:\raccessModes:\r- ReadWriteOnce\rcapacity:\rstorage: 20Gi\rlocal:\rpath: /opt/hipay/lib/grafana\rnodeAffinity:\rrequired:\rnodeSelectorTerms:\r- matchExpressions:\r- key: category\roperator: In\rvalues:\r- monitoring\rpersistentVolumeReclaimPolicy: Delete storageClassName: local-storage-grafana\rvolumeMode: Filesystem\rprometheus-storage.yaml\napiVersion: storage.k8s.io/v1\rkind: StorageClass\rmetadata:\rname: local-storage-prometheus\rprovisioner: kubernetes.io/no-provisioner\rreclaimPolicy: Delete\rvolumeBindingMode: WaitForFirstConsumer\r---\rapiVersion: v1\rkind: PersistentVolume\rmetadata:\rfinalizers:\r- kubernetes.io/pv-protection\rname: local-pv-prometheus\rspec:\raccessModes:\r- ReadWriteOnce\rcapacity:\rstorage: 400Gi\rlocal:\rpath: /opt/hipay/lib/prometheus nodeAffinity:\rrequired:\rnodeSelectorTerms:\r- matchExpressions:\r- key: category\roperator: In\rvalues:\r- monitoring\rpersistentVolumeReclaimPolicy: Delete\rstorageClassName: local-storage-prometheus\rvolumeMode: Filesystem\r其实创建一个Storageclass就足够了，不过我这里为了便与区分，每一个服务都创建了一个，无所谓了\u0026hellip;\u0026hellip;.\n（5）创建自定义配置文件 my-values.yaml\nalertmanager:\rconfig:\rglobal:\rresolve_timeout: 5m\rtemplates:\r- '/etc/alertmanager/config/*.tmpl'\rroute:\rgroup_by: ['job']\rgroup_wait: 30s\rgroup_interval: 5m\rrepeat_interval: 12h\rreceiver: webhook\rroutes:\r- match:\ralertname: Watchdog\rreceiver: 'webhook'\rreceivers:\r- name: webhook\rwebhook_configs:\r- url: \u0026quot;http://prometheus-alter-webhook.monitoring.svc:9000\u0026quot;\rsend_resolved: false\ringress:\renabled: true\rhosts:\r- altermanager.coolops.cn\ralertmanagerSpec:\rimage:\rrepository: quay.io/prometheus/alertmanager\rtag: v0.21.0\rsha: \u0026quot;\u0026quot;\rreplicas: 1\rstorage:\rvolumeClaimTemplate:\rspec:\rstorageClassName: local-storage-alertmanager\raccessModes: [\u0026quot;ReadWriteOnce\u0026quot;]\rresources:\rrequests:\rstorage: 50Gi\raffinity: nodeAffinity:\rrequiredDuringSchedulingIgnoredDuringExecution:\rnodeSelectorTerms:\r- matchExpressions:\r- key: category\roperator: In\rvalues:\r- monitoring\rgrafana:\radminPassword: dmCE9M$PQt@Q%eht\ringress:\renabled: true\rhosts:\r- grafana.coolops.cn\rpersistence:\rtype: pvc\renabled: true\rstorageClassName: local-storage-grafana\raccessModes:\r- ReadWriteOnce\rsize: 20Gi\rprometheusOperator:\raffinity: nodeAffinity:\rrequiredDuringSchedulingIgnoredDuringExecution:\rnodeSelectorTerms:\r- matchExpressions:\r- key: category\roperator: In\rvalues:\r- monitoring\rprometheus:\ringress:\renabled: true\rhosts:\r- prometheus.coolops.cn\rprometheusSpec:\raffinity: nodeAffinity:\rrequiredDuringSchedulingIgnoredDuringExecution:\rnodeSelectorTerms:\r- matchExpressions:\r- key: category\roperator: In\rvalues:\r- monitoring\rstorageSpec: volumeClaimTemplate:\rspec:\rstorageClassName: local-storage-prometheus\raccessModes: [\u0026quot;ReadWriteOnce\u0026quot;]\rresources:\rrequests:\rstorage: 400Gi\r PS：这里使用了nodeAffinity做高级调度，所以需要给Node节点打标签\nkubectl label node xxxxx category=monitoring\n （6）安装 helm install prometheus -n monitoring kube-prometheus-stack -f kube-prometheus-stack/my-values.yaml\r然后可以在monitoring的namespace下看到创建的如下信息\n# kubectl get all -n monitoring NAME READY STATUS RESTARTS AGE\rpod/alertmanager-prometheus-kube-prometheus-alertmanager-0 2/2 Running 0 4d20h\rpod/prometheus-grafana-6d644ff97b-hxdxq 2/2 Running 0 4d20h\rpod/prometheus-kube-prometheus-operator-db6d5c564-nwm82 1/1 Running 0 4d20h\rpod/prometheus-kube-state-metrics-c65b87574-4qx95 1/1 Running 0 4d20h\rpod/prometheus-prometheus-kube-prometheus-prometheus-0 2/2 Running 1 4d20h\rpod/prometheus-prometheus-node-exporter-2t8pt 1/1 Running 0 4d20h\rpod/prometheus-prometheus-node-exporter-52bj4 1/1 Running 0 4d20h\rpod/prometheus-prometheus-node-exporter-xwsbw 1/1 Running 0 4d20h\rNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rservice/alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,9094/TCP,9094/UDP 4d20h\rservice/prometheus-alter-webhook ClusterIP 192.168.12.211 \u0026lt;none\u0026gt; 9000/TCP 2d18h\rservice/prometheus-grafana ClusterIP 192.168.11.93 \u0026lt;none\u0026gt; 80/TCP 4d20h\rservice/prometheus-kube-prometheus-alertmanager ClusterIP 192.168.11.95 \u0026lt;none\u0026gt; 9093/TCP 4d20h\rservice/prometheus-kube-prometheus-operator ClusterIP 192.168.10.102 \u0026lt;none\u0026gt; 443/TCP 4d20h\rservice/prometheus-kube-prometheus-prometheus ClusterIP 192.168.7.216 \u0026lt;none\u0026gt; 9090/TCP 4d20h\rservice/prometheus-kube-state-metrics ClusterIP 192.168.13.249 \u0026lt;none\u0026gt; 8080/TCP 4d20h\rservice/prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 4d20h\rservice/prometheus-prometheus-node-exporter ClusterIP 192.168.8.215 \u0026lt;none\u0026gt; 9100/TCP 4d20h\rNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE\rdaemonset.apps/prometheus-prometheus-node-exporter 15 15 15 15 15 \u0026lt;none\u0026gt; 4d20h\rNAME READY UP-TO-DATE AVAILABLE AGE\rdeployment.apps/prometheus-alert-webhook 1/1 1 1 2d18h\rdeployment.apps/prometheus-grafana 1/1 1 1 4d20h\rdeployment.apps/prometheus-kube-prometheus-operator 1/1 1 1 4d20h\rdeployment.apps/prometheus-kube-state-metrics 1/1 1 1 4d20h\rNAME DESIRED CURRENT READY AGE\rreplicaset.apps/prometheus-alert-webhook-7bd7766977 1 1 1 2d18h\rreplicaset.apps/prometheus-grafana-6d644ff97b 1 1 1 4d20h\rreplicaset.apps/prometheus-kube-prometheus-operator-db6d5c564 1 1 1 4d20h\rreplicaset.apps/prometheus-kube-state-metrics-c65b87574 1 1 1 4d20h\rNAME READY AGE\rstatefulset.apps/alertmanager-prometheus-kube-prometheus-alertmanager 1/1 4d20h\rstatefulset.apps/prometheus-prometheus-kube-prometheus-prometheus 1/1 4d20h\r然后就可以通过域名进行访问了。\n（7）其他配置 自定义告警规则 如果要自定义监控告警规则，可以直接在my-values.yaml添加additionalPrometheusRules字段，如下：\n## 自定义告警规则\radditionalPrometheusRules:\r- name: blackbox-monitoring-rule\rgroups:\r- name: blackbox_check\rrules:\r- alert: \u0026quot;ssl证书过期警告\u0026quot;\rexpr: (probe_ssl_earliest_cert_expiry - time())/86400 \u0026lt;30\rfor: 1h\rlabels:\rseverity: warn\rannotations:\rdescription: '域名{{$labels.instance}}的证书还有{{ printf \u0026quot;%.1f\u0026quot; $value }}天就过期了,请尽快更新证书'\rsummary: \u0026quot;ssl证书过期警告\u0026quot;\r- alert: \u0026quot;接口/主机/端口 可用性异常\u0026quot;\rexpr: probe_success == 0\rfor: 1m\rlabels:\rseverity: critical\rannotations:\rsummary: \u0026quot;接口/主机/端口异常检测\u0026quot;\rdescription: \u0026quot;接口/主机/端口 {{ $labels.instance }} 无法联通\u0026quot;\r自定义job_name 如果要自定义job抓取规则，则prometheus.prometheusSpec字段下新增additionalScrapeConfigs，如下：\nprometheus:\r......\rprometheusSpec:\r......\radditionalScrapeConfigs:\r- job_name: \u0026quot;ingress-endpoint-status\u0026quot;\rmetrics_path: /probe\rparams:\rmodule: [http_2xx] # Look for a HTTP 200 response.\rstatic_configs:\r- targets:\r- http://172.16.51.23/healthz\r- http://172.16.51.24/healthz\r- http://172.16.51.25/healthz\rlabels:\rgroup: nginx-ingress\rrelabel_configs:\r- source_labels: [__address__]\rtarget_label: __param_target\r- source_labels: [__param_target]\rtarget_label: instance\r- target_label: __address__\rreplacement: blackbox.monitoring:9115\r自定义serviceMonitor 如果要自定义serviceMonitor监控，则直接在prometheus下新增additionalServiceMonitors，如下：\nprometheus:\r......\r## 自定义监控\radditionalServiceMonitors:\r- name: ingress-nginx-controller\rselector:\rmatchLabels:\rapp: ingress-nginx-metrics namespaceSelector:\rmatchNames:\r- ingress-nginx\rendpoints:\r- port: metrics\rinterval: 30s\r","description":"本文主要介绍如何使用Helm安装prometheus-stack","id":8,"section":"posts","tags":["prometheus","helm"],"title":"使用helm安装prometheus-stack","uri":"https://www.coolops.cn/posts/prometheus-stack-helm-install/"},{"content":"使用kubeadm搭建的集群默认证书有效期是1年，续费证书其实是一件很快的事情。但是就怕出事了才发现，毕竟作为专业搬砖工程师，每天都很忙的。\n鉴于此，监控集群证书有效期是一件不得不做的事情。Prometheus作为云原生领域的王者，如果能用它来监控证书有效期并能及时告警，那就再好不过了。\nssl_exporter就是来做这个事情的。ssh_exporter是一个Prometheus Exporter能提供多种针对 SSL 的检测手段，包括：https 证书生效/失效时间、文件证书生效/失效时间，OCSP 等相关指标。\n下面就来监听集群证书的有效期。\n安装 apiVersion: v1\rkind: Service\rmetadata:\rlabels:\rname: ssl-exporter\rname: ssl-exporter\rspec:\rports:\r- name: ssl-exporter\rprotocol: TCP\rport: 9219\rtargetPort: 9219\rselector:\rapp: ssl-exporter\r---\rapiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: ssl-exporter\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: ssl-exporter\rtemplate:\rmetadata:\rname: ssl-exporter\rlabels:\rapp: ssl-exporter\rspec:\rinitContainers:\r# Install kube ca cert as a root CA\r- name: ca\rimage: alpine\rcommand:\r- sh\r- -c\r- |\rset -e\rapk add --update ca-certificates\rcp /var/run/secrets/kubernetes.io/serviceaccount/ca.crt /usr/local/share/ca-certificates/kube-ca.crt\rupdate-ca-certificates\rcp /etc/ssl/certs/* /ssl-certs\rvolumeMounts:\r- name: ssl-certs\rmountPath: /ssl-certs\rcontainers:\r- name: ssl-exporter\rimage: ribbybibby/ssl-exporter:v0.6.0\rports:\r- name: tcp\rcontainerPort: 9219\rvolumeMounts:\r- name: ssl-certs\rmountPath: /etc/ssl/certs\rvolumes:\r- name: ssl-certs\remptyDir: {}\r执行kubectl apply -f .安装即可。\n待Pod正常运行，如下：\n# kubectl get po -n monitoring -l app=ssl-exporter\rNAME READY STATUS RESTARTS AGE\rssl-exporter-7ff4759679-f4qbs 1/1 Running 0 21m\r然后配置prometheus抓取规则。\n 由于我的Prometheus是通过Prometheus Operator部署的，所以通过additional的方式进行抓取。\n 首先创建一个文件prometheus-additional.yaml，其内容如下：\n- job_name: ssl-exporter\rmetrics_path: /probe\rstatic_configs:\r- targets:\r- kubernetes.default.svc:443\rrelabel_configs:\r- source_labels: [__address__]\rtarget_label: __param_target\r- source_labels: [__param_target]\rtarget_label: instance\r- target_label: __address__\rreplacement: ssl-exporter.monitoring:9219\r然后创建secret，命令如下：\nkubectl delete secret additional-config -n monitoring\rkubectl -n monitoring create secret generic additional-config --from-file=prometheus-additional.yaml\r然后修改prometheus-prometheus.yaml配置文件，新增如下内容：\n additionalScrapeConfigs:\rname: additional-config key: prometheus-additional.yaml prometheus-prometheus.yaml的整体配置如下：\napiVersion: monitoring.coreos.com/v1\rkind: Prometheus\rmetadata:\rlabels:\rprometheus: k8s\rname: k8s\rnamespace: monitoring\rspec:\ralerting:\ralertmanagers:\r- name: alertmanager-main\rnamespace: monitoring\rport: web\rbaseImage: quay.io/prometheus/prometheus\rnodeSelector:\rkubernetes.io/os: linux\rpodMonitorNamespaceSelector: {}\rpodMonitorSelector: {}\rreplicas: 2\rresources:\rrequests:\rmemory: 400Mi\rruleSelector:\rmatchLabels:\rprometheus: k8s\rrole: alert-rules\rsecurityContext:\rfsGroup: 2000\rrunAsNonRoot: true\rrunAsUser: 1000\radditionalScrapeConfigs:\rname: additional-config key: prometheus-additional.yaml serviceAccountName: prometheus-k8s\rserviceMonitorNamespaceSelector: {}\rserviceMonitorSelector: {}\rversion: v2.11.0\rstorage:\rvolumeClaimTemplate:\rspec:\rstorageClassName: managed-nfs-storage resources:\rrequests:\rstorage: 10Gi\r然后重新执行prometheus-prometheus.yaml文件，命令如下：\nkubectl apply -f prometheus-prometheus.yaml\r现在可以在prometheus的web界面看到正常的抓取任务了，如下：然后通过(ssl_cert_not_after-time())/3600/24即可看到证书还有多久失效。\n通过ssl_tls_connect_success可以观测ssl链接是否正常。\n告警 上面已经安装ssl_exporter成功，并且能正常监控数据了，下面就配置一些告警规则，以便于运维能快速知道这个事情。\napiVersion: monitoring.coreos.com/v1\rkind: PrometheusRule\rmetadata:\rname: monitoring-ssl-tls-rules\rnamespace: monitoring\rlabels:\rprometheus: k8s\rrole: alert-rules\rspec:\rgroups:\r- name: check_ssl_validity\rrules:\r- alert: \u0026quot;K8S集群证书在30天后过期\u0026quot;\rexpr: (ssl_cert_not_after-time())/3600/24 \u0026lt;30\rfor: 1h\rlabels:\rseverity: critical\rannotations:\rdescription: 'K8S集群的证书还有{{ printf \u0026quot;%.1f\u0026quot; $value }}天就过期了,请尽快更新证书'\rsummary: \u0026quot;K8S集群证书证书过期警告\u0026quot;\r- name: ssl_connect_status\rrules:\r- alert: \u0026quot;K8S集群证书可用性异常\u0026quot;\rexpr: ssl_tls_connect_success == 0\rfor: 1m\rlabels:\rseverity: critical\rannotations:\rsummary: \u0026quot;K8S集群证书连接异常\u0026quot;\rdescription: \u0026quot;K8S集群 {{ $labels.instance }} 证书连接异常\u0026quot;\r如下展示规则正常，在异常的时候就可以接收到告警了。\n","description":"本文主要介绍如何使用ssl_exporter监控K8S集群证书的有效期","id":9,"section":"posts","tags":["Prometheus Exporter","ssl_exporter","kubernetes"],"title":"使用ssl_exporter监控K8S集群证书有效期","uri":"https://www.coolops.cn/posts/prometheus-ssl-exporter/"},{"content":"概述 在监控体系里面，通常我们认为监控分为：白盒监控和黑盒监控。\n黑盒监控：主要关注的现象，一般都是正在发生的东西，例如出现一个告警，业务接口不正常，那么这种监控就是站在用户的角度能看到的监控，重点在于能对正在发生的故障进行告警。\n白盒监控：主要关注的是原因，也就是系统内部暴露的一些指标，例如 redis 的 info 中显示 redis slave down，这个就是 redis info 显示的一个内部的指标，重点在于原因，可能是在黑盒监控中看到 redis down，而查看内部信息的时候，显示 redis port is refused connection。\nBlackbox Exporter Blackbox Exporter 是 Prometheus 社区提供的官方黑盒监控解决方案，其允许用户通过：HTTP、HTTPS、DNS、TCP 以及 ICMP 的方式对网络进行探测。\n1、HTTP 测试\n 定义 Request Header 信息 判断 Http status / Http Respones Header / Http Body 内容  2、TCP 测试\n 业务组件端口状态监听 应用层协议定义与监听  3、ICMP 测试\n 主机探活机制  4、POST 测试\n 接口联通性  5、SSL 证书过期时间\n安装Blackbox Exporter （1）创建YAML配置文件（blackbox-deploymeny.yaml）\napiVersion: v1\rkind: Service\rmetadata:\rname: blackbox\rnamespace: monitoring\rlabels:\rapp: blackbox\rspec:\rselector:\rapp: blackbox\rports:\r- port: 9115\rtargetPort: 9115\r---\rapiVersion: v1\rkind: ConfigMap\rmetadata:\rname: blackbox-config\rnamespace: monitoring\rdata:\rblackbox.yaml: |-\rmodules:\rhttp_2xx:\rprober: http\rtimeout: 10s\rhttp:\rvalid_http_versions: [\u0026quot;HTTP/1.1\u0026quot;, \u0026quot;HTTP/2\u0026quot;]\rvalid_status_codes: [200]\rmethod: GET\rpreferred_ip_protocol: \u0026quot;ip4\u0026quot;\rhttp_post_2xx:\rprober: http\rtimeout: 10s\rhttp:\rvalid_http_versions: [\u0026quot;HTTP/1.1\u0026quot;, \u0026quot;HTTP/2\u0026quot;]\rvalid_status_codes: [200]\rmethod: POST\rpreferred_ip_protocol: \u0026quot;ip4\u0026quot;\rtcp_connect:\rprober: tcp\rtimeout: 10s\rping:\rprober: icmp\rtimeout: 5s\ricmp:\rpreferred_ip_protocol: \u0026quot;ip4\u0026quot;\rdns:\rprober: dns\rdns:\rtransport_protocol: \u0026quot;tcp\u0026quot;\rpreferred_ip_protocol: \u0026quot;ip4\u0026quot;\rquery_name: \u0026quot;kubernetes.defalut.svc.cluster.local\u0026quot;\r---\rapiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: blackbox\rnamespace: monitoring\rspec:\rselector:\rmatchLabels:\rapp: blackbox\rtemplate:\rmetadata:\rlabels:\rapp: blackbox\rspec:\rcontainers:\r- name: blackbox\rimage: prom/blackbox-exporter:v0.18.0\rargs:\r- \u0026quot;--config.file=/etc/blackbox_exporter/blackbox.yaml\u0026quot;\r- \u0026quot;--log.level=error\u0026quot;\rports:\r- containerPort: 9115\rvolumeMounts:\r- name: config\rmountPath: /etc/blackbox_exporter\rvolumes:\r- name: config\rconfigMap:\rname: blackbox-config\r（2）创建即可\nkubectl apply -f blackbox-deploymeny.yaml\r配置监控 由于集群是用的Prometheus Operator方式部署的，所以就以additional的形式添加配置。\n（1）创建prometheus-additional.yaml文件，定义内容如下：\n- job_name: \u0026quot;ingress-endpoint-status\u0026quot;\rmetrics_path: /probe\rparams:\rmodule: [http_2xx] # Look for a HTTP 200 response.\rstatic_configs:\r- targets:\r- http://172.17.100.134/healthz\rlabels:\rgroup: nginx-ingress\rrelabel_configs:\r- source_labels: [__address__]\rtarget_label: __param_target\r- source_labels: [__param_target]\rtarget_label: instance\r- target_label: __address__\rreplacement: blackbox.monitoring:9115\r- job_name: \u0026quot;kubernetes-service-dns\u0026quot;\rmetrics_path: /probe\rparams:\rmodule: [dns]\rstatic_configs:\r- targets:\r- kube-dns.kube-system:53\rrelabel_configs:\r- source_labels: [__address__]\rtarget_label: __param_target\r- source_labels: [__param_target]\rtarget_label: instance\r- target_label: __address__\rreplacement: blackbox.monitoring:9115\r- job_name: \u0026quot;node-icmp-status\u0026quot;\r（2）创建secret\nkubectl -n monitoring create secret generic additional-config --from-file=prometheus-additional.yaml\r（3）修改prometheus的配置，文件prometheus-prometheus.yaml\n添加以下三行内容：\nadditionalScrapeConfigs:\rname: additional-config key: prometheus-additional.yaml\r完整配置如下：\napiVersion: monitoring.coreos.com/v1\rkind: Prometheus\rmetadata:\rlabels:\rprometheus: k8s\rname: k8s\rnamespace: monitoring\rspec:\ralerting:\ralertmanagers:\r- name: alertmanager-main\rnamespace: monitoring\rport: web\rbaseImage: quay.io/prometheus/prometheus\rnodeSelector:\rkubernetes.io/os: linux\rpodMonitorNamespaceSelector: {}\rpodMonitorSelector: {}\rreplicas: 2\rresources:\rrequests:\rmemory: 400Mi\rruleSelector:\rmatchLabels:\rprometheus: k8s\rrole: alert-rules\rsecurityContext:\rfsGroup: 2000\rrunAsNonRoot: true\rrunAsUser: 1000\radditionalScrapeConfigs:\rname: additional-config key: prometheus-additional.yaml serviceAccountName: prometheus-k8s\rserviceMonitorNamespaceSelector: {}\rserviceMonitorSelector: {}\rversion: v2.11.0\rstorage:\rvolumeClaimTemplate:\rspec:\rstorageClassName: managed-nfs-storage resources:\rrequests:\rstorage: 10Gi\r（4）重新apply配置\nkubectl apply -f prometheus-prometheus.yaml\r（5）reload prometheus\n先找到svc的IP\n# kubectl get svc -n monitoring -l prometheus=k8s\rNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rprometheus-k8s ClusterIP 10.99.93.157 \u0026lt;none\u0026gt; 9090/TCP 33m\r使用以下命令reload\ncurl -X POST \u0026quot;http://10.99.93.157:9090/-/reload\u0026quot;\r后面修改配置文件，使用以下三条命令即可\nkubectl delete secret additional-config -n monitoring\rkubectl -n monitoring create secret generic additional-config --from-file=prometheus-additional.yaml\rcurl -X POST \u0026quot;http://10.99.93.157:9090/-/reload\u0026quot;\r等待一段时间，即可在prometheus的web界面看到如下target\nICMP监控 ICMP主要是通过ping命令来检测目的主机的连通性。\n配置如下：\n- job_name: \u0026quot;node-icmp-status\u0026quot;\rmetrics_path: /probe\rparams:\rmodule: [ping] # Look for a HTTP 200 response.\rstatic_configs:\r- targets:\r- 172.17.100.134\r- 172.17.100.50\r- 172.17.100.135\r- 172.17.100.136\r- 172.17.100.137\r- 172.17.100.138\rlabels:\rgroup: k8s-node-ping\rrelabel_configs:\r- source_labels: [__address__]\rtarget_label: __param_target\r- source_labels: [__param_target]\rtarget_label: instance\r- target_label: __address__\rreplacement: blackbox.monitoring:9115\r然后重载配置文件\nkubectl delete secret additional-config -n monitoring\rkubectl -n monitoring create secret generic additional-config --from-file=prometheus-additional.yaml\rcurl -X POST \u0026quot;http://10.99.93.157:9090/-/reload\u0026quot;\r接下来可以看到监控成功，如下：\nHTTP监控 HTTP就是通过GET或者POST的方式来检测应用是否正常。\n这里配置GET方式。\n- job_name: \u0026quot;check-web-status\u0026quot;\rmetrics_path: /probe\rparams:\rmodule: [http_2xx] # Look for a HTTP 200 response.\rstatic_configs:\r- targets:\r- https://www.coolops.cn\r- https://www.baidu.com\rlabels:\rgroup: web-url\rrelabel_configs:\r- source_labels: [__address__]\rtarget_label: __param_target\r- source_labels: [__param_target]\rtarget_label: instance\r- target_label: __address__\rreplacement: blackbox.monitoring:9115\r重载配置后可以看到监控如下：\nTCP监控 TCP监控主要是通过类似于Telnet的方式进行检测，配置如下：\n- job_name: \u0026quot;check-middleware-tcp\u0026quot;\rmetrics_path: /probe\rparams:\rmodule: [tcp_connect] # Look for a HTTP 200 response.\rstatic_configs:\r- targets:\r- 172.17.100.135:80\r- 172.17.100.74:3306\r- 172.17.100.25:3306\r- 172.17.100.8:3306\r- 172.17.100.75:3306\r- 172.17.100.72:3306\r- 172.17.100.73:3306\rlabels:\rgroup: middleware-tcp\rrelabel_configs:\r- source_labels: [__address__]\rtarget_label: __param_target\r- source_labels: [__param_target]\rtarget_label: instance\r- target_label: __address__\rreplacement: blackbox.monitoring:9115\r重载配置文件后监控如下：\n告警规则 1、业务正常性\n icmp、tcp、http、post 监测是否正常可以观察probe_success 这一指标 probe_success == 0 ##联通性异常 probe_success == 1 ##联通性正常 告警也是判断这个指标是否等于0，如等于0 则触发异常报警  2、通过 http 模块我们可以获取证书的过期时间，可以根据过期时间添加相关告警\nprobe_ssl_earliest_cert_expiry ：可以查询证书到期时间。\n经过单位转换我们可以得到一下，按天来计算：(probe_ssl_earliest_cert_expiry - time())/86400\n3、所以我们结合上面的配置可以定制如下告警规则\ngroups:\r- name: blackbox_network_stats\rrules:\r- alert: blackbox_network_stats\rexpr: probe_success == 0\rfor: 1m\rlabels:\rseverity: critical\rannotations:\rsummary: \u0026quot;接口/主机/端口连通异常告警\u0026quot;\rdescription: \u0026quot;接口/主机/端口 {{ $labels.instance }} 连通异常\u0026quot;\rssl检测\ngroups:\r- name: check_ssl_status\rrules:\r- alert: \u0026quot;ssl证书过期警告\u0026quot;\rexpr: (probe_ssl_earliest_cert_expiry - time())/86400 \u0026lt;30\rfor: 1h\rlabels:\rseverity: warn\rannotations:\rdescription: '域名{{$labels.instance}}的证书还有{{ printf \u0026quot;%.1f\u0026quot; $value }}天就过期了,请尽快更新证书'\rsummary: \u0026quot;ssl证书过期警告\u0026quot;\rGrafana面板 直接使用12559，导入即可。\n导入后就是这个样子。\n","description":"本文主要介绍如何在K8S中进行黑盒监控","id":10,"section":"posts","tags":["prometheus","blackbox_exporter"],"title":"使用blackbox_exporter进行黑盒监控","uri":"https://www.coolops.cn/posts/prometheus-blackbox-exporter/"},{"content":"升级说明  可用的K8S集群，使用kubeadm搭建 可以小版本升级，也可以跨一个大版本升级，不建议跨两个大版本升级 对集群资源做好备份  升级目标 将kubernetes 1.17.9版本升级到1.18.9版本\n现有集群版本已经节点如下：\n# kubectl get nodes NAME STATUS ROLES AGE VERSION\recs-968f-0005 Ready node 102d v1.17.9\rk8s-master Ready master 102d v1.17.9\r备份集群 kubeadm upgrade 不会影响你的工作负载，只会涉及 Kubernetes 内部的组件，但备份终究是好的。这里主要是对集群的所有资源进行备份，我使用的是一个开源的脚本，项目地址是：https://github.com/solomonxu/k8s-backup-restore\n（1）下载脚本\n$ mkdir -p /data\rcd /data\rgit clone https://github.com/solomonxu/k8s-backup-restore.git\r（2）执行备份\ncd /data/k8s-backup-restore\r./bin/k8s_backup.sh 如果要恢复怎么办？只需要执行如下步骤。\n（1）创建恢复目录\n$ mkdir -p /data/k8s-backup-restore/data/restore\r（2）将需要恢复的YAML清单复制到该目录下\n$ cp devops_deployments_gitlab.yaml ../../restore/\r（3）执行恢复命令\ncd /data/k8s-backup-restore\r./bin/k8s_restore.sh\r会输出如下信息。\n2021-01-06 15:09:43.954083 [11623] - INFO Kubernetes Restore start now. All yaml files which located in path [/data/k8s-backup-restore/data/restore] will be applied.\r2021-01-06 15:09:43.957265 [11623] - INFO If you want to read the log record of restore, please input command ' tail -100f '\r2021-01-06 15:09:43.986869 [11623] - WARN WARNING!!! This will create 1 resources from yaml files into kubernetes cluster. While same name of resources will be deleted. Please consider it carefully!\rDo you want to continue? [yes/no/show] y\r2021-01-06 15:10:00.062598 [11623] - INFO Restore No.1 resources from yaml file: /data/k8s-backup-restore/data/restore/devops_deployments_gitlab.yaml...\r2021-01-06 15:10:00.066011 [11623] - INFO Run shell: kubectl delete -f /data/k8s-backup-restore/data/restore/devops_deployments_gitlab.yaml.\rdeployment.apps \u0026quot;gitlab\u0026quot; deleted\r2021-01-06 15:10:00.423109 [11623] - INFO Delete resource from /data/k8s-backup-restore/data/restore/devops_deployments_gitlab.yaml: ok.\r2021-01-06 15:10:00.426383 [11623] - INFO Run shell: kubectl create -f /data/k8s-backup-restore/data/restore/devops_deployments_gitlab.yaml.\rdeployment.apps/gitlab created\r2021-01-06 15:10:00.614960 [11623] - INFO Create resource from /data/k8s-backup-restore/data/restore/devops_deployments_gitlab.yaml: ok.\r2021-01-06 15:10:00.618572 [11623] - INFO Restore 1 resources from yaml files in all: count_delete_ok=1, count_delete_failed=0, count_create_ok=1, count_create_failed=0.\r2021-01-06 15:10:00.622002 [11623] - INFO Kubernetes Restore completed, all done.\r（4）验证是否正常修复\n$ kubectl get po -n devops\rNAME READY STATUS RESTARTS AGE\rgitlab-65896f7557-786hj 1/1 Running 0 66s\r升级集群 Master升级 （1）确定要升级的版本\n$ yum list --showduplicates kubeadm --disableexcludes=kubernetes\r我这里选择的是1.18.9版本。\n（2）升级kubeadm\n$ yum install -y kubeadm-1.18.9-0 --disableexcludes=kubernetes\r升级完成后验证版本是否正确。\n$ kubeadm version\rkubeadm version: \u0026amp;version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;18\u0026quot;, GitVersion:\u0026quot;v1.18.9\u0026quot;, GitCommit:\u0026quot;94f372e501c973a7fa9eb40ec9ebd2fe7ca69848\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2020-09-16T13:54:01Z\u0026quot;, GoVersion:\u0026quot;go1.13.15\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;}\r（3）排空节点\n$ kubectl drain k8s-master （4）运行升级计划，查看是否可以升级\n$ kubeadm upgrade plan\r[upgrade/config] Making sure the configuration is correct:\r[upgrade/config] Reading configuration from the cluster...\r[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'\r[preflight] Running pre-flight checks.\r[upgrade] Running cluster health checks\r[upgrade] Fetching available versions to upgrade to\r[upgrade/versions] Cluster version: v1.17.9\r[upgrade/versions] kubeadm version: v1.18.9\rI0106 14:22:58.709642 10455 version.go:252] remote version is much newer: v1.20.1; falling back to: stable-1.18\r[upgrade/versions] Latest stable version: v1.18.14\r[upgrade/versions] Latest stable version: v1.18.14\r[upgrade/versions] Latest version in the v1.17 series: v1.17.16\r[upgrade/versions] Latest version in the v1.17 series: v1.17.16\rComponents that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':\rCOMPONENT CURRENT AVAILABLE\rKubelet 2 x v1.17.9 v1.17.16\rUpgrade to the latest version in the v1.17 series:\rCOMPONENT CURRENT AVAILABLE\rAPI Server v1.17.9 v1.17.16\rController Manager v1.17.9 v1.17.16\rScheduler v1.17.9 v1.17.16\rKube Proxy v1.17.9 v1.17.16\rCoreDNS 1.6.5 1.6.7\rEtcd 3.4.3 3.4.3-0\rYou can now apply the upgrade by executing the following command:\rkubeadm upgrade apply v1.17.16\r_____________________________________________________________________\rComponents that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':\rCOMPONENT CURRENT AVAILABLE\rKubelet 2 x v1.17.9 v1.18.14\rUpgrade to the latest stable version:\rCOMPONENT CURRENT AVAILABLE\rAPI Server v1.17.9 v1.18.14\rController Manager v1.17.9 v1.18.14\rScheduler v1.17.9 v1.18.14\rKube Proxy v1.17.9 v1.18.14\rCoreDNS 1.6.5 1.6.7\rEtcd 3.4.3 3.4.3-0\rYou can now apply the upgrade by executing the following command:\rkubeadm upgrade apply v1.18.14\rNote: Before you can perform this upgrade, you have to update kubeadm to v1.18.14.\r_____________________________________________________________________\r上面显示我可以升级到更高版本，不过我这里还是升级到1.18.9。\n（5）升级集群\n$ kubeadm upgrade apply v1.18.9 --config kubeadm.yaml W0106 14:23:58.359112 11936 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]\r[upgrade/config] Making sure the configuration is correct:\rW0106 14:23:58.367062 11936 common.go:94] WARNING: Usage of the --config flag for reconfiguring the cluster during upgrade is not recommended!\rW0106 14:23:58.367816 11936 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]\r[preflight] Running pre-flight checks.\r[upgrade] Running cluster health checks\r[upgrade/version] You have chosen to change the cluster version to \u0026quot;v1.18.9\u0026quot;\r[upgrade/versions] Cluster version: v1.17.9\r[upgrade/versions] kubeadm version: v1.18.9\r[upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y\r[upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd]\r[upgrade/prepull] Prepulling image for component etcd.\r[upgrade/prepull] Prepulling image for component kube-controller-manager.\r[upgrade/prepull] Prepulling image for component kube-scheduler.\r[upgrade/prepull] Prepulling image for component kube-apiserver.\r[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager\r[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver\r[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler\r[apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-etcd\r[apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-etcd\r[upgrade/prepull] Prepulled image for component etcd.\r[upgrade/prepull] Prepulled image for component kube-controller-manager.\r[upgrade/prepull] Prepulled image for component kube-apiserver.\r[upgrade/prepull] Prepulled image for component kube-scheduler.\r[upgrade/prepull] Successfully prepulled the images for all the control plane components\r[upgrade/apply] Upgrading your Static Pod-hosted control plane to version \u0026quot;v1.18.9\u0026quot;...\rStatic pod: kube-apiserver-k8s-master hash: d002f0455950f5b76f6097191f93db28\rStatic pod: kube-controller-manager-k8s-master hash: 54e96591b22cec4a1f5b76965fa90be7\rStatic pod: kube-scheduler-k8s-master hash: da215ebee0354225c20c7bdf28b467f8\r[upgrade/etcd] Upgrading to TLS for etcd\r[upgrade/etcd] Non fatal issue encountered during upgrade: the desired etcd version for this Kubernetes version \u0026quot;v1.18.9\u0026quot; is \u0026quot;3.4.3-0\u0026quot;, but the current etcd version is \u0026quot;3.4.3\u0026quot;. Won't downgrade etcd, instead just continue\r[upgrade/staticpods] Writing new Static Pod manifests to \u0026quot;/etc/kubernetes/tmp/kubeadm-upgraded-manifests328986264\u0026quot;\r[upgrade/staticpods] Preparing for \u0026quot;kube-apiserver\u0026quot; upgrade\r[upgrade/staticpods] Renewing apiserver certificate\r[upgrade/staticpods] Renewing apiserver-kubelet-client certificate\r[upgrade/staticpods] Renewing front-proxy-client certificate\r[upgrade/staticpods] Renewing apiserver-etcd-client certificate\r[upgrade/staticpods] Moved new manifest to \u0026quot;/etc/kubernetes/manifests/kube-apiserver.yaml\u0026quot; and backed up old manifest to \u0026quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-01-06-14-24-18/kube-apiserver.yaml\u0026quot;\r[upgrade/staticpods] Waiting for the kubelet to restart the component\r[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)\rStatic pod: kube-apiserver-k8s-master hash: d002f0455950f5b76f6097191f93db28\rStatic pod: kube-apiserver-k8s-master hash: d002f0455950f5b76f6097191f93db28\rStatic pod: kube-apiserver-k8s-master hash: 6bc4f16364bf23910ec81c9e91593d95\r[apiclient] Found 1 Pods for label selector component=kube-apiserver\r[upgrade/staticpods] Component \u0026quot;kube-apiserver\u0026quot; upgraded successfully!\r[upgrade/staticpods] Preparing for \u0026quot;kube-controller-manager\u0026quot; upgrade\r[upgrade/staticpods] Renewing controller-manager.conf certificate\r[upgrade/staticpods] Moved new manifest to \u0026quot;/etc/kubernetes/manifests/kube-controller-manager.yaml\u0026quot; and backed up old manifest to \u0026quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-01-06-14-24-18/kube-controller-manager.yaml\u0026quot;\r[upgrade/staticpods] Waiting for the kubelet to restart the component\r[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)\rStatic pod: kube-controller-manager-k8s-master hash: 54e96591b22cec4a1f5b76965fa90be7\rStatic pod: kube-controller-manager-k8s-master hash: a96ac50aab8a064c2101f684d34ee058\r[apiclient] Found 1 Pods for label selector component=kube-controller-manager\r[upgrade/staticpods] Component \u0026quot;kube-controller-manager\u0026quot; upgraded successfully!\r[upgrade/staticpods] Preparing for \u0026quot;kube-scheduler\u0026quot; upgrade\r[upgrade/staticpods] Renewing scheduler.conf certificate\r[upgrade/staticpods] Moved new manifest to \u0026quot;/etc/kubernetes/manifests/kube-scheduler.yaml\u0026quot; and backed up old manifest to \u0026quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-01-06-14-24-18/kube-scheduler.yaml\u0026quot;\r[upgrade/staticpods] Waiting for the kubelet to restart the component\r[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)\rStatic pod: kube-scheduler-k8s-master hash: da215ebee0354225c20c7bdf28b467f8\rStatic pod: kube-scheduler-k8s-master hash: 1a0670b7d3bff3fd96dbd08f176c1461\r[apiclient] Found 1 Pods for label selector component=kube-scheduler\r[upgrade/staticpods] Component \u0026quot;kube-scheduler\u0026quot; upgraded successfully!\r[upload-config] Storing the configuration used in ConfigMap \u0026quot;kubeadm-config\u0026quot; in the \u0026quot;kube-system\u0026quot; Namespace\r[kubelet] Creating a ConfigMap \u0026quot;kubelet-config-1.18\u0026quot; in namespace kube-system with the configuration for the kubelets in the cluster\r[kubelet-start] Downloading configuration for the kubelet from the \u0026quot;kubelet-config-1.18\u0026quot; ConfigMap in the kube-system namespace\r[kubelet-start] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot;\r[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes\r[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\r[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\r[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\r[addons] Applied essential addon: CoreDNS\r[addons] Applied essential addon: kube-proxy\r[upgrade/successful] SUCCESS! Your cluster was upgraded to \u0026quot;v1.18.9\u0026quot;. Enjoy!\r[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.\r由输出可以看出升级执行成功。\n（6）取消调度保护\n# kubectl uncordon k8s-master\r（7）升级节点\n$ kubeadm upgrade node\r[upgrade] Reading configuration from the cluster...\r[upgrade] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'\r[upgrade] Upgrading your Static Pod-hosted control plane instance to version \u0026quot;v1.18.9\u0026quot;...\rStatic pod: kube-apiserver-k8s-master hash: 6bc4f16364bf23910ec81c9e91593d95\rStatic pod: kube-controller-manager-k8s-master hash: a96ac50aab8a064c2101f684d34ee058\rStatic pod: kube-scheduler-k8s-master hash: 1a0670b7d3bff3fd96dbd08f176c1461\r[upgrade/etcd] Upgrading to TLS for etcd\r[upgrade/etcd] Non fatal issue encountered during upgrade: the desired etcd version for this Kubernetes version \u0026quot;v1.18.9\u0026quot; is \u0026quot;3.4.3-0\u0026quot;, but the current etcd version is \u0026quot;3.4.3\u0026quot;. Won't downgrade etcd, instead just continue\r[upgrade/staticpods] Writing new Static Pod manifests to \u0026quot;/etc/kubernetes/tmp/kubeadm-upgraded-manifests315032619\u0026quot;\rW0106 14:36:33.013476 30507 manifests.go:225] the default kube-apiserver authorization-mode is \u0026quot;Node,RBAC\u0026quot;; using \u0026quot;Node,RBAC\u0026quot;\r[upgrade/staticpods] Preparing for \u0026quot;kube-apiserver\u0026quot; upgrade\r[upgrade/staticpods] Current and new manifests of kube-apiserver are equal, skipping upgrade\r[upgrade/staticpods] Preparing for \u0026quot;kube-controller-manager\u0026quot; upgrade\r[upgrade/staticpods] Current and new manifests of kube-controller-manager are equal, skipping upgrade\r[upgrade/staticpods] Preparing for \u0026quot;kube-scheduler\u0026quot; upgrade\r[upgrade/staticpods] Current and new manifests of kube-scheduler are equal, skipping upgrade\r[upgrade] The control plane instance for this node was successfully updated!\r[kubelet-start] Downloading configuration for the kubelet from the \u0026quot;kubelet-config-1.18\u0026quot; ConfigMap in the kube-system namespace\r[kubelet-start] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot;\r[upgrade] The configuration for this node was successfully updated!\r[upgrade] Now you should go ahead and upgrade the kubelet package using your package manager.\r（8）升级kubectl和kubelet\n$ yum install -y kubelet-1.18.9-0 kubectl-1.18.9-0 --disableexcludes=kubernetes\r重启kubelet\n$ systemctl daemon-reload\r$ systemctl restart kubelet\rNode升级 （1）升级kubeadm\nyum install -y kubeadm-1.18.9-0 --disableexcludes=kubernetes\r（2）设置节点不可调度并排空节点\n$ kubectl cordon ecs-968f-0005\r$ kubectl drain ecs-968f-0005\r（3）升级节点\n$ kubeadm upgrade node\r[upgrade] Reading configuration from the cluster...\r[upgrade] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'\r[upgrade] Skipping phase. Not a control plane node.\r[kubelet-start] Downloading configuration for the kubelet from the \u0026quot;kubelet-config-1.18\u0026quot; ConfigMap in the kube-system namespace\r[kubelet-start] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot;\r[upgrade] The configuration for this node was successfully updated!\r[upgrade] Now you should go ahead and upgrade the kubelet package using your package manager.\r（4）升级kubelet\nyum install -y kubelet-1.18.9-0 --disableexcludes=kubernetes\r重启kubelet\n$ systemctl daemon-reload\r$ systemctl restart kubelet\r（5）设置节点可调度\nkubectl uncordon ecs-968f-0005\r验证集群 （1）、验证集群状态是否正常\n$ kubectl get no\rNAME STATUS ROLES AGE VERSION\recs-968f-0005 Ready node 102d v1.18.9\rk8s-master Ready master 102d v1.18.9\r（2）、验证集群证书是否正常\n$ kubeadm alpha certs check-expiration\r[check-expiration] Reading configuration from the cluster...\r[check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'\rCERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED\radmin.conf Jan 06, 2022 06:36 UTC 364d no apiserver Jan 06, 2022 06:24 UTC 364d ca no apiserver-etcd-client Jan 06, 2022 06:24 UTC 364d etcd-ca no apiserver-kubelet-client Jan 06, 2022 06:24 UTC 364d ca no controller-manager.conf Jan 06, 2022 06:24 UTC 364d no etcd-healthcheck-client Sep 25, 2021 06:55 UTC 262d etcd-ca no etcd-peer Sep 25, 2021 06:55 UTC 262d etcd-ca no etcd-server Sep 25, 2021 06:55 UTC 262d etcd-ca no front-proxy-client Jan 06, 2022 06:24 UTC 364d front-proxy-ca no scheduler.conf Jan 06, 2022 06:24 UTC 364d no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED\rca Sep 23, 2030 06:55 UTC 9y no etcd-ca Sep 23, 2030 06:55 UTC 9y no front-proxy-ca Sep 23, 2030 06:55 UTC 9y no\r 注意：kubeadm upgrade 也会自动对它在此节点上管理的证书进行续约。 如果选择不对证书进行续约，可以使用 --certificate-renewal=false。\n 故障恢复 在升级过程中如果升级失败并且没有回滚，可以继续执行kubeadm upgrade。如果要从故障状态恢复，可以执行kubeadm upgrade --force。\n在升级期间，会在/etc/kubernetes/tmp目录下生成备份文件：\n kubeadm-backup-etcd- kubeadm-backup-manifests-  kubeadm-backup-etcd中包含本地etcd的数据备份，如果升级失败并且无法修复，可以将其数据复制到etcd数据目录进行手动修复。\nkubeadm-backup-manifests中保存的是节点静态pod的YAML清单，如果升级失败并且无法修复，可以将其复制到/etc/kubernetes/manifests下进行手动修复。\n","description":"本文主要介绍如何给kubeadm搭建的K8S集群进行升级","id":11,"section":"posts","tags":["kubernetes","kubeadm"],"title":"Kubeadm搭建的K8S集群升级","uri":"https://www.coolops.cn/posts/kubeadm-upgrade-k8s/"},{"content":"链路监控的软件很多，比如skywalking、pinpoint、zipkin等。其中\n Zipkin：由Twitter公司开源，开放源代码分布式的跟踪系统，用于收集服务的定时数据，以解决微服务架构中的延迟问题，包括：数据的收集、存储、查找和展现。 Pinpoint：一款对Java编写的大规模分布式系统的APM工具，由韩国人开源的分布式跟踪组件。 Skywalking：国产的优秀APM组件，是一个对JAVA分布式应用程序集群的业务运行情况进行追踪、告警和分析的系统。  这里主要来介绍Skywalking及其安装。\nSkyWalking 是观察性分析平台和应用性能管理系统。提供分布式追踪、服务网格遥测分析、度量聚合和可视化一体化解决方案.支持Java, .Net Core, PHP, NodeJS, Golang, LUA语言探针，支持Envoy + Istio构建的Service Mesh。\n安装服务端 这里是安装在kubernetes中，使用helm安装。\n安装helm 使用helm3，安装如下：\nwget https://get.helm.sh/helm-v3.0.0-linux-amd64.tar.gz\rtar zxvf helm-v3.0.0-linux-amd64.tar.gz\rmv linux-amd64/helm /usr/bin/\r 说明：helm3没有tiller这个服务端了，直接用kubeconfig进行验证通信，所以建议部署在master节点\n 安装skywalking skywalking的安装比较简单，具体可以参照其官方文档。\n这里采用es做后端存储。\n（1）、下载项目\ngit clone https://github.com/apache/skywalking-kubernetes.git\r（2）、默认安装使用的是es7做后端存储，安装步骤如下：\n$ cd chart\r$ helm repo add elastic https://helm.elastic.co\r$ helm dep up skywalking\r$ helm install \u0026lt;release_name\u0026gt; skywalking -n \u0026lt;namespace\u0026gt;\r如果要使用es6做后端存储，则按照下面的步骤：\n$ helm dep up skywalking\r$ helm install \u0026lt;release_name\u0026gt; skywalking -n \u0026lt;namespace\u0026gt; --values ./skywalking/values-es6.yaml\r如果本身有ES，如果是ES7，则用下面的命令安装：\n$ cd chart\r$ helm repo add elastic https://helm.elastic.co\r$ helm dep up skywalking\r$ helm install skywalking skywalking -n skywalking \\\r--set elasticsearch.enabled=false \\\r--set elasticsearch.config.host=elasticsearch-client.elastic.svc.default.local \\\r--set elasticsearch.config.port.http=9200 \\\r--set elasticsearch.config.user=elastic \\\r--set elasticsearch.config.password=elastic@123456 如果是es6，则用下面的命令：\n$ helm dep up skywalking\r$ helm install \u0026lt;release_name\u0026gt; skywalking -n \u0026lt;namespace\u0026gt; \\\r--values ./skywalking/values-es6.yaml \\\r--set elasticsearch.enabled=false \\\r--set elasticsearch.config.host=\u0026lt;es_host\u0026gt; \\\r--set elasticsearch.config.port.http=\u0026lt;es_port\u0026gt; \\\r--set elasticsearch.config.user=\u0026lt;es_user\u0026gt; \\\r--set elasticsearch.config.password=\u0026lt;es_password\u0026gt; 然后观察所以pod是否处于running状态：\n# kubectl get pod\rNAME READY STATUS RESTARTS AGE\relasticsearch-master-0 1/1 Running 0 16h\relasticsearch-master-1 1/1 Running 0 16h\relasticsearch-master-2 1/1 Running 0 16h\rnfs-client-provisioner-f85644675-ftq2v 1/1 Running 7 80d\rskywalking-es-init-7968s 0/1 Completed 0 16h\rskywalking-es-init-x89pr 0/1 Completed 0 15h\rskywalking-oap-694fc79d55-2dmgr 1/1 Running 0 16h\rskywalking-oap-694fc79d55-bl5hk 1/1 Running 4 16h\rskywalking-ui-6bccffddbd-d2xhs 1/1 Running 0 16h\r查看其所有的svc：\n# kubectl get svc\rNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\relasticsearch-master ClusterIP 10.108.144.131 \u0026lt;none\u0026gt; 9200/TCP,9300/TCP 16h\relasticsearch-master-headless ClusterIP None \u0026lt;none\u0026gt; 9200/TCP,9300/TCP 16h\rkubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 83d\rskywalking-oap ClusterIP 10.109.142.213 \u0026lt;none\u0026gt; 12800/TCP,11800/TCP 16h\rskywalking-ui NodePort 10.109.0.122 \u0026lt;none\u0026gt; 80:30969/TCP 16h\r将skywalking-ui的svc type改为nodeport，然后通过浏览器访问出现如下界面。\n到此服务端安装完成了，注意默认安装es并没有持久化处理，如果es重建会导致数据丢失。\n安装客户端 这里以接入java项目为例。\n由于我们应用是部署在kubernetes中的，所以有两种方式将agent的加入到我们的应用中：\n 在项目的Docker镜像中加入agent 以sidecar的形式给应用添加agent  下载对应版本的软件包\nwget https://mirrors.tuna.tsinghua.edu.cn/apache/skywalking/8.1.0/apache-skywalking-apm-8.1.0.tar.gz\rtar xf apache-skywalking-apm-8.1.0.tar.gz\r在项目的Docker镜像中加入agent  PS：如果每个应用都放一次就太麻烦，简单点的就是放到基础镜像中去。\n 如下：\nFROM harbor-test.coolops.com/coolops/jdk:8u144_test\rRUN mkdir -p /usr/skywalking/agent/\rADD apache-skywalking-apm-bin/agent/ /usr/skywalking/agent/\r以sidecar的形式给应用添加agent 构建sidecar的Dockerfile\nFROM busybox:latest ENV LANG=C.UTF-8\rRUN set -eux \u0026amp;\u0026amp; mkdir -p /usr/skywalking/agent/\rADD apache-skywalking-apm-bin/agent/ /usr/skywalking/agent/\rWORKDIR /\r然后在使用的时候如下使用：\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rlabels:\rname: demo-sw\rname: demo-sw\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rname: demo-sw\rtemplate:\rmetadata:\rlabels:\rname: demo-sw\rspec:\rinitContainers:\r- image: innerpeacez/sw-agent-sidecar:latest\rname: sw-agent-sidecar\rimagePullPolicy: IfNotPresent\rcommand: ['sh']\rargs: ['-c','mkdir -p /skywalking/agent \u0026amp;\u0026amp; cp -r /usr/skywalking/agent/* /skywalking/agent']\rvolumeMounts:\r- mountPath: /skywalking/agent\rname: sw-agent\rcontainers:\r- image: nginx:1.7.9\rname: nginx\rvolumeMounts:\r- mountPath: /usr/skywalking/agent\rname: sw-agent\rports:\r- containerPort: 80\rvolumes:\r- name: sw-agent\remptyDir: {}\r 上面这个YAML文件只是作为参考例子，思路是如此而已。\n 在真正的java应用中，我们应该如何启用agent呢？\n其实很简单。\n（1）、如果是部署在tomcat中，修改catalina.sh即可，如下：\nCATALINA_OPTS=\u0026quot;$CATALINA_OPTS -javaagent:/path/to/skywalking-agent/skywalking-agent.jar\u0026quot;; export CATALINA_OPTS\r（2）、如果是jar文件启动，则如下：\njava -javaagent:/path/to/skywalking-agent/skywalking-agent.jar -Dskywalking.agent.service_name=${SW_AGENT_NAME} -jar yourApp.jar\r注意这里有两个环境变量：\n SW_AGENT_COLLECTOR_BACKEND_SERVICES：后端服务的地址 SW_AGENT_NAME：在UI上显示的应用名  所以完整的demo如下：\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rlabels:\rapp: \u0026lt;APP_NAME\u0026gt;\rname: \u0026lt;APP_NAME\u0026gt;\rnamespace: \u0026lt;NEMESPACE\u0026gt;\rspec:\rminReadySeconds: 60\rprogressDeadlineSeconds: 600\rreplicas: 1\rrevisionHistoryLimit: 10\rselector:\rmatchLabels:\rapp: \u0026lt;APP_NAME\u0026gt;\rstrategy:\rrollingUpdate:\rmaxSurge: 25%\rmaxUnavailable: 25%\rtype: RollingUpdate\rtemplate:\rmetadata:\rlabels:\rapp: \u0026lt;APP_NAME\u0026gt;\rspec:\raffinity:\rnodeAffinity:\rpreferredDuringSchedulingIgnoredDuringExecution:\r- preference: {}\rweight: 100\rrequiredDuringSchedulingIgnoredDuringExecution:\rnodeSelectorTerms:\r- matchExpressions:\r- key: category\roperator: In\rvalues:\r- other\rcontainers:\r- args:\r- -jar /opt/\u0026lt;JAR_NAME\u0026gt;.jar\r- -javaagent:/usr/skywalking/agent/skywalking-agent.jar\rcommand:\r- java\renv:\r- name: JAVA_HEAP_SIZE\rvalue: 2g\r- name: POD_NAME\rvalueFrom:\rfieldRef:\rapiVersion: v1\rfieldPath: metadata.name\r- name: SW_AGENT_COLLECTOR_BACKEND_SERVICES\rvalue: skywalking-oap.default:11800\r- name: SW_AGENT_NAME\rvalue: \u0026lt;APP_NAME\u0026gt;\rimage: \u0026lt;IMAGE\u0026gt;:\u0026lt;IMAGE_TAG\u0026gt;\rimagePullPolicy: Always\rlivenessProbe:\rfailureThreshold: 3\rhttpGet:\rpath: healthCheck\rport: tcp-\u0026lt;PORT\u0026gt;\rscheme: HTTP\rinitialDelaySeconds: 60\rperiodSeconds: 10\rsuccessThreshold: 1\rtimeoutSeconds: 1\rreadinessProbe:\rfailureThreshold: 3\rhttpGet:\rpath: healthCheck\rport: tcp-\u0026lt;PORT\u0026gt;\rscheme: HTTP\rinitialDelaySeconds: 55\rperiodSeconds: 10\rsuccessThreshold: 1\rtimeoutSeconds: 1\rname: \u0026lt;APP_NAME\u0026gt;\rports:\r- containerPort: \u0026lt;PORT\u0026gt;\rname: tcp-\u0026lt;PORT\u0026gt;\rprotocol: TCP\rresources:\rlimits:\rcpu: '4'\rmemory: 3Gi\rrequests:\rcpu: 500m\rmemory: 2Gi\rterminationMessagePath: /dev/termination-log\rterminationMessagePolicy: File\rdnsPolicy: ClusterFirst\rimagePullSecrets:\r- name: image_pull\rrestartPolicy: Always\rschedulerName: default-scheduler\rsecurityContext: {}\rterminationGracePeriodSeconds: 120\r参考文档：\n1、https://github.com/apache/skywalking-kubernetes\n2、http://skywalking.apache.org/zh/blog/2019-08-30-how-to-use-Skywalking-Agent.html\n3、https://github.com/apache/skywalking/blob/5.x/docs/cn/Deploy-skywalking-agent-CN.md\n","description":"本文主要介绍skywalking的几种使用方式","id":12,"section":"posts","tags":["skywalking","apm"],"title":"Skywalking的几种使用方式","uri":"https://www.coolops.cn/posts/skywalking/"},{"content":"skywalking是什么？为什么要给你的应用加上skywalking?\n在介绍skywalking之前，我们先来了解一个东西，那就是APM（Application Performance Management）系统。\n一、什么是APM系统 APM (Application Performance Management) 即应用性能管理系统，是对企业系统即时监控以实现\n对应用程序性能管理和故障管理的系统化的解决方案。应用性能管理，主要指对企业的关键业务应用进\n行监测、优化，提高企业应用的可靠性和质量，保证用户得到良好的服务，降低IT总拥有成本。\nAPM系统是可以帮助理解系统行为、用于分析性能问题的工具，以便发生故障的时候，能够快速定位和\n解决问题。\n说白了就是随着微服务的的兴起，传统的单体应用拆分为不同功能的小应用，用户的一次请求会经过多个系统，不同服务之间的调用非常复杂，其中任何一个系统出错都可能影响整个请求的处理结果。为了解决这个问题，Google 推出了一个分布式链路跟踪系统 Dapper ，之后各个互联网公司都参照Dapper 的思想推出了自己的分布式链路跟踪系统，而这些系统就是分布式系统下的APM系统。\n目前市面上的APM系统有很多，比如skywalking、pinpoint、zipkin等。其中\n Zipkin：由Twitter公司开源，开放源代码分布式的跟踪系统，用于收集服务的定时数据，以解决微服务架构中的延迟问题，包括：数据的收集、存储、查找和展现。 Pinpoint：一款对Java编写的大规模分布式系统的APM工具，由韩国人开源的分布式跟踪组件。 Skywalking：国产的优秀APM组件，是一个对JAVA分布式应用程序集群的业务运行情况进行追踪、告警和分析的系统。  二、什么是skywalking SkyWalking是apache基金会下面的一个开源APM项目，为微服务架构和云原生架构系统设计。它通过探针自动收集所需的标，并进行分布式追踪。通过这些调用链路以及指标，Skywalking APM会感知应用间关系和服务间关系，并进行相应的指标统计。Skywalking支持链路追踪和监控应用组件基本涵盖主流框架和容器，如国产RPC Dubbo和motan等，国际化的spring boot，spring cloud。官方网站：http://skywalking.apache.org/\nSkywalking的具有以下几个特点：\n 多语言自动探针，Java，.NET Core和Node.JS。 多种监控手段，语言探针和service mesh。 轻量高效。不需要额外搭建大数据平台。 模块化架构。UI、存储、集群管理多种机制可选。 支持告警。 优秀的可视化效果。  Skywalking整体架构如下：\n整体架构包含如下三个组成部分：\n\\1. 探针(agent)负责进行数据的收集，包含了Tracing和Metrics的数据，agent会被安装到服务所在的服务器上，以方便数据的获取。\n\\2. 可观测性分析平台OAP(Observability Analysis Platform)，接收探针发送的数据，并在内存中使用分析引擎（Analysis Core)进行数据的整合运算，然后将数据存储到对应的存储介质上，比如Elasticsearch、MySQL数据库、H2数据库等。同时OAP还使用查询引擎(Query Core)提供HTTP查询接口。\n\\3. Skywalking提供单独的UI进行数据的查看，此时UI会调用OAP提供的接口，获取对应的数据然后进行展示。\n三、搭建并使用 搭建其实很简单，官方有提供搭建案例。\n上文提到skywalking的后端数据存储的介质可以是Elasticsearch、MySQL数据库、H2数据库等，我这里使用Elasticsearch作为数据存储，而且为了便与扩展和收集其他应用日志，我将单独搭建Elasticsearch。\n3.1、搭建elasticsearch 为了增加es的扩展性，按角色功能分为master节点、data数据节点、client客户端节点。其整体架构如下：\n其中：\n Elasticsearch数据节点Pods被部署为一个有状态集（StatefulSet） Elasticsearch master节点Pods被部署为一个Deployment Elasticsearch客户端节点Pods是以Deployment的形式部署的，其内部服务将允许访问R/W请求的数据节点 Kibana部署为Deployment，其服务可在Kubernetes集群外部访问  （1）先创建estatic的命名空间（es-ns.yaml）：\napiVersion: v1\rkind: Namespace\rmetadata:\rname: elastic\r执行kubectl apply -f es-ns.yaml\n（2）部署es master\n配置清单如下（es-master.yaml）：\n---\rapiVersion: v1\rkind: ConfigMap\rmetadata:\rnamespace: elastic\rname: elasticsearch-master-config\rlabels:\rapp: elasticsearch\rrole: master\rdata:\relasticsearch.yml: |-\rcluster.name: ${CLUSTER_NAME}\rnode.name: ${NODE_NAME}\rdiscovery.seed_hosts: ${NODE_LIST}\rcluster.initial_master_nodes: ${MASTER_NODES}\rnetwork.host: 0.0.0.0\rnode:\rmaster: true\rdata: false\ringest: false\rxpack.security.enabled: true\rxpack.monitoring.collection.enabled: true\r---\rapiVersion: v1\rkind: Service\rmetadata:\rnamespace: elastic\rname: elasticsearch-master\rlabels:\rapp: elasticsearch\rrole: master\rspec:\rports:\r- port: 9300\rname: transport\rselector:\rapp: elasticsearch\rrole: master\r---\rapiVersion: apps/v1\rkind: Deployment\rmetadata:\rnamespace: elastic\rname: elasticsearch-master\rlabels:\rapp: elasticsearch\rrole: master\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: elasticsearch\rrole: master\rtemplate:\rmetadata:\rlabels:\rapp: elasticsearch\rrole: master\rspec:\rinitContainers:\r- name: init-sysctl\rimage: busybox:1.27.2\rcommand:\r- sysctl\r- -w\r- vm.max_map_count=262144\rsecurityContext:\rprivileged: true\rcontainers:\r- name: elasticsearch-master\rimage: docker.elastic.co/elasticsearch/elasticsearch:7.8.0\renv:\r- name: CLUSTER_NAME\rvalue: elasticsearch\r- name: NODE_NAME\rvalue: elasticsearch-master\r- name: NODE_LIST\rvalue: elasticsearch-master,elasticsearch-data,elasticsearch-client\r- name: MASTER_NODES\rvalue: elasticsearch-master\r- name: \u0026quot;ES_JAVA_OPTS\u0026quot;\rvalue: \u0026quot;-Xms512m -Xmx512m\u0026quot;\rports:\r- containerPort: 9300\rname: transport\rvolumeMounts:\r- name: config\rmountPath: /usr/share/elasticsearch/config/elasticsearch.yml\rreadOnly: true\rsubPath: elasticsearch.yml\r- name: storage\rmountPath: /data\rvolumes:\r- name: config\rconfigMap:\rname: elasticsearch-master-config\r- name: \u0026quot;storage\u0026quot;\remptyDir:\rmedium: \u0026quot;\u0026quot;\r---\r然后执行kubectl apply -f ``es-master.yaml创建配置清单，然后pod变为running状态即为部署成功。\n# kubectl get pod -n elastic\rNAME READY STATUS RESTARTS AGE\relasticsearch-master-77d5d6c9db-xt5kq 1/1 Running 0 67s\r（3）部署es data\n配置清单如下（es-data.yaml）：\n---\rapiVersion: v1\rkind: ConfigMap\rmetadata:\rnamespace: elastic\rname: elasticsearch-data-config\rlabels:\rapp: elasticsearch\rrole: data\rdata:\relasticsearch.yml: |-\rcluster.name: ${CLUSTER_NAME}\rnode.name: ${NODE_NAME}\rdiscovery.seed_hosts: ${NODE_LIST}\rcluster.initial_master_nodes: ${MASTER_NODES}\rnetwork.host: 0.0.0.0\rnode:\rmaster: false\rdata: true\ringest: false\rxpack.security.enabled: true\rxpack.monitoring.collection.enabled: true\r---\rapiVersion: v1\rkind: Service\rmetadata:\rnamespace: elastic\rname: elasticsearch-data\rlabels:\rapp: elasticsearch\rrole: data\rspec:\rports:\r- port: 9300\rname: transport\rselector:\rapp: elasticsearch\rrole: data\r---\rapiVersion: apps/v1\rkind: StatefulSet\rmetadata:\rnamespace: elastic\rname: elasticsearch-data\rlabels:\rapp: elasticsearch\rrole: data\rspec:\rserviceName: \u0026quot;elasticsearch-data\u0026quot;\rselector:\rmatchLabels:\rapp: elasticsearch\rrole: data\rtemplate:\rmetadata:\rlabels:\rapp: elasticsearch\rrole: data\rspec:\rinitContainers:\r- name: init-sysctl\rimage: busybox:1.27.2\rcommand:\r- sysctl\r- -w\r- vm.max_map_count=262144\rsecurityContext:\rprivileged: true\rcontainers:\r- name: elasticsearch-data\rimage: docker.elastic.co/elasticsearch/elasticsearch:7.8.0\renv:\r- name: CLUSTER_NAME\rvalue: elasticsearch\r- name: NODE_NAME\rvalue: elasticsearch-data\r- name: NODE_LIST\rvalue: elasticsearch-master,elasticsearch-data,elasticsearch-client\r- name: MASTER_NODES\rvalue: elasticsearch-master\r- name: \u0026quot;ES_JAVA_OPTS\u0026quot;\rvalue: \u0026quot;-Xms1024m -Xmx1024m\u0026quot;\rports:\r- containerPort: 9300\rname: transport\rvolumeMounts:\r- name: config\rmountPath: /usr/share/elasticsearch/config/elasticsearch.yml\rreadOnly: true\rsubPath: elasticsearch.yml\r- name: elasticsearch-data-persistent-storage\rmountPath: /data/db\rvolumes:\r- name: config\rconfigMap:\rname: elasticsearch-data-config\rvolumeClaimTemplates:\r- metadata:\rname: elasticsearch-data-persistent-storage\rspec:\raccessModes: [ \u0026quot;ReadWriteOnce\u0026quot; ]\rstorageClassName: managed-nfs-storage\rresources:\rrequests:\rstorage: 20Gi\r---\r执行kubectl apply -f es-data.yaml创建配置清单，其状态变为running即为部署成功。\n# kubectl get pod -n elastic\rNAME READY STATUS RESTARTS AGE\relasticsearch-data-0 1/1 Running 0 4s\relasticsearch-master-77d5d6c9db-gklgd 1/1 Running 0 2m35s\relasticsearch-master-77d5d6c9db-gvhcb 1/1 Running 0 2m35s\relasticsearch-master-77d5d6c9db-pflz6 1/1 Running 0 2m35s\r（4）部署es client\n配置清单如下（es-client.yaml）：\n---\rapiVersion: v1\rkind: ConfigMap\rmetadata:\rnamespace: elastic\rname: elasticsearch-client-config\rlabels:\rapp: elasticsearch\rrole: client\rdata:\relasticsearch.yml: |-\rcluster.name: ${CLUSTER_NAME}\rnode.name: ${NODE_NAME}\rdiscovery.seed_hosts: ${NODE_LIST}\rcluster.initial_master_nodes: ${MASTER_NODES}\rnetwork.host: 0.0.0.0\rnode:\rmaster: false\rdata: false\ringest: true\rxpack.security.enabled: true\rxpack.monitoring.collection.enabled: true\r---\rapiVersion: v1\rkind: Service\rmetadata:\rnamespace: elastic\rname: elasticsearch-client\rlabels:\rapp: elasticsearch\rrole: client\rspec:\rports:\r- port: 9200\rname: client\r- port: 9300\rname: transport\rselector:\rapp: elasticsearch\rrole: client\r---\rapiVersion: apps/v1\rkind: Deployment\rmetadata:\rnamespace: elastic\rname: elasticsearch-client\rlabels:\rapp: elasticsearch\rrole: client\rspec:\rselector:\rmatchLabels:\rapp: elasticsearch\rrole: client\rtemplate:\rmetadata:\rlabels:\rapp: elasticsearch\rrole: client\rspec:\rinitContainers:\r- name: init-sysctl\rimage: busybox:1.27.2\rcommand:\r- sysctl\r- -w\r- vm.max_map_count=262144\rsecurityContext:\rprivileged: true\rcontainers:\r- name: elasticsearch-client\rimage: docker.elastic.co/elasticsearch/elasticsearch:7.8.0\renv:\r- name: CLUSTER_NAME\rvalue: elasticsearch\r- name: NODE_NAME\rvalue: elasticsearch-client\r- name: NODE_LIST\rvalue: elasticsearch-master,elasticsearch-data,elasticsearch-client\r- name: MASTER_NODES\rvalue: elasticsearch-master\r- name: \u0026quot;ES_JAVA_OPTS\u0026quot;\rvalue: \u0026quot;-Xms256m -Xmx256m\u0026quot;\rports:\r- containerPort: 9200\rname: client\r- containerPort: 9300\rname: transport\rvolumeMounts:\r- name: config\rmountPath: /usr/share/elasticsearch/config/elasticsearch.yml\rreadOnly: true\rsubPath: elasticsearch.yml\r- name: storage\rmountPath: /data\rvolumes:\r- name: config\rconfigMap:\rname: elasticsearch-client-config\r- name: \u0026quot;storage\u0026quot;\remptyDir:\rmedium: \u0026quot;\u0026quot;\r执行kubectl apply -f es-client.yaml创建配置清单，其状态变为running即为部署成功。\n# kubectl get pod -n elastic\rNAME READY STATUS RESTARTS AGE\relasticsearch-client-f79cf4f7b-pbz9d 1/1 Running 0 5s\relasticsearch-data-0 1/1 Running 0 3m11s\relasticsearch-master-77d5d6c9db-gklgd 1/1 Running 0 5m42s\relasticsearch-master-77d5d6c9db-gvhcb 1/1 Running 0 5m42s\relasticsearch-master-77d5d6c9db-pflz6 1/1 Running 0 5m42s\r（5）生成密码\n我们启用了 xpack 安全模块来保护我们的集群，所以我们需要一个初始化的密码。我们可以执行如下所示的命令，在客户端节点容器内运行 bin/elasticsearch-setup-passwords 命令来生成默认的用户名和密码：\n# kubectl exec $(kubectl get pods -n elastic | grep elasticsearch-client | sed -n 1p | awk '{print $1}') \\\r-n elastic \\\r-- bin/elasticsearch-setup-passwords auto -b\rChanged password for user apm_system\rPASSWORD apm_system = QNSdaanAQ5fvGMrjgYnM\rChanged password for user kibana_system\rPASSWORD kibana_system = UFPiUj0PhFMCmFKvuJuc\rChanged password for user kibana\rPASSWORD kibana = UFPiUj0PhFMCmFKvuJuc\rChanged password for user logstash_system\rPASSWORD logstash_system = Nqes3CCxYFPRLlNsuffE\rChanged password for user beats_system\rPASSWORD beats_system = Eyssj5NHevFjycfUsPnT\rChanged password for user remote_monitoring_user\rPASSWORD remote_monitoring_user = 7Po4RLQQZ94fp7F31ioR\rChanged password for user elastic\rPASSWORD elastic = n816QscHORFQMQWQfs4U\r注意需要将 elastic 用户名和密码也添加到 Kubernetes 的 Secret 对象中：\nkubectl create secret generic elasticsearch-pw-elastic \\\r-n elastic \\\r--from-literal password=n816QscHORFQMQWQfs4U\r（6）、验证集群状态\nkubectl exec -n elastic \\\r$(kubectl get pods -n elastic | grep elasticsearch-client | sed -n 1p | awk '{print $1}') \\\r-- curl -u elastic:elastic@123456 http://elasticsearch-client.elastic:9200/_cluster/health?pretty\r{\r\u0026quot;cluster_name\u0026quot; : \u0026quot;elasticsearch\u0026quot;,\r\u0026quot;status\u0026quot; : \u0026quot;green\u0026quot;,\r\u0026quot;timed_out\u0026quot; : false,\r\u0026quot;number_of_nodes\u0026quot; : 3,\r\u0026quot;number_of_data_nodes\u0026quot; : 1,\r\u0026quot;active_primary_shards\u0026quot; : 2,\r\u0026quot;active_shards\u0026quot; : 2,\r\u0026quot;relocating_shards\u0026quot; : 0,\r\u0026quot;initializing_shards\u0026quot; : 0,\r\u0026quot;unassigned_shards\u0026quot; : 0,\r\u0026quot;delayed_unassigned_shards\u0026quot; : 0,\r\u0026quot;number_of_pending_tasks\u0026quot; : 0,\r\u0026quot;number_of_in_flight_fetch\u0026quot; : 0,\r\u0026quot;task_max_waiting_in_queue_millis\u0026quot; : 0,\r\u0026quot;active_shards_percent_as_number\u0026quot; : 100.0\r}\r上面status的状态为green，表示集群正常。到这里ES集群就搭建完了。为了方便操作可以再部署一个kibana服务，如下：\n---\rapiVersion: v1\rkind: ConfigMap\rmetadata:\rnamespace: elastic\rname: kibana-config\rlabels:\rapp: kibana\rdata:\rkibana.yml: |-\rserver.host: 0.0.0.0\relasticsearch:\rhosts: ${ELASTICSEARCH_HOSTS}\rusername: ${ELASTICSEARCH_USER}\rpassword: ${ELASTICSEARCH_PASSWORD}\r---\rapiVersion: v1\rkind: Service\rmetadata:\rnamespace: elastic\rname: kibana\rlabels:\rapp: kibana\rspec:\rports:\r- port: 5601\rname: webinterface\rselector:\rapp: kibana\r---\rapiVersion: networking.k8s.io/v1beta1\rkind: Ingress\rmetadata:\rannotations:\rprometheus.io/http-probe: 'true'\rprometheus.io/scrape: 'true'\rname: kibana\rnamespace: elastic\rspec:\rrules:\r- host: kibana.coolops.cn\rhttp:\rpaths:\r- backend:\rserviceName: kibana\rservicePort: 5601 path: /\r---\rapiVersion: apps/v1\rkind: Deployment\rmetadata:\rnamespace: elastic\rname: kibana\rlabels:\rapp: kibana\rspec:\rselector:\rmatchLabels:\rapp: kibana\rtemplate:\rmetadata:\rlabels:\rapp: kibana\rspec:\rcontainers:\r- name: kibana\rimage: docker.elastic.co/kibana/kibana:7.8.0\rports:\r- containerPort: 5601\rname: webinterface\renv:\r- name: ELASTICSEARCH_HOSTS\rvalue: \u0026quot;http://elasticsearch-client.elastic.svc.cluster.local:9200\u0026quot;\r- name: ELASTICSEARCH_USER\rvalue: \u0026quot;elastic\u0026quot;\r- name: ELASTICSEARCH_PASSWORD\rvalueFrom:\rsecretKeyRef:\rname: elasticsearch-pw-elastic\rkey: password\rvolumeMounts:\r- name: config\rmountPath: /usr/share/kibana/config/kibana.yml\rreadOnly: true\rsubPath: kibana.yml\rvolumes:\r- name: config\rconfigMap:\rname: kibana-config\r---\r然后执行kubectl apply -f kibana.yaml创建kibana，查看pod的状态是否为running。\n# kubectl get pod -n elastic NAME READY STATUS RESTARTS AGE\relasticsearch-client-f79cf4f7b-pbz9d 1/1 Running 0 30m\relasticsearch-data-0 1/1 Running 0 33m\relasticsearch-master-77d5d6c9db-gklgd 1/1 Running 0 36m\relasticsearch-master-77d5d6c9db-gvhcb 1/1 Running 0 36m\relasticsearch-master-77d5d6c9db-pflz6 1/1 Running 0 36m\rkibana-6b9947fccb-4vp29 1/1 Running 0 3m51s\r如下图所示，使用上面我们创建的 Secret 对象的 elastic 用户和生成的密码即可登录：\n登录后界面如下：\n3.2、搭建skywalking server  我这里使用helm安装\n （1）安装helm，这里是使用的helm3\nwget https://get.helm.sh/helm-v3.0.0-linux-amd64.tar.gz\rtar zxvf helm-v3.0.0-linux-amd64.tar.gz\rmv linux-amd64/helm /usr/bin/\r 说明：helm3没有tiller这个服务端了，直接用kubeconfig进行验证通信，所以建议部署在master节点\n （2）下载skywalking的代码\nmkdir /home/install/package -p\rcd /home/install/package\rgit clone https://github.com/apache/skywalking-kubernetes.git\r（3）进入chart目录进行安装\ncd skywalking-kubernetes/chart\rhelm repo add elastic https://helm.elastic.co\rhelm dep up skywalking\rhelm install my-skywalking skywalking -n skywalking \\\r--set elasticsearch.enabled=false \\\r--set elasticsearch.config.host=elasticsearch-client.elastic.svc.cluster.local \\\r--set elasticsearch.config.port.http=9200 \\\r--set elasticsearch.config.user=elastic \\\r--set elasticsearch.config.password=n816QscHORFQMQWQfs4U\r 先要创建一个skywalking的namespace： kubectl create ns skywalking\n （4）查看所有pod是否处于running\n# kubectl get pod\rNAME READY STATUS RESTARTS AGE\rmy-skywalking-es-init-x89pr 0/1 Completed 0 15h\rmy-skywalking-oap-694fc79d55-2dmgr 1/1 Running 0 16h\rmy-skywalking-oap-694fc79d55-bl5hk 1/1 Running 4 16h\rmy-skywalking-ui-6bccffddbd-d2xhs 1/1 Running 0 16h\r也可以通过以下命令来查看chart。\n# helm list --all-namespaces\rNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION\rmy-skywalking skywalking 1 2020-09-29 14:42:10.952238898 +0800 CST deployed skywalking-3.1.0 8.1.0\r如果要修改配置，则直接修改value.yaml，如下我们修改my-skywalking-ui的service为NodePort，则如下修改：\n.....\rui:\rname: ui\rreplicas: 1\rimage:\rrepository: apache/skywalking-ui\rtag: 8.1.0\rpullPolicy: IfNotPresent\r....\rservice:\rtype: NodePort # clusterIP: None\rexternalPort: 80\rinternalPort: 8080\r....\r然后使用以下命名升级即可。\nhelm upgrade sky-server ../skywalking -n skywalking\r然后我们可以查看service是否变为NodePort了。\n# kubectl get svc -n skywalking NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rmy-skywalking-oap ClusterIP 10.109.109.131 \u0026lt;none\u0026gt; 12800/TCP,11800/TCP 88s\rmy-skywalking-ui NodePort 10.102.247.110 \u0026lt;none\u0026gt; 80:32563/TCP 88s\r现在就可以通过UI界面查看skywalking了，界面如下：\n3.3、应用接入skywalking agent 现在skywalking的服务端已经安装好了，接下来就是应用接入了，所谓的应用接入就是应用在启动的时候加入skywalking agent，在容器中接入agent的方式我这里介绍两种。\n 在制作应用镜像的时候把agent所需的文件和包一起打进去 以sidecar的形式给应用容器接入agent  首先我们应该下载对应的agent软件包：\nwget https://mirrors.tuna.tsinghua.edu.cn/apache/skywalking/8.1.0/apache-skywalking-apm-8.1.0.tar.gz\rtar xf apache-skywalking-apm-8.1.0.tar.gz\r（1）在制作应用镜像的时候把agent所需的文件和包一起打进去\n开发类似下面的Dockerfile，然后直接build镜像即可，这种方法比较简单\nFROM harbor-test.coolops.com/coolops/jdk:8u144_test\rRUN mkdir -p /usr/skywalking/agent/\rADD apache-skywalking-apm-bin/agent/ /usr/skywalking/agent/\r 注意：这个Dockerfile是咱们应用打包的基础镜像，不是应用的Dockerfile\n （2）、以sidecar的形式添加agent包\n首先制作一个只有agent的镜像，如下：\nFROM busybox:latest ENV LANG=C.UTF-8\rRUN set -eux \u0026amp;\u0026amp; mkdir -p /usr/skywalking/agent/\rADD apache-skywalking-apm-bin/agent/ /usr/skywalking/agent/\rWORKDIR /\r然后我们像下面这样开发deployment的yaml清单。\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rlabels:\rname: demo-sw\rname: demo-sw\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rname: demo-sw\rtemplate:\rmetadata:\rlabels:\rname: demo-sw\rspec:\rinitContainers:\r- image: innerpeacez/sw-agent-sidecar:latest\rname: sw-agent-sidecar\rimagePullPolicy: IfNotPresent\rcommand: ['sh']\rargs: ['-c','mkdir -p /skywalking/agent \u0026amp;\u0026amp; cp -r /usr/skywalking/agent/* /skywalking/agent']\rvolumeMounts:\r- mountPath: /skywalking/agent\rname: sw-agent\rcontainers:\r- image: harbor.coolops.cn/skywalking-java:1.7.9\rname: demo\rcommand:\r- java -javaagent:/usr/skywalking/agent/skywalking-agent.jar -Dskywalking.agent.service_name=${SW_AGENT_NAME} -jar demo.jar\rvolumeMounts:\r- mountPath: /usr/skywalking/agent\rname: sw-agent\rports:\r- containerPort: 80\renv:\r- name: SW_AGENT_COLLECTOR_BACKEND_SERVICES\rvalue: 'my-skywalking-oap.skywalking.svc.cluster.local:11800'\r- name: SW_AGENT_NAME\rvalue: cartechfin-open-platform-skywalking\rvolumes:\r- name: sw-agent\remptyDir: {}\r我们在启动应用的时候只要引入skywalking的javaagent即可，如下：\njava -javaagent:/path/to/skywalking-agent/skywalking-agent.jar -Dskywalking.agent.service_name=${SW_AGENT_NAME} -jar yourApp.jar\r然后我们就可以在UI界面看到注册上来的应用了，如下：\n可以查看JVM数据，如下：\n也可以查看其拓扑图，如下：\n还可以追踪不同的uri，如下：\n到这里整个服务就搭建完了，你也可以试一下。\n参考文档：\n1、https://github.com/apache/skywalking-kubernetes\n2、http://skywalking.apache.org/zh/blog/2019-08-30-how-to-use-Skywalking-Agent.html\n3、https://github.com/apache/skywalking/blob/5.x/docs/cn/Deploy-skywalking-agent-CN.md\n","description":"本文介绍什么是apm，什么是skywalking以及其搭建过程","id":13,"section":"posts","tags":["kubernetes","skywalking","apm"],"title":"在Kubernetes中使用skywalking进行链路监控","uri":"https://www.coolops.cn/posts/skywalking-in-kubernetes/"},{"content":" 前提：有可用的kubernetes集群和skywalking监控。\n 软件版本信息：\n   软件 版本     kubernetes 1.17.2   nginx-ingress-controller 0.34.1   skywalking 8.1.0   skywalking-nginx-lua 0.2.0    下面直接进入正题\u0026hellip;\u0026hellip;\n（1）下载skywalking-nginx-lua\ngit clone https://github.com/apache/skywalking-nginx-lua.git\r（2）修改util.lua 文件名，因为其和nginx-ingress默认的一个lua脚本名字冲突，这里改一下。\n$ skywalking-nginx-lua/lib/skywalking\r# 修改文件名\r$ mv util.lua swutil.lua\r# 修改文件调用\r## 可以使用grep查看一下哪些文件有调用这个lua\r$ grep util `find ./ -type f`\r./correlation_context.lua:local Util = require('util')\r./segment_ref.lua:local Util = require('util')\r./span.lua:local Util = require('util')\r./tracing_context.lua:local Util = require('util')\r./swutil_test.lua:local Util = require('util')\r# 将里面调用的util都改为swuitl\r（3）修改Nginx-ingress的nginx.tmpl模板文件，增加Skywalking的配置。\n 添加Skywalking Lua脚本扫描路径 增加环境变量读取，如：SW_SERVICE_NAME、SW_SERVICE_INSTANCE_NAME、SW_BACKEND_SERVERS 添加Tracing使用的缓存tracing_buffer 设置Skywalking Lua Agent的初始化方法，并将相关配置从环境变量中提取。 设置http节点的追踪配置。  # Skywalking ENV\renv SW_SERVICE_NAME;\renv SW_SERVICE_INSTANCE_NAME;\renv SW_BACKEND_SERVERS;\revents {\rmulti_accept {{ if $cfg.EnableMultiAccept }}on{{ else }}off{{ end }};\rworker_connections {{ $cfg.MaxWorkerConnections }};\ruse epoll;\r}\rhttp {\r# 引入lua脚本\rlua_package_path \u0026quot;/etc/nginx/lua/?.lua;/etc/nginx/lua/skywalking/?.lua;;\u0026quot;;\r# 使用缓存\rlua_shared_dict tracing_buffer 100m;\r{{ buildLuaSharedDictionaries $cfg $servers }}\rinit_by_lua_block {\rcollectgarbage(\u0026quot;collect\u0026quot;)\r-- init modules\rlocal ok, res\rok, res = pcall(require, \u0026quot;lua_ingress\u0026quot;)\rif not ok then\rerror(\u0026quot;require failed: \u0026quot; .. tostring(res))\relse\rlua_ingress = res\rlua_ingress.set_config({{ configForLua $all }})\rend\rok, res = pcall(require, \u0026quot;configuration\u0026quot;)\rif not ok then\rerror(\u0026quot;require failed: \u0026quot; .. tostring(res))\relse\rconfiguration = res\rend\rok, res = pcall(require, \u0026quot;balancer\u0026quot;)\rif not ok then\rerror(\u0026quot;require failed: \u0026quot; .. tostring(res))\relse\rbalancer = res\rend\r{{ if $all.EnableMetrics }}\rok, res = pcall(require, \u0026quot;monitor\u0026quot;)\rif not ok then\rerror(\u0026quot;require failed: \u0026quot; .. tostring(res))\relse\rmonitor = res\rend\r{{ end }}\rok, res = pcall(require, \u0026quot;certificate\u0026quot;)\rif not ok then\rerror(\u0026quot;require failed: \u0026quot; .. tostring(res))\relse\rcertificate = res\rcertificate.is_ocsp_stapling_enabled = {{ $cfg.EnableOCSP }}\rend\rok, res = pcall(require, \u0026quot;plugins\u0026quot;)\rif not ok then\rerror(\u0026quot;require failed: \u0026quot; .. tostring(res))\relse\rplugins = res\rend\r-- load all plugins that'll be used here\rplugins.init({ {{ range $idx, $plugin := $cfg.Plugins }}{{ if $idx }},{{ end }}{{ $plugin | quote }}{{ end }} })\r}\rinit_worker_by_lua_block {\r......\r-- Skywalking\rlocal metadata_buffer = ngx.shared.tracing_buffer\rmetadata_buffer:set('serviceName', os.getenv(\u0026quot;SW_SERVICE_NAME\u0026quot;))\rmetadata_buffer:set('serviceInstanceName', os.getenv(\u0026quot;SW_SERVICE_INSTANCE_NAME\u0026quot;))\r-- set random seed\rrequire(\u0026quot;swutil\u0026quot;).set_randomseed()\rrequire(\u0026quot;client\u0026quot;):startBackendTimer(os.getenv(\u0026quot;SW_BACKEND_SERVERS\u0026quot;))\r}\r...... rewrite_by_lua_block {\rlua_ingress.rewrite({{ locationConfigForLua $location $all }})\rbalancer.rewrite()\rplugins.run()\r-- Skywalking\rrequire(\u0026quot;tracer\u0026quot;):start({{ buildUpstreamName $location | quote }})\r}\r...... body_filter_by_lua_block {\r-- Skywalking\rif ngx.arg[2] then\rrequire(\u0026quot;tracer\u0026quot;):finish()\rend\r}\r...... log_by_lua_block {\rbalancer.log()\r{{ if $all.EnableMetrics }}\rmonitor.call()\r{{ end }}\rplugins.run()\r-- Skywalking\rrequire(\u0026quot;tracer\u0026quot;):prepareForReport()\r}\r.......\r（4）将这些脚本添加到nginx-ingress镜像中，Dockerfile如下：\nFROM quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0\rADD --chown=www-data nginx.tmpl /etc/nginx/template\rADD --chown=www-data skywalking /etc/nginx/lua/skywalking\r（5）构建镜像并上传仓库\n$ docker build -t registry.cn-hangzhou.aliyuncs.com/rookieops/ingress-nginx-controller-skywalking:0.34.1 .\r$ docker push registry.cn-hangzhou.aliyuncs.com/rookieops/ingress-nginx-controller-skywalking:0.34.1\r（6）修改nginx-ingress的deployment文件，主要增加以下环境变量\n......\rcontainers:\r- name: controller\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/nginx-ingress-controller:0.32.0\rimagePullPolicy: IfNotPresent\r......\r- name: SW_SERVICE_NAME\rvalue: Kubernetes Ingress\r- name: SW_BACKEND_SERVERS\rvalue: http://skywalking-oap.skywalking.svc.cluster.local:12800\r- name: SW_SERVICE_INSTANCE_NAME\rvalueFrom:\rfieldRef:\rfieldPath: metadata.uid\r......\r（7）然后重新部署ingress-controller应用。\n然后可以在skywalking的面板上看到了。\n已将所需的代码都放在github了，仓库地址：https://github.com/sunsharing-note/skywalking-ingress.git\n参考：\n https://github.com/lipangeng/Skywalking-Ingress-Overlay https://github.com/apache/skywalking-nginx-lua   ","description":"本文主要介紹如何使用skywalking監控nginx-ingress。","id":14,"section":"posts","tags":["skywalking","nginx-lua","nginx-ingress"],"title":"使用skywalking監控nginx-ingress","uri":"https://www.coolops.cn/posts/skywalking-nginx-ingress/"},{"content":"在kubernetes中,对于日志的收集，对于日志的收集，使用最多的是FEK， 不过有时候，FEK在架构上会略显重， ES的查询及全文检索功能其实使用的不是很多。LoKi做为日志架构的新面孔，由grafana开源， 使用了与Prometheus同样的label理念, 同时摒弃了全文检索的能力， 因此比较轻便, 非常具有潜力。\nlike Prometheus, but for logs Loki是 Grafana Labs 团队最新的开源项目，是一个水平可扩展，高可用性，多租户的日志聚合系统。它的设计非常经济高效且易于操作，因为它不会为日志内容编制索引，而是为每个日志流编制一组标签。项目受 Prometheus 启发，官方的介绍就是：Like Prometheus, but for logs，类似于 Prometheus 的日志系统\n与其他日志聚合系统相比，Loki具有下面的一些特性：\n 不对日志进行全文索引。通过存储压缩非结构化日志和仅索引元数据，Loki 操作起来会更简单，更省成本。 通过使用与 Prometheus 相同的标签记录流对日志进行索引和分组，这使得日志的扩展和操作效率更高。 特别适合储存 Kubernetes Pod 日志; 诸如 Pod 标签之类的元数据会被自动删除和编入索引。 受 Grafana 原生支持。  Loki 由以下3个部分组成：\n loki是主服务器，负责存储日志和处理查询。 promtail是代理，负责收集日志并将其发送给 loki，当然也支持其它的收集端如fluentd等 Grafana用于 UI 展示  同时Loki也提示了command line工具，通过这个工具可以使用http的方式与loki进行交互。\n架构 \n安装 官方提供了多种的部署方式, 这里选择使用helm, 如果只是想试用的话则非常简单, 直接参考helm即可run起来。\nhelm repo add loki https://grafana.github.io/loki/charts\rhelm repo update\rhelm upgrade --install loki loki/loki-stack\r我这里为了方便配置，就将其下载下来了，使用如下命令\nhelm pull loki/loki-stack\rtar xf loki-stack-2.1.2.tgz\rhelm install loki loki-stack/\r配置Nginx-Ingress 这里将NG的日志落盘，便与处理。\n（1）、修改ConfigMap，如下：\n# Source: ingress-nginx/templates/controller-configmap.yaml\rapiVersion: v1\rkind: ConfigMap\rmetadata:\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rapp.kubernetes.io/component: controller\rname: ingress-nginx-controller\rnamespace: ingress-nginx\rdata:\rcompute-full-forwarded-for: 'true'\renable-underscores-in-headers: 'true'\rclient_max_body_size: \u0026quot;100m\u0026quot;\rproxy_body_size: \u0026quot;100m\u0026quot;\raccess-log-path: /var/log/nginx/access.log\rlog-format-escape-json: \u0026quot;true\u0026quot;\rlog-format-upstream: '{\u0026quot;timestamp\u0026quot;: \u0026quot;$time_iso8601\u0026quot;, \u0026quot;requestID\u0026quot;: \u0026quot;$req_id\u0026quot;, \u0026quot;proxyUpstreamName\u0026quot;:\r\u0026quot;$proxy_upstream_name\u0026quot;,\u0026quot;host\u0026quot;: \u0026quot;$host\u0026quot;,\u0026quot;body_bytes_sent\u0026quot;: \u0026quot;$body_bytes_sent\u0026quot;,\u0026quot;proxyAlternativeUpstreamName\u0026quot;: \u0026quot;$proxy_alternative_upstream_name\u0026quot;,\u0026quot;upstreamStatus\u0026quot;:\r\u0026quot;$upstream_status\u0026quot;, \u0026quot;geoip_country_code\u0026quot;: \u0026quot;$geoip_country_code\u0026quot;,\u0026quot;upstreamAddr\u0026quot;: \u0026quot;$upstream_addr\u0026quot;,\u0026quot;request_time\u0026quot;:\r\u0026quot;$request_time\u0026quot;,\u0026quot;httpRequest\u0026quot;:{\u0026quot;requestMethod\u0026quot;: \u0026quot;$request_method\u0026quot;, \u0026quot;requestUrl\u0026quot;:\r\u0026quot;$request_uri\u0026quot;, \u0026quot;status\u0026quot;: $status,\u0026quot;requestSize\u0026quot;: \u0026quot;$request_length\u0026quot;, \u0026quot;responseSize\u0026quot;:\r\u0026quot;$upstream_response_length\u0026quot;, \u0026quot;userAgent\u0026quot;: \u0026quot;$http_user_agent\u0026quot;, \u0026quot;remoteIp\u0026quot;: \u0026quot;$remote_addr\u0026quot;,\r\u0026quot;referer\u0026quot;: \u0026quot;$http_referer\u0026quot;, \u0026quot;latency\u0026quot;: \u0026quot;$upstream_response_time\u0026quot;, \u0026quot;protocol\u0026quot;:\u0026quot;$server_protocol\u0026quot;}}'\r（2）修改NG的deployment，主要增加如下配置\n# Source: ingress-nginx/templates/controller-deployment.yaml\rapiVersion: apps/v1\rkind: Deployment\rmetadata:\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rapp.kubernetes.io/component: controller\rname: ingress-nginx-controller\rnamespace: ingress-nginx\rspec:\r......\rinitContainers:\r- name: adddirperm\rimage: busybox\rcommand:\r- /bin/sh\r- -c\r- chown -R ${USER_ID}:${USER_ID} ${LOG_DIR}\renv:\r- name: LOG_DIR\rvalue: /var/log/nginx\r- name: USER_ID\rvalue: \u0026quot;101\u0026quot;\rvolumeMounts:\r- name: logdir\rmountPath: /var/log/nginx\rcontainers:\r- name: controller\rimage: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0\rimagePullPolicy: IfNotPresent\rlifecycle:\rpreStop:\rexec:\rcommand:\r- /wait-shutdown\rargs:\r- /nginx-ingress-controller\r- --publish-service=ingress-nginx/ingress-nginx-controller\r- --election-id=ingress-controller-leader\r- --ingress-class=nginx\r- --configmap=ingress-nginx/ingress-nginx-controller\r- --validating-webhook=:8443\r- --validating-webhook-certificate=/usr/local/certificates/cert\r- --validating-webhook-key=/usr/local/certificates/key\r- --log_dir=/var/log/nginx\r- --logtostderr=false\r......\rvolumeMounts:\r- name: webhook-cert\rmountPath: /usr/local/certificates/\rreadOnly: true\r- name: logdir\rmountPath: /var/log/nginx\rresources:\rrequests:\rcpu: 100m\rmemory: 90Mi\rserviceAccountName: ingress-nginx\rterminationGracePeriodSeconds: 300\rvolumes:\r......\r- name: logdir\rhostPath:\rpath: /var/log/nginx\rtype: DirectoryOrCreate\r重新创建后即可在本地的/var/log/nginx下查找到日志。\n配置promtail收集日志 在chart根目录下的value.yaml，将promtail模块修改如下：\npromtail:\renabled: true\rnodeSelector:\rkubernetes.io/hostname: \u0026quot;k8s-master-134\u0026quot;\rvolumeMounts:\r- name: nginx\rmountPath: /var/log/nginx\rreadOnly: true\rvolumes:\r- name: nginx\rhostPath:\rpath: /var/log/nginx\rscrapeConfigs:\r- job_name: kubernetes-nginx-ingress\rstatic_configs:\r- targets:\r- localhost\rlabels:\rjob: nginx_access_log\rapp: ingress-nginx\r__path__: /var/log/nginx/*.log\r其中：\n nodeSelector是nginx-ingress所在的主机，只收集nginx-ingress日志，所以使用了强制调度 volumeMounts是挂载到promtail pod里的目录 volumes是主机节点持久化的目录 scrapeConfigs是抓取的配置，和prometheus很像  然后重新更新loki的chart。\nhelm upgrade loki loki-stack/\r配置Grafana Grafana搭建过程这里就不说了，loki的chart包里可以直接搭建，也可以单独搭建。\n 注意：Grafana版本要高于6.6，不然有些图表无法显示。\n （1）添加数据源\n选择loki，配置URL，URL根据实际部署的情况填写\n（2）在Explore界面查看是否收到日志\n查看是否有对应的app，如下\n点击查看是否能正常收到日志。\n也可以看查看filename label，是否有nginx-ingress的日志\n（3）添加Dashboard\n通过上面的步骤一切准备就绪了，现在就添加Dashboard，我这里将完整的Json文件放在gitee上，地址为：https://gitee.com/coolops/grafana-dashboard/blob/master/loki-nginx-ingress/dashboard.json，可以直接复制导入。也可以导入官方的Dashboard（ID：12559）自己做相应的修改，或者直接用官方的日志格式，就可以直接导入使用了。\n导入过后，展示如下：\n可以根据自己实际情况进行添加或删除。\n","description":"本文主要介绍使用grafana loki展示nginx-ingress日志大盘","id":15,"section":"posts","tags":["nginx-ingress","kubernetes","helm","grafana loki","loki"],"title":"使用loki和grafana展示ingress-nginx的日志","uri":"https://www.coolops.cn/posts/grafana-loki-nginx-ingress/"},{"content":"本文主要介绍使用Jenkins配合argocd以及argo rollouts实现CI/CD。其中jenkins配合argocd做CI/CD前面已经介绍过了，这里不再赘述，不懂的地方可以移步《使用Jenkins和Argocd实现CI/CD》。\n本篇文章新增了如下几个功能：\n 优化argocd的触发CD的速度 使用argo rollouts进行金丝雀发布 给代码仓库打tag  优化Argocd触发CD的速度 Argo CD每三分钟轮询一次Git存储库，以检测清单的变化。为了消除轮询带来的延迟，可以将API服务器配置为接收Webhook事件。Argo CD支持来自GitHub，GitLab，Bitbucket，Bitbucket Server和Gogs的Git Webhook通知，更多点击官网。\n我这里使用Gitlab作为仓库地址。\n（1）在argocd中配置webhook token\n使用kubectl edit secret argocd-secret -n argocd命令进行配置：\napiVersion: v1\rkind: Secret\rmetadata:\rname: argocd-secret\rnamespace: argocd\rtype: Opaque\rdata:\r...\rstringData:\r# gitlab webhook secret\rwebhook.gitlab.secret: coolops\r配置完点击保存会自动生成一个secret，如下：\n# kubectl describe secret argocd-secret -n argocd\rName: argocd-secret\rNamespace: argocd\rLabels: app.kubernetes.io/name=argocd-secret\rapp.kubernetes.io/part-of=argocd\rAnnotations: Type: Opaque\rData\r====\radmin.passwordMtime: 20 bytes\rserver.secretkey: 44 bytes\rtls.crt: 1237 bytes\rtls.key: 1679 bytes\rwebhook.gitlab.secret: 7 bytes\radmin.password: 60 bytes\r（2）在gitlab的代码仓库配置webhook，如下：\n由于集群内部证书是无效证书，所有要把Enabled SSL去掉，如下：\n然后点击保存，点击测试，看是否链接成功。如果有如下提示则表示webhook配置没问题了。\n现在可以进行修改gitlab仓库，观察是否一提交，argocd那边就可以响应了。\n使用argo rollouts进行金丝雀发布 关于argo rollouts的更多介绍可以查看之前的文章《使用argo-rollouts实现金丝雀发布》。\n按着官方文档进行安装，官方地址为：https://argoproj.github.io/argo-rollouts/installation/#kubectl-plugin-installation\n（1）在Kubernetes集群中安装argo-rollouts\nkubectl create namespace argo-rollouts\rkubectl apply -n argo-rollouts -f https://raw.githubusercontent.com/argoproj/argo-rollouts/stable/manifests/install.yaml\r（2）安装argo-rollouts的kubectl plugin\ncurl -LO https://github.com/argoproj/argo-rollouts/releases/latest/download/kubectl-argo-rollouts-linux-amd64\rchmod +x ./kubectl-argo-rollouts-linux-amd64\rmv ./kubectl-argo-rollouts-linux-amd64 /usr/local/bin/kubectl-argo-rollouts\r（3）我们这里主要是要重写deployment的配置文件，主要的配置清单如下。\nrollout.yaml\napiVersion: argoproj.io/v1alpha1\rkind: Rollout\rmetadata:\rname: rollouts-simple-java\rspec:\rreplicas: 3\rstrategy:\rcanary:\rcanaryService: rollouts-simple-java-canary\rstableService: rollouts-simple-java-stable\rtrafficRouting:\rnginx:\rstableIngress: rollouts-simple-java-stable\rsteps:\r- setWeight: 20\r- pause: {duration: 60}\r- setWeight: 50\r- pause: {duration: 10}\r- setWeight: 80\r- pause: {duration: 10}\rrevisionHistoryLimit: 2\rselector:\rmatchLabels:\rapp: rollouts-simple-java\rtemplate:\rmetadata:\rlabels:\rapp: rollouts-simple-java\rspec:\rcontainers:\r- args:\r- -jar\r- /opt/myapp.jar\r- --server.port=8080\rcommand:\r- java\renv:\r- name: HOST_IP\rvalueFrom:\rfieldRef:\rapiVersion: v1\rfieldPath: status.hostIP\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/myapp:latest\rimagePullPolicy: IfNotPresent\rlifecycle:\rpreStop:\rexec:\rcommand:\r- /bin/sh\r- -c\r- /bin/sleep 30\rlivenessProbe:\rfailureThreshold: 3\rhttpGet:\rpath: /hello\rport: 8080\rscheme: HTTP\rinitialDelaySeconds: 60\rperiodSeconds: 15\rsuccessThreshold: 1\rtimeoutSeconds: 1\rname: myapp\rports:\r- containerPort: 8080\rname: http\rprotocol: TCP\rreadinessProbe:\rfailureThreshold: 3\rhttpGet:\rpath: /hello\rport: 8080\rscheme: HTTP\rperiodSeconds: 15\rsuccessThreshold: 1\rtimeoutSeconds: 1\rresources:\rlimits:\rcpu: \u0026quot;1\u0026quot;\rmemory: 2Gi\rrequests:\rcpu: 100m\rmemory: 1Gi\rterminationMessagePath: /dev/termination-log\rterminationMessagePolicy: File\rdnsPolicy: ClusterFirstWithHostNet\rimagePullSecrets:\r- name: gitlab-registry\rservices.yaml\napiVersion: v1\rkind: Service\rmetadata:\rname: rollouts-simple-java-canary\rspec:\rports:\r- port: 8080\rtargetPort: http\rprotocol: TCP\rname: http\rselector:\rapp: rollouts-simple-java\r# This selector will be updated with the pod-template-hash of the canary ReplicaSet. e.g.:\r# rollouts-pod-template-hash: 7bf84f9696\r---\rapiVersion: v1\rkind: Service\rmetadata:\rname: rollouts-simple-java-stable\rspec:\rports:\r- port: 8080\rtargetPort: http\rprotocol: TCP\rname: http\rselector:\rapp: rollouts-simple-java\r# This selector will be updated with the pod-template-hash of the stable ReplicaSet. e.g.:\r# rollouts-pod-template-hash: 789746c88d\ringress.yaml\napiVersion: networking.k8s.io/v1beta1\rkind: Ingress\rmetadata:\rname: rollouts-simple-java-stable\rannotations:\rkubernetes.io/ingress.class: nginx\rspec:\rrules:\r- host: rollouts-simple-java.coolops.cn\rhttp:\rpaths:\r- path: /\rbackend:\r# Reference to a Service name, also specified in the Rollout spec.strategy.canary.stableService field\rserviceName: rollouts-simple-java-stable\rservicePort: 8080\rkustomization.yaml\n# Example configuration for the webserver\r# at https://github.com/monopole/hello\rcommonLabels:\rapp: rollouts-simple-java\rresources:\r- rollout.yaml\r- services.yaml\r- ingress.yaml\rapiVersion: kustomize.config.k8s.io/v1beta1\rkind: Kustomization\rimages:\r- name: registry.cn-hangzhou.aliyuncs.com/rookieops/myapp\rnewTag: \u0026quot;latest\u0026quot;\rnamespace: dev\r让后将这些文件保存到gitlab yaml仓库里，比如：\n这里的金丝雀发布是才有的时间暂停的方式，还可以才有手动继续的方式。我这里方便测试就才有了时间暂停。\n给代码仓库打Tag 为啥要给代码仓库打Tag呢？\n当一个代码仓库进过长时间的迭代，针对不同的时期和需求，必定会有不同的版本。而借助 Git 提供的标签功能，可以快捷方便地记录代码版本。无论什么时候，想取回某个版本，不再需要查找冗长的commit_id，只需要取出打标签的历史版本即可。\n可以这么理解：标签是版本库的一个快照。在主流的 Git 平台上，版本可以直接下载的，节省了开发者的不少精力。\n这里通过gitlab的api对代码仓库打tag。API的具体操作见https://docs.gitlab.com/ee/api/tags.html\n这里在shareLibrary的代码仓库中创建了gitlab.groovy文件。\n具体内容如下：\npackage org.devops\r//封装HTTP请求\rdef HttpReq(reqType,reqUrl,reqBody){\rdef gitServer = \u0026quot;http://172.17.100.135:32080/api/v4\u0026quot;\rwithCredentials([string(credentialsId: 'gitlab-token', variable: 'gitlabToken')]) {\rresult = httpRequest customHeaders: [[maskValue: true, name: 'PRIVATE-TOKEN', value: \u0026quot;${gitlabToken}\u0026quot;]], httpMode: reqType, contentType: \u0026quot;APPLICATION_JSON\u0026quot;,\rconsoleLogResponseBody: true,\rignoreSslErrors: true, requestBody: reqBody,\rurl: \u0026quot;${gitServer}/${reqUrl}\u0026quot;\r//quiet: true\r}\rreturn result\r}\r//获取项目ID\rdef GetProjectID(projectName){\rprojectApi = \u0026quot;projects?search=${projectName}\u0026quot;\rresponse = HttpReq('GET',projectApi,'')\rdef result = readJSON text: \u0026quot;\u0026quot;\u0026quot;${response.content}\u0026quot;\u0026quot;\u0026quot;\rfor (repo in result){\r// println(repo['path_with_namespace'])\rif (repo['path'] == \u0026quot;${projectName}\u0026quot;){\rrepoId = repo['id']\rprintln(repoId)\r}\r}\rreturn repoId\r}\r// 给仓库打tag\rdef TagGitlab(projectId,tag_name,tag_ref){\rdef apiUrl = \u0026quot;projects/${projectId}/repository/tags\u0026quot;\rreqBody = \u0026quot;\u0026quot;\u0026quot;{\u0026quot;tag_name\u0026quot;: \u0026quot;${tag_name}\u0026quot;,\u0026quot;ref\u0026quot;: \u0026quot;${tag_ref}\u0026quot;}\u0026quot;\u0026quot;\u0026quot;\rHttpReq('POST',apiUrl,reqBody)\r}\r首先通过GetProjectID获取到项目仓库的ID，然后再调用TagGitlab进行打Tag。\n然后我们需要在Jenkins上创建一个名叫gitlab-token的token凭据。\n（1）在gitlab上生成token\n（2）在Jenkins上创建凭据\n系统管理-\u0026gt;凭据管理-\u0026gt;全局凭据-\u0026gt;添加凭据\n 注意这个ID，要和gitlab.groovy中的ID一一对应。\n 在argocd中配置项目 可以直接在UI上配置项目，我这里采用的是YAML清单的方式，如下：\nrollout-simple-java.yaml\napiVersion: argoproj.io/v1alpha1\rkind: Application\rmetadata:\rname: 'rollout-simple-java'\rnamespace: argocd\rspec:\rdestination:\rnamespace: 'dev'\rserver: 'https://kubernetes.default.svc'\rsource:\rpath: 'rollout-simple-java/'\rrepoURL: 'http://172.17.100.135:32080/root/devops-cd.git'\rtargetRevision: HEAD\rproject: 'default'\rsyncPolicy:\rautomated: {}\r创建之后可以在UI界面看到新建的应用了。\n在Jenkins上配置项目 （1）在shareLibrary上创建如下Jenkinsfile\ndef labels = \u0026quot;slave-${UUID.randomUUID().toString()}\u0026quot;\r// 引用共享库\r@Library(\u0026quot;jenkins_shareLibrary\u0026quot;)\r// 应用共享库中的方法\rdef tools = new org.devops.tools()\rdef sonarapi = new org.devops.sonarAPI()\rdef sendEmail = new org.devops.sendEmail()\rdef build = new org.devops.build()\rdef sonar = new org.devops.sonarqube()\r// 前端传来的变量\rdef gitBranch = env.branch\rdef gitUrl = env.git_url\rdef buildShell = env.build_shell\rdef image = env.image\rdef dockerRegistryUrl = env.dockerRegistryUrl\rdef devops_cd_git = env.devops_cd_git\rdef repo_name = env.repo_name\rdef gitlab = new org.devops.gitlab()\rdef deploy = new org.devops.deploy()\r// 固定变量\r// def SonarServer = \u0026quot;http://sonar.devops.svc.cluster.local:9000/api\u0026quot;\r// def dockerRegistryUrl = \u0026quot;registry.cn-hangzhou.aliyuncs.com\u0026quot;\rdef isUpdate = ''\rpipeline {\ragent {\rkubernetes {\rlabel labels\ryaml \u0026quot;\u0026quot;\u0026quot;\rapiVersion: v1\rkind: Pod\rmetadata:\rlabels:\rsome-label: some-label-value\rspec:\rvolumes:\r- name: docker-sock\rhostPath:\rpath: /var/run/docker.sock\rtype: ''\r- name: maven-cache\rpersistentVolumeClaim:\rclaimName: maven-cache-pvc\rcontainers:\r- name: jnlp\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/inbound-agent:4.3-4\r- name: maven\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/maven:3.5.0-alpine\rcommand:\r- cat\rtty: true\rvolumeMounts:\r- name: maven-cache\rmountPath: /root/.m2\r- name: docker\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/docker:19.03.11\rcommand:\r- cat\rtty: true\rvolumeMounts:\r- name: docker-sock\rmountPath: /var/run/docker.sock\r- name: sonar-scanner\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/sonar-scanner:latest\rcommand:\r- cat\rtty: true\r- name: kustomize\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/kustomize:v3.8.1\rcommand:\r- cat\rtty: true\r\u0026quot;\u0026quot;\u0026quot;\r}\r}\renvironment{\rauth = 'joker'\r}\roptions {\rtimestamps() // 日志会有时间\rskipDefaultCheckout() // 删除隐式checkout scm语句\rdisableConcurrentBuilds() //禁止并行\rtimeout(time:1,unit:'HOURS') //设置流水线超时时间\r}\rstages {\r// 拉取代码\rstage('GetCode') {\rsteps {\rcheckout([$class: 'GitSCM', branches: [[name: \u0026quot;${gitBranch}\u0026quot;]],\rdoGenerateSubmoduleConfigurations: false,\rextensions: [],\rsubmoduleCfg: [],\ruserRemoteConfigs: [[credentialsId: '83d2e934-75c9-48fe-9703-b48e2feff4d8', url: \u0026quot;${gitUrl}\u0026quot;]]])\r}\r}\r// 单元测试和编译打包\rstage('Build\u0026amp;Test') {\rsteps {\rcontainer('maven') {\rscript{\rtools.PrintMes(\u0026quot;编译打包\u0026quot;,\u0026quot;blue\u0026quot;)\rbuild.DockerBuild(\u0026quot;${buildShell}\u0026quot;)\r}\r}\r}\r}\r// 代码扫描\rstage('CodeScanner') {\rsteps {\rcontainer('sonar-scanner') {\rscript {\rtools.PrintMes(\u0026quot;代码扫描\u0026quot;,\u0026quot;green\u0026quot;)\rtools.PrintMes(\u0026quot;搜索项目\u0026quot;,\u0026quot;green\u0026quot;)\rresult = sonarapi.SearchProject(\u0026quot;${JOB_NAME}\u0026quot;)\rprintln(result)\rif (result == \u0026quot;false\u0026quot;){\rprintln(\u0026quot;${JOB_NAME}---项目不存在,准备创建项目---\u0026gt; ${JOB_NAME}！\u0026quot;)\rsonarapi.CreateProject(\u0026quot;${JOB_NAME}\u0026quot;)\r} else {\rprintln(\u0026quot;${JOB_NAME}---项目已存在！\u0026quot;)\r}\rtools.PrintMes(\u0026quot;代码扫描\u0026quot;,\u0026quot;green\u0026quot;)\rsonar.SonarScan(\u0026quot;${JOB_NAME}\u0026quot;,\u0026quot;${JOB_NAME}\u0026quot;,\u0026quot;src\u0026quot;)\rsleep 10\rtools.PrintMes(\u0026quot;获取扫描结果\u0026quot;,\u0026quot;green\u0026quot;)\rresult = sonarapi.GetProjectStatus(\u0026quot;${JOB_NAME}\u0026quot;)\rprintln(result)\rif (result.toString() == \u0026quot;ERROR\u0026quot;){\rtoemail.Email(\u0026quot;代码质量阈错误！请及时修复！\u0026quot;,userEmail)\rerror \u0026quot; 代码质量阈错误！请及时修复！\u0026quot;\r} else {\rprintln(result)\r}\r}\r}\r}\r}\r// 构建镜像\rstage('BuildImage') {\rsteps {\rwithCredentials([[$class: 'UsernamePasswordMultiBinding',\rcredentialsId: 'dockerhub',\rusernameVariable: 'DOCKER_HUB_USER',\rpasswordVariable: 'DOCKER_HUB_PASSWORD']]) {\rcontainer('docker') {\rscript{\rtools.PrintMes(\u0026quot;构建镜像\u0026quot;,\u0026quot;green\u0026quot;)\rimageTag = tools.createVersion()\rsh \u0026quot;\u0026quot;\u0026quot;\rdocker login ${dockerRegistryUrl} -u ${DOCKER_HUB_USER} -p ${DOCKER_HUB_PASSWORD}\rdocker build -t ${image}:${imageTag} .\rdocker push ${image}:${imageTag}\rdocker rmi ${image}:${imageTag}\r\u0026quot;\u0026quot;\u0026quot;\r}\r}\r}\r}\r}\r// 部署\rstage('Deploy') {\rsteps {\rwithCredentials([[$class: 'UsernamePasswordMultiBinding',\rcredentialsId: 'ci-devops',\rusernameVariable: 'DEVOPS_USER',\rpasswordVariable: 'DEVOPS_PASSWORD']]){\rcontainer('kustomize') {\rscript{\rAPP_DIR=\u0026quot;${JOB_NAME}\u0026quot;.split(\u0026quot;_\u0026quot;)[0]\rsh \u0026quot;\u0026quot;\u0026quot;\rgit remote set-url origin http://${DEVOPS_USER}:${DEVOPS_PASSWORD}@${devops_cd_git}\rgit config --global user.name \u0026quot;Administrator\u0026quot;\rgit config --global user.email \u0026quot;coolops@163.com\u0026quot;\rgit clone http://${DEVOPS_USER}:${DEVOPS_PASSWORD}@${devops_cd_git} /opt/devops-cd\rcd /opt/devops-cd\rgit pull\rcd /opt/devops-cd/${APP_DIR}\rkustomize edit set image ${image}:${imageTag}\rgit commit -am 'image update'\rgit push origin master\r\u0026quot;\u0026quot;\u0026quot;\r}\r}\r}\r}\r}\r// 接口测试\rstage('InterfaceTest') {\rsteps{\rsh 'echo \u0026quot;接口测试\u0026quot;'\r}\r}\r// 继续更新或回滚\rstage('UpdateOrRollBack') {\rinput {\rmessage 'Should we continue?'\rok 'Yes, we should.'\rsubmitter 'alice,bob'\rparameters {\rstring(name: 'input', defaultValue: 'yes', description: 'continue update?')\r}\r}\rsteps {\rscript {\r// 调用更新或者回滚函数\rtools.PrintMes(\u0026quot;更新或者回滚\u0026quot;,\u0026quot;green\u0026quot;)\r// 将input的值赋值给全局变量isUpdate，供下阶段使用\risUpdate = \u0026quot;${input}\u0026quot;\r}\r}\r}\r// 如果是继续更新服务，待验证通过后给gitlab代码仓库打tag\rstage('TagGitlab') {\rsteps {\rscript {\rif (\u0026quot;${isUpdate}\u0026quot; == 'yes' \u0026amp;\u0026amp; \u0026quot;${gitBranch }\u0026quot; == 'master') {\rtools.PrintMes('给仓库打TAG', 'green')\r// 获取项目的projectId\rrepo_id = gitlab.GetProjectID(\u0026quot;${repo_name}\u0026quot;)\rsh \u0026quot;echo ${repo_id}\u0026quot;\r// 生产tag,以当前时间为tag\rtag_name = \u0026quot;release\u0026quot;+\u0026quot;-\u0026quot;+tools.getTime()\rgitlab.TagGitlab(\u0026quot;${repo_id}\u0026quot;, \u0026quot;${tag_name}\u0026quot;, 'master')\r}else {\rtools.PrintMes('不打TAG', 'red')\r}\r}\r}\r}\r}\r// 构建后的操作\rpost {\rsuccess {\rscript{\rprintln(\u0026quot;success:只有构建成功才会执行\u0026quot;)\rcurrentBuild.description += \u0026quot;\\n构建成功！\u0026quot;\r// deploy.AnsibleDeploy(\u0026quot;${deployHosts}\u0026quot;,\u0026quot;-m ping\u0026quot;)\rsendEmail.SendEmail(\u0026quot;构建成功\u0026quot;,toEmailUser)\r// dingmes.SendDingTalk(\u0026quot;构建成功 ✅\u0026quot;)\r}\r}\rfailure {\rscript{\rprintln(\u0026quot;failure:只有构建失败才会执行\u0026quot;)\rcurrentBuild.description += \u0026quot;\\n构建失败!\u0026quot;\rsendEmail.SendEmail(\u0026quot;构建失败\u0026quot;,toEmailUser)\r// dingmes.SendDingTalk(\u0026quot;构建失败 ❌\u0026quot;)\r}\r}\raborted {\rscript{\rprintln(\u0026quot;aborted:只有取消构建才会执行\u0026quot;)\rcurrentBuild.description += \u0026quot;\\n构建取消!\u0026quot;\rsendEmail.SendEmail(\u0026quot;取消构建\u0026quot;,toEmailUser)\r// dingmes.SendDingTalk(\u0026quot;构建失败 ❌\u0026quot;,\u0026quot;暂停或中断\u0026quot;)\r}\r}\r}\r}\rJenkinsfile和之前的大同小异，只是增加了两个stage。\n其中UpdateOrRollBack这个stage只是占了一个坑，并没有具体实现，其思路是：\n  在部署新版本的时候第一次暂停，然后通过Jenkins这里的输入决定是否继续\n   如果继续则表示该版本上线没什么问题，继续后面的TagGitlab 如果不继续则表示该版本上线有问题，取消本次上线，并将应用回滚至上一版本    （2）、在Jenkins上配置项目\n 注意项目名字的前缀和YAML清单所在的文件夹名一致\n 然后添加几个参数。\n配置流水线\n发布应用 （1）打开一个终端，输入以下命令，让其一直curl页面\nwhile true;do curl http://rollouts-simple-java.coolops.cn:30122/hello;sleep 2;echo \u0026quot;\\n\u0026quot;;done\r输出如下：\n（2）修改源代码，进行发布，我将源码中的Hello world改成hello joker，如下\n然后提交到代码库。\n（3）、在Jenkins上进行build\n然后可以在终端上看到少量的流量访问到了hello joker，如下\n（4）、点击继续部署\n上面能正常访问到hello joker，表示测试通过，在Jenkins流水线上点击继续部署，对当前代码仓库进行打tag\n待其执行完后，在gitlab的代码仓库中可以看到新的tag，如下\n点击进去可以看到更改的内容。\n后面金丝雀发布完成后，可以看到终端输出如下：\n到此整个过程完成。\n写在最后 argo全家桶还是非常不错，目前我使用了argocd和argo rollouts，初步使用来看运行都比较稳定，不过argocd有几个需要注意的点：\n 建议对创建在argocd上的每个应用的yaml文件进行备份，因为argocd本身是无状态的，保不齐你啥时候就将其清空了。 argocd-cm这个configmap每次修改过后就会清空部署在上面的应用，不过对我应用本身不受影响，这也是为什么要备份的原因，方便重建 argo rollouts对ingress的支持有限，目前只支持ingress和alb  ","description":"本文主要介绍使用Jenkins配合argocd以及argo rollouts实现CI/CD","id":16,"section":"posts","tags":["kubernetes","devops","CI/CD","argocd","argo-rollouts"],"title":"基于Jenkins+Argocd+Argo Rollouts的CI/CD实现并用金丝雀发布","uri":"https://www.coolops.cn/posts/jenkins-argocd-argo-rollouts/"},{"content":"什么是argo rollouts Argo-Rollout是一个Kubernetes Controller和对应一系列的CRD，提供更强大的Deployment能力。包括灰度发布、蓝绿部署、更新测试(experimentation)、渐进式交付(progressive delivery)等特性。\n支持特性如下：\n 蓝绿色更新策略 金丝雀更新策略 细粒度，加权流量转移 自动回rollback和promotion 手动判断 可定制的指标查询和业务KPI分析 入口控制器集成：NGINX，ALB 服务网格集成：Istio，Linkerd，SMI Metric provider集成：Prometheus，Wavefront，Kayenta，Web，Kubernetes Jobs  Argo原理和Deployment差不多，只是加强rollout的策略和流量控制。当spec.template发送变化时，Argo-Rollout就会根据spec.strategy进行rollout，通常会产生一个新的ReplicaSet，逐步scale down之前的ReplicaSet的pod数量。\n安装 按着官方文档进行安装，官方地址为：https://argoproj.github.io/argo-rollouts/installation/#kubectl-plugin-installation\n（1）在Kubernetes集群中安装argo-rollouts\nkubectl create namespace argo-rollouts\rkubectl apply -n argo-rollouts -f https://raw.githubusercontent.com/argoproj/argo-rollouts/stable/manifests/install.yaml\r（2）安装argo-rollouts的kubectl plugin\ncurl -LO https://github.com/argoproj/argo-rollouts/releases/latest/download/kubectl-argo-rollouts-linux-amd64\rchmod +x ./kubectl-argo-rollouts-linux-amd64\rmv ./kubectl-argo-rollouts-linux-amd64 /usr/local/bin/kubectl-argo-rollouts\r金丝雀发布 灰度发布包含Replica Shifting和Traffic Shifting两个过程。\n Replica Shifting：版本替换 Traffic Shifting：流量接入  这里使用官方的demo来进行测试。例子：https://argoproj.github.io/argo-rollouts/getting-started/\nReplica Shifting 部署应用 使用如下命令部署示例：\nkubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/rollout.yaml\rkubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/service.yaml\r我们先看看第一个rollout.yaml的具体内容，如下：\napiVersion: argoproj.io/v1alpha1\rkind: Rollout\rmetadata:\rname: rollouts-demo\rspec:\rreplicas: 5\rstrategy:\rcanary:\rsteps:\r- setWeight: 20\r- pause: {}\r- setWeight: 40\r- pause: {duration: 10}\r- setWeight: 60\r- pause: {duration: 10}\r- setWeight: 80\r- pause: {duration: 10}\rrevisionHistoryLimit: 2\rselector:\rmatchLabels:\rapp: rollouts-demo\rtemplate:\rmetadata:\rlabels:\rapp: rollouts-demo\rspec:\rcontainers:\r- name: rollouts-demo\rimage: argoproj/rollouts-demo:blue\rports:\r- name: http\rcontainerPort: 8080\rprotocol: TCP\rresources:\rrequests:\rmemory: 32Mi\rcpu: 5m\r可以看到除了apiVersion，kind以及strategy之外，其他和Deployment无异。\nstrategy字段定义的是发布策略，其中：\n setWeight：设置流量的权重 pause：暂停，如果里面没有跟duration: 10则表示需要手动更新，如果跟了表示等待多长时间会自动跟新。  而service.yaml文件定义的就是普通的service，如下：\napiVersion: v1\rkind: Service\rmetadata:\rname: rollouts-demo\rspec:\rports:\r- port: 80\rtargetPort: http\rprotocol: TCP\rname: http\rselector:\rapp: rollouts-demo\r执行上面命令部署后，会在default命名空间下创建5个pod，如下：\n# kubectl get pod NAME READY STATUS RESTARTS AGE\rnfs-client-prosioner-598d477ff6-fmgwf 1/1 Running 2 17d\rrollouts-demo-7bf84f9696-4glv6 1/1 Running 0 78s\rrollouts-demo-7bf84f9696-7kqt6 1/1 Running 0 78s\rrollouts-demo-7bf84f9696-8k9hw 1/1 Running 0 78s\rrollouts-demo-7bf84f9696-9cz2r 1/1 Running 0 78s\rrollouts-demo-7bf84f9696-jvzvd 1/1 Running 0 78s\r可以使用kubectl-argo-rollouts get rollout rollouts-demo命令来查看部署状态，如下：\n# kubectl-argo-rollouts get rollout rollouts-demo\rName: rollouts-demo\rNamespace: default\rStatus: ✔ Healthy\rStrategy: Canary\rStep: 8/8\rSetWeight: 100\rActualWeight: 100\rImages: argoproj/rollouts-demo:blue (stable)\rReplicas:\rDesired: 5\rCurrent: 5\rUpdated: 5\rReady: 5\rAvailable: 5\rNAME KIND STATUS AGE INFO\r⟳ rollouts-demo Rollout ✔ Healthy 114s └──# revision:1 └──⧉ rollouts-demo-7bf84f9696 ReplicaSet ✔ Healthy 114s stable\r├──□ rollouts-demo-7bf84f9696-4glv6 Pod ✔ Running 114s ready:1/1\r├──□ rollouts-demo-7bf84f9696-7kqt6 Pod ✔ Running 114s ready:1/1\r├──□ rollouts-demo-7bf84f9696-8k9hw Pod ✔ Running 114s ready:1/1\r├──□ rollouts-demo-7bf84f9696-9cz2r Pod ✔ Running 114s ready:1/1\r└──□ rollouts-demo-7bf84f9696-jvzvd Pod ✔ Running 114s ready:1/1\r可以看到该版本被标记为stable，而且STATUS为healthy。还可以在命令后面加一个--watch来实时监控服务状态，完整命令为kubectl argo rollouts get rollout rollouts-demo --watch。\n更新应用 接下来对应用进行更新。对应用进行更新和更新用Deployment部署的应用一样，更新镜像即可。argo rollouts插件有一个set image命令来更新镜像，如下：\nkubectl argo rollouts set image rollouts-demo \\\rrollouts-demo=argoproj/rollouts-demo:yellow\r更新过后，我们可以通过观察kubectl argo rollouts get rollout rollouts-demo --watch服务状态，如下：\nName: rollouts-demo\rNamespace: default\rStatus: ॥ Paused\rMessage: CanaryPauseStep\rStrategy: Canary\rStep: 1/8\rSetWeight: 20\rActualWeight: 20\rImages: argoproj/rollouts-demo:blue (stable)\rargoproj/rollouts-demo:yellow (canary)\rReplicas:\rDesired: 5\rCurrent: 5\rUpdated: 1\rReady: 5\rAvailable: 5\rNAME KIND STATUS AGE INFO\r⟳ rollouts-demo Rollout ॥ Paused 9m12s ├──# revision:2 │ └──⧉ rollouts-demo-789746c88d ReplicaSet ✔ Healthy 44s canary\r│ └──□ rollouts-demo-789746c88d-l4gmd Pod ✔ Running 44s ready:1/1\r└──# revision:1 └──⧉ rollouts-demo-7bf84f9696 ReplicaSet ✔ Healthy 9m12s stable\r├──□ rollouts-demo-7bf84f9696-4glv6 Pod ✔ Running 9m12s ready:1/1\r├──□ rollouts-demo-7bf84f9696-8k9hw Pod ✔ Running 9m12s ready:1/1\r├──□ rollouts-demo-7bf84f9696-9cz2r Pod ✔ Running 9m12s ready:1/1\r└──□ rollouts-demo-7bf84f9696-jvzvd Pod ✔ Running 9m12s ready:1/1\r可以看到多了一个revision:2，而且该版本被标记为canary，而且状态是Status: Paused，canary接入流量为20%。\n部署之所以处于Paused阶段，是因为我们在rollout.yaml中定义了发布第一个版本后会暂停，这时候需要手动接入接下来的更新。\nargo rollouts提供了promote 来进行后续的更新，命令如下：\nkubectl argo rollouts promote rollouts-demo\r然后我们可以在watch界面，看到如下的更新过程。\nName: rollouts-demo\rNamespace: default\rStatus: ॥ Paused\rMessage: CanaryPauseStep\rStrategy: Canary\rStep: 3/8\rSetWeight: 40\rActualWeight: 40\rImages: argoproj/rollouts-demo:blue (stable)\rargoproj/rollouts-demo:yellow (canary)\rReplicas:\rDesired: 5\rCurrent: 5\rUpdated: 2\rReady: 5\rAvailable: 5\rNAME KIND STATUS AGE INFO\r⟳ rollouts-demo Rollout ॥ Paused 15m ├──# revision:2 │ └──⧉ rollouts-demo-789746c88d ReplicaSet ✔ Healthy 6m46s canary\r│ ├──□ rollouts-demo-789746c88d-l4gmd Pod ✔ Running 6m46s ready:1/1\r│ └──□ rollouts-demo-789746c88d-67dwp Pod ✔ Running 19s ready:1/1\r└──# revision:1 └──⧉ rollouts-demo-7bf84f9696 ReplicaSet ✔ Healthy 15m stable\r├──□ rollouts-demo-7bf84f9696-4glv6 Pod ✔ Running 15m ready:1/1\r├──□ rollouts-demo-7bf84f9696-8k9hw Pod ✔ Running 15m ready:1/1\r├──□ rollouts-demo-7bf84f9696-9cz2r Pod ✔ Running 15m ready:1/1\r└──□ rollouts-demo-7bf84f9696-jvzvd Pod ◌ Terminating 15m ready:1/1\r因为后续的更新在pause阶段只暂停10s，所以会依次自动更新完，不需要手动介入，待更新完后整体的状态如下：\nName: rollouts-demo\rNamespace: default\rStatus: ✔ Healthy\rStrategy: Canary\rStep: 8/8\rSetWeight: 100\rActualWeight: 100\rImages: argoproj/rollouts-demo:yellow (stable)\rReplicas:\rDesired: 5\rCurrent: 5\rUpdated: 5\rReady: 5\rAvailable: 5\rNAME KIND STATUS AGE INFO\r⟳ rollouts-demo Rollout ✔ Healthy 17m ├──# revision:2 │ └──⧉ rollouts-demo-789746c88d ReplicaSet ✔ Healthy 8m35s stable\r│ ├──□ rollouts-demo-789746c88d-l4gmd Pod ✔ Running 8m35s ready:1/1\r│ ├──□ rollouts-demo-789746c88d-67dwp Pod ✔ Running 2m8s ready:1/1\r│ ├──□ rollouts-demo-789746c88d-k7mfk Pod ✔ Running 106s ready:1/1\r│ ├──□ rollouts-demo-789746c88d-glbfb Pod ✔ Running 94s ready:1/1\r│ └──□ rollouts-demo-789746c88d-d7m4f Pod ✔ Running 83s ready:1/1\r└──# revision:1 └──⧉ rollouts-demo-7bf84f9696 ReplicaSet • ScaledDown 17m 可以看到第一个版本已经下线，第二个版本的状态为Healthy，而且镜像被标记为stable。\n终止更新 如果在更新应用的过程中，最新的应用有问题，需要终止更新需要怎么做呢？\n我们先使用下面命令发布新版本应用，如下：\nkubectl argo rollouts set image rollouts-demo \\\rrollouts-demo=argoproj/rollouts-demo:red\r然后更新动作会在第一次更新的时候处于Paused状态，现在我们可以用abort来终止发布，如下：\nkubectl argo rollouts abort rollouts-demo\r待执行完命令后，可以在watch页面，看到如下信息：\nName: rollouts-demo\rNamespace: default\rStatus: ✖ Degraded\rMessage: RolloutAborted: Rollout is aborted\rStrategy: Canary\rStep: 0/8\rSetWeight: 0\rActualWeight: 0\rImages: argoproj/rollouts-demo:yellow (stable)\rReplicas:\rDesired: 5\rCurrent: 5\rUpdated: 0\rReady: 5\rAvailable: 5\rNAME KIND STATUS AGE INFO\r⟳ rollouts-demo Rollout ✖ Degraded 21m ├──# revision:3 │ └──⧉ rollouts-demo-6f75f48b7 ReplicaSet • ScaledDown 90s canary\r├──# revision:2 │ └──⧉ rollouts-demo-789746c88d ReplicaSet ✔ Healthy 13m stable\r│ ├──□ rollouts-demo-789746c88d-l4gmd Pod ✔ Running 13m ready:1/1\r│ ├──□ rollouts-demo-789746c88d-67dwp Pod ✔ Running 6m46s ready:1/1\r│ ├──□ rollouts-demo-789746c88d-k7mfk Pod ✔ Running 6m24s ready:1/1\r│ ├──□ rollouts-demo-789746c88d-glbfb Pod ✔ Running 6m12s ready:1/1\r│ └──□ rollouts-demo-789746c88d-nntc9 Pod ✔ Running 18s ready:1/1\r└──# revision:1 └──⧉ rollouts-demo-7bf84f9696 ReplicaSet • ScaledDown 21m 最终应用会回退到稳定版本。\n但是我们可以看到Status是Degraded状态而并非Healthy状态，我们有必须要将其变成Healthy状态。最简单的办法就是执行如下命令重新发布一下版本：\nkubectl argo rollouts set image rollouts-demo \\\rrollouts-demo=argoproj/rollouts-demo:yellow\r执行过后，可以看到其状态立即变成Healthy，并且没有创建新的副本、新的版本，如下：\nName: rollouts-demo\rNamespace: default\rStatus: ✔ Healthy\rStrategy: Canary\rStep: 8/8\rSetWeight: 100\rActualWeight: 100\rImages: argoproj/rollouts-demo:yellow (stable)\rReplicas:\rDesired: 5\rCurrent: 5\rUpdated: 5\rReady: 5\rAvailable: 5\rNAME KIND STATUS AGE INFO\r⟳ rollouts-demo Rollout ✔ Healthy 40m ├──# revision:4 │ └──⧉ rollouts-demo-789746c88d ReplicaSet ✔ Healthy 32m stable\r│ ├──□ rollouts-demo-789746c88d-l4gmd Pod ✔ Running 32m ready:1/1\r│ ├──□ rollouts-demo-789746c88d-67dwp Pod ✔ Running 26m ready:1/1\r│ ├──□ rollouts-demo-789746c88d-k7mfk Pod ✔ Running 25m ready:1/1\r│ ├──□ rollouts-demo-789746c88d-glbfb Pod ✔ Running 25m ready:1/1\r│ └──□ rollouts-demo-789746c88d-nntc9 Pod ✔ Running 19m ready:1/1\r├──# revision:3 │ └──⧉ rollouts-demo-6f75f48b7 ReplicaSet • ScaledDown 20m └──# revision:1 └──⧉ rollouts-demo-7bf84f9696 ReplicaSet • ScaledDown 40m 回退应用 有时候在应用上线过后，有些BUG并没有发现，这时候要回退怎么办呢？argo rollouts有一个undo命令，可以进行回退。\n比如我们要将版本回退到第一个版本，则执行一下命令：\nkubectl-argo-rollouts undo rollouts-demo --to-revision=1\r然后通过watch界面可以看到如下信息：\nName: rollouts-demo\rNamespace: default\rStatus: ॥ Paused\rMessage: CanaryPauseStep\rStrategy: Canary\rStep: 1/8\rSetWeight: 20\rActualWeight: 20\rImages: argoproj/rollouts-demo:blue (canary)\rargoproj/rollouts-demo:yellow (stable)\rReplicas:\rDesired: 5\rCurrent: 5\rUpdated: 1\rReady: 5\rAvailable: 5\rNAME KIND STATUS AGE INFO\r⟳ rollouts-demo Rollout ॥ Paused 45m ├──# revision:5 │ └──⧉ rollouts-demo-7bf84f9696 ReplicaSet ✔ Healthy 45m canary\r│ └──□ rollouts-demo-7bf84f9696-bn2lz Pod ✔ Running 36s ready:1/1\r├──# revision:4 │ └──⧉ rollouts-demo-789746c88d ReplicaSet ✔ Healthy 36m stable\r│ ├──□ rollouts-demo-789746c88d-l4gmd Pod ✔ Running 36m ready:1/1\r│ ├──□ rollouts-demo-789746c88d-67dwp Pod ✔ Running 30m ready:1/1\r│ ├──□ rollouts-demo-789746c88d-k7mfk Pod ✔ Running 29m ready:1/1\r│ └──□ rollouts-demo-789746c88d-glbfb Pod ✔ Running 29m ready:1/1\r└──# revision:3 └──⧉ rollouts-demo-6f75f48b7 ReplicaSet • ScaledDown 25m 首先revision为1的版本标记没有，重新创建了一个为5的标记，而且第一步处于暂停状态，然后我们执行promote命令继续后续的更新，如下：\nkubectl argo rollouts promote rollouts-demo\r然后我们可以看到如下信息：\nName: rollouts-demo\rNamespace: default\rStatus: ✔ Healthy\rStrategy: Canary\rStep: 8/8\rSetWeight: 100\rActualWeight: 100\rImages: argoproj/rollouts-demo:blue (stable)\rReplicas:\rDesired: 5\rCurrent: 5\rUpdated: 5\rReady: 5\rAvailable: 5\rNAME KIND STATUS AGE INFO\r⟳ rollouts-demo Rollout ✔ Healthy 48m ├──# revision:5 │ └──⧉ rollouts-demo-7bf84f9696 ReplicaSet ✔ Healthy 48m stable\r│ ├──□ rollouts-demo-7bf84f9696-bn2lz Pod ✔ Running 3m21s ready:1/1\r│ ├──□ rollouts-demo-7bf84f9696-xn6dr Pod ✔ Running 56s ready:1/1\r│ ├──□ rollouts-demo-7bf84f9696-w58vm Pod ✔ Running 44s ready:1/1\r│ ├──□ rollouts-demo-7bf84f9696-fns8d Pod ✔ Running 33s ready:1/1\r│ └──□ rollouts-demo-7bf84f9696-qt6f9 Pod ✔ Running 22s ready:1/1\r├──# revision:4 │ └──⧉ rollouts-demo-789746c88d ReplicaSet • ScaledDown 39m └──# revision:3 └──⧉ rollouts-demo-6f75f48b7 ReplicaSet • ScaledDown 27m 从Images可以看到回退到我们最初版本为blue的镜像了。\nTraffic Shifting 上面我们并没有接入外部流量，仅仅是在内部使用展示了金丝雀部署过程，下面我们接入外部流量进行测试。\nArgo-Rollout主要集成了Ingress和ServiceMesh两种流量控制方法。\n目前Ingress支持ALB和NGINX ingress。但是我使用的是nginx ingress。\n部署应用 我们依然使用官方的例子进行展示。\n首先删除上面的例子。\nkubectl delete -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/rollout.yaml\rkubectl delete -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/service.yaml\r然后重新部署一个官方的例子，如下：\nkubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/rollout.yaml\rkubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/services.yaml\rkubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/ingress.yaml\r这个例子包含1个rollout，2个service，1个ingress。\n它们的配置文件分别如下。\nrollout.yaml，为了便与测试，我将权重改为了50\napiVersion: argoproj.io/v1alpha1\rkind: Rollout\rmetadata:\rname: rollouts-demo\rspec:\rreplicas: 1\rstrategy:\rcanary:\rcanaryService: rollouts-demo-canary\rstableService: rollouts-demo-stable\rtrafficRouting:\rnginx:\rstableIngress: rollouts-demo-stable\rsteps:\r- setWeight: 50\r- pause: {}\rrevisionHistoryLimit: 2\rselector:\rmatchLabels:\rapp: rollouts-demo\rtemplate:\rmetadata:\rlabels:\rapp: rollouts-demo\rspec:\rcontainers:\r- name: rollouts-demo\rimage: argoproj/rollouts-demo:blue\rports:\r- name: http\rcontainerPort: 8080\rprotocol: TCP\rresources:\rrequests:\rmemory: 32Mi\rcpu: 5m\rservices.yaml\napiVersion: v1\rkind: Service\rmetadata:\rname: rollouts-demo-canary\rspec:\rports:\r- port: 80\rtargetPort: http\rprotocol: TCP\rname: http\rselector:\rapp: rollouts-demo\r# This selector will be updated with the pod-template-hash of the canary ReplicaSet. e.g.:\r# rollouts-pod-template-hash: 7bf84f9696\r---\rapiVersion: v1\rkind: Service\rmetadata:\rname: rollouts-demo-stable\rspec:\rports:\r- port: 80\rtargetPort: http\rprotocol: TCP\rname: http\rselector:\rapp: rollouts-demo\r# This selector will be updated with the pod-template-hash of the stable ReplicaSet. e.g.:\r# rollouts-pod-template-hash: 789746c88d\ringress.yaml\napiVersion: networking.k8s.io/v1beta1\rkind: Ingress\rmetadata:\rname: rollouts-demo-stable\rannotations:\rkubernetes.io/ingress.class: nginx\rspec:\rrules:\r- host: rollouts-demo.local\rhttp:\rpaths:\r- path: /\rbackend:\r# Reference to a Service name, also specified in the Rollout spec.strategy.canary.stableService field\rserviceName: rollouts-demo-stable\rservicePort: 80\r从配置文件可以看出Rollout里分别用canaryService和stableService分别定义了该应用灰度的Service Name(rollouts-demo-canary)和当前版本的Service Name(rollouts-demo-stable)。而且rollouts-demo-canary 和 rollouts-demo-stable的service的内容是一样的。selector中暂时没有填上pod-template-hash，Argo-Rollout Controller会根据实际的ReplicaSet hash来修改该值。\n当我们创建完ingress后，Rollout Controller会根据ingress rollouts-demo-stable内容，自动创建一个ingress用了灰度的流量，名字为--canary，所以这里多了一个ingress rollouts-demo-rollouts-demo-stable-canary，将流量导向Canary Service(rollouts-demo-canary)。如下：\n# kubectl get ingress\rNAME HOSTS ADDRESS PORTS AGE\rrollouts-demo-rollouts-demo-stable-canary rollout-demo.coolops.cn 80 9m25s\rrollouts-demo-stable rollout-demo.coolops.cn 80 4m12s\rrollouts-demo-rollouts-demo-stable-canary的内容如下：\n# kubectl get ingress rollouts-demo-rollouts-demo-stable-canary -o yaml\rapiVersion: extensions/v1beta1\rkind: Ingress\rmetadata:\rannotations:\rkubernetes.io/ingress.class: traefik\rnginx.ingress.kubernetes.io/canary: \u0026quot;true\u0026quot;\rnginx.ingress.kubernetes.io/canary-weight: \u0026quot;0\u0026quot;\rcreationTimestamp: \u0026quot;2020-12-09T02:21:52Z\u0026quot;\rgeneration: 2\rname: rollouts-demo-rollouts-demo-stable-canary\rnamespace: default\rownerReferences:\r- apiVersion: argoproj.io/v1alpha1\rblockOwnerDeletion: true\rcontroller: true\rkind: Rollout\rname: rollouts-demo\ruid: 4e74913b-5c89-4275-8f4c-768f23c63c34\rresourceVersion: \u0026quot;15681411\u0026quot;\rselfLink: /apis/extensions/v1beta1/namespaces/default/ingresses/rollouts-demo-rollouts-demo-stable-canary\ruid: bc66dfc4-6e98-419b-a288-f67e1233ef3e\rspec:\rrules:\r- host: rollout-demo.coolops.cn\rhttp:\rpaths:\r- backend:\rserviceName: rollouts-demo-canary\rservicePort: 80\rpath: /\r通过域名访问，可以看到如下界面。\n更新应用 现在通过以下命令来进行应用更新操作。\nkubectl argo rollouts set image rollouts-demo rollouts-demo=argoproj/rollouts-demo:yellow\r然后通过状态窗口可以看到如下信息。\nName: rollouts-demo\rNamespace: default\rStatus: ॥ Paused\rMessage: CanaryPauseStep\rStrategy: Canary\rStep: 1/2\rSetWeight: 50\rActualWeight: 50\rImages: argoproj/rollouts-demo:blue (stable)\rargoproj/rollouts-demo:yellow (canary)\rReplicas:\rDesired: 1\rCurrent: 2\rUpdated: 1\rReady: 2\rAvailable: 2\rNAME KIND STATUS AGE INFO\r⟳ rollouts-demo Rollout ॥ Paused 2m13s ├──# revision:2 │ └──⧉ rollouts-demo-789746c88d ReplicaSet ✔ Healthy 89s canary\r│ └──□ rollouts-demo-789746c88d-spn4s Pod ✔ Running 89s ready:1/1\r└──# revision:1 └──⧉ rollouts-demo-7bf84f9696 ReplicaSet ✔ Healthy 2m stable\r└──□ rollouts-demo-7bf84f9696-7rwkx Pod ✔ Running 2m ready:1/1\r然后可以看到rollouts-demo-rollouts-demo-stable-canary的ingress的annotations中新增了两个参数，如下：\n# kubectl get ingress rollouts-demo-rollouts-demo-stable-canary -o yaml\rapiVersion: extensions/v1beta1\rkind: Ingress\rmetadata:\rannotations:\rkubernetes.io/ingress.class: nginx\rnginx.ingress.kubernetes.io/canary: \u0026quot;true\u0026quot;\rnginx.ingress.kubernetes.io/canary-weight: \u0026quot;50\u0026quot;\rcreationTimestamp: \u0026quot;2020-12-09T03:01:04Z\u0026quot;\rgeneration: 1\rname: rollouts-demo-rollouts-demo-stable-canary\rnamespace: default\rownerReferences:\r- apiVersion: argoproj.io/v1alpha1\rblockOwnerDeletion: true\rcontroller: true\rkind: Rollout\rname: rollouts-demo\ruid: 4d956f2a-9e15-4453-b918-926c4a75f884\rresourceVersion: \u0026quot;15686969\u0026quot;\rselfLink: /apis/extensions/v1beta1/namespaces/default/ingresses/rollouts-demo-rollouts-demo-stable-canary\ruid: c9242819-d088-4fc4-bd4d-8870360fa96e\rspec:\rrules:\r- host: rollout-demo.coolops.cn\rhttp:\rpaths:\r- backend:\rserviceName: rollouts-demo-canary\rservicePort: 80\rpath: /\r然后通过网页，可以看到如下的输出展示。\n然后可以通过验证结果来判断是否继续还是终止。\n如果继续使用如下命令：\nkubectl argo rollouts promote rollouts-demo\r如果终止使用如下命令：\nkubectl argo rollouts abort rollouts-demo\r写在最后 目前我还在测试阶段，并没有实际接入使用。通过测试来看，Argo-Rollout提供更加强大的Deployment，包含比较适合运维的金丝雀发布和蓝绿发布功能，要使用蓝绿发布，仅需要配置rollout，如下：\napiVersion: argoproj.io/v1alpha1\rkind: Rollout ##部署完rollout后就有了这个kind 资源，这个资源和deployment类似也是管理你的副本集的，所以不能像deployment那样在k8s界面看见，只能通过kubectl命令行\rmetadata:\rname: rollout-bluegreen\rnamespace: rollout-test\rspec:\rtemplate:\rspec:\rterminationGracePeriodSeconds: 30\rcontainers:\r- resources: #{}\rrequests:\rcpu: \u0026quot;1\u0026quot;\rmemory: \u0026quot;2Gi\u0026quot;\rlimits:\rcpu: \u0026quot;2\u0026quot;\rmemory: \u0026quot;2Gi\u0026quot;\rterminationMessagePolicy: File\rimagePullPolicy: Always\rname: rollout-bluegreen\rimage: argoproj/rollouts-demo:green #nginx:1.17.1\rschedulerName: default-scheduler\rsecurityContext: {}\rdnsPolicy: ClusterFirst\rrestartPolicy: Always\rmetadata:\rlabels:\rapp: rollout-bluegreen\rselector:\rmatchLabels:\rapp: rollout-bluegreen\rreplicas: 2\rstrategy:\rblueGreen: ##蓝绿启用配置\ractiveService: rollout-bluegreen-active #生效的服务，需要自己创建建本代码最下面service资源。\rpreviewService: rollout-bluegreen-preview #配置预览服务，同理需要自己创建\rautoPromotionEnabled: true ##是否直接切换，如为true，会在新版本变绿后直接切换到对外服务。\rscaleDownDelayRevisionLimit: 0\rpreviewReplicaCount: 1 #新版本的pod数量，设为一个从而控制资源消耗\rrollingUpdate:\rmaxUnavailable: 25%\rmaxSurge: 25%\rtype: RollingUpdate\rrevisionHistoryLimit: 2\rprogressDeadlineSeconds: 600\r---\rapiVersion: v1\rkind: Service\rmetadata:\rname: rollout-bluegreen-active\rnamespace: rollout-test\rspec:\rsessionAffinity: None\rselector:\rapp: rollout-bluegreen\rports:\r- protocol: TCP\rport: 80\rtargetPort: 8080\rtype: LoadBalancer\r整体使用还是比较丝滑，如果测试通过后续考虑集成进CD中。更多内容可以到https://argoproj.github.io/argo-rollouts/进行学习。\n","description":"本文主要介绍在Kubernetes中使用argo rollouts实现金丝雀发布。","id":17,"section":"posts","tags":["kubernetes","argo-rollouts"],"title":"使用Argo Rollouts实现金丝雀发布","uri":"https://www.coolops.cn/posts/argo-rollouts/"},{"content":"一、动态生成Slave 1.1、简介 之前我们都是在物理机或者虚拟机上部署jenkins，但是这种部署方式会有一些难点，如下：\n 主 Master 发生单点故障时，整个流程都不可用了 每个 Slave 的配置环境不一样，来完成不同语言的编译打包等操作，但是这些差异化的配置导致管理起来非常不方便，维护起来也是比较费劲 资源分配不均衡，有的 Slave 要运行的 job 出现排队等待，而有的 Slave 处于空闲状态 资源有浪费，每台 Slave 可能是物理机或者虚拟机，当 Slave 处于空闲状态时，也不会完全释放掉资源。  正因为上面的这些种种痛点，我们渴望一种更高效更可靠的方式来完成这个 CI/CD 流程，而 Docker 虚拟化容器技术能很好的解决这个痛点，又特别是在 Kubernetes 集群环境下面能够更好来解决上面的问题，下图是基于 Kubernetes 搭建 Jenkins 集群的简单示意图：\n从图上可以看到 Jenkins Master 和 Jenkins Slave 以 Pod 形式运行在 Kubernetes 集群的 Node 上，Master 运行在其中一个节点，并且将其配置数据存储到一个 Volume 上去，Slave 运行在各个节点上，并且它不是一直处于运行状态，它会按照需求动态的创建并自动删除。\n这种方式的工作流程大致为：当 Jenkins Master 接受到 Build 请求时，会根据配置的 Label 动态创建一个运行在 Pod 中的 Jenkins Slave 并注册到 Master 上，当运行完 Job 后，这个 Slave 会被注销并且这个 Pod 也会自动删除，恢复到最初状态。\n这种方式部署给我们带来如下好处：\n 服务高可用，当 Jenkins Master 出现故障时，Kubernetes 会自动创建一个新的 Jenkins Master 容器，并且将 Volume 分配给新创建的容器，保证数据不丢失，从而达到集群服务高可用。 动态伸缩，合理使用资源，每次运行 Job 时，会自动创建一个 Jenkins Slave，Job 完成后，Slave 自动注销并删除容器，资源自动释放，而且 Kubernetes 会根据每个资源的使用情况，动态分配 Slave 到空闲的节点上创建，降低出现因某节点资源利用率高，还排队等待在该节点的情况。 扩展性好，当 Kubernetes 集群的资源严重不足而导致 Job 排队等待时，可以很容易的添加一个 Kubernetes Node 到集群中，从而实现扩展。  1.2、部署 1、创建PV、PVC，为Jenkins提供数据持久化：\n---\rapiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: jenkins-pv\rspec:\rcapacity:\rstorage: 5Gi\raccessModes:\r- ReadWriteMany\rpersistentVolumeReclaimPolicy: Delete\rnfs:\rserver: 172.16.1.128\rpath: /data/k8s/jenkins\r---\rapiVersion: v1\rkind: PersistentVolumeClaim\rmetadata:\rname: jenkins-pvc\rnamespace: devops\rspec:\raccessModes:\r- ReadWriteMany\rresources:\rrequests:\rstorage: 5Gi\r2、创建角色授权\napiVersion: v1\rkind: ServiceAccount\rmetadata:\rname: jenkins-sa\rnamespace: devops\r---\rapiVersion: rbac.authorization.k8s.io/v1beta1\rkind: ClusterRole\rmetadata:\rname: jenkins-cr\rrules:\r- apiGroups: [\u0026quot;extensions\u0026quot;, \u0026quot;apps\u0026quot;]\rresources: [\u0026quot;deployments\u0026quot;]\rverbs: [\u0026quot;create\u0026quot;, \u0026quot;delete\u0026quot;, \u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;patch\u0026quot;, \u0026quot;update\u0026quot;]\r- apiGroups: [\u0026quot;\u0026quot;]\rresources: [\u0026quot;services\u0026quot;]\rverbs: [\u0026quot;create\u0026quot;, \u0026quot;delete\u0026quot;, \u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;patch\u0026quot;, \u0026quot;update\u0026quot;]\r- apiGroups: [\u0026quot;\u0026quot;]\rresources: [\u0026quot;pods\u0026quot;]\rverbs: [\u0026quot;create\u0026quot;,\u0026quot;delete\u0026quot;,\u0026quot;get\u0026quot;,\u0026quot;list\u0026quot;,\u0026quot;patch\u0026quot;,\u0026quot;update\u0026quot;,\u0026quot;watch\u0026quot;]\r- apiGroups: [\u0026quot;\u0026quot;]\rresources: [\u0026quot;pods/exec\u0026quot;]\rverbs: [\u0026quot;create\u0026quot;,\u0026quot;delete\u0026quot;,\u0026quot;get\u0026quot;,\u0026quot;list\u0026quot;,\u0026quot;patch\u0026quot;,\u0026quot;update\u0026quot;,\u0026quot;watch\u0026quot;]\r- apiGroups: [\u0026quot;\u0026quot;]\rresources: [\u0026quot;pods/log\u0026quot;]\rverbs: [\u0026quot;get\u0026quot;,\u0026quot;list\u0026quot;,\u0026quot;watch\u0026quot;]\r- apiGroups: [\u0026quot;\u0026quot;]\rresources: [\u0026quot;secrets\u0026quot;]\rverbs: [\u0026quot;get\u0026quot;]\r---\rapiVersion: rbac.authorization.k8s.io/v1beta1\rkind: ClusterRoleBinding\rmetadata:\rname: jenkins-crd\rroleRef:\rkind: ClusterRole\rname: jenkins-cr\rapiGroup: rbac.authorization.k8s.io\rsubjects:\r- kind: ServiceAccount\rname: jenkins-sa\rnamespace: devops\r1、在Kubernetes中部署Jenkins，新建Deployment，jenkins-deploy.yaml\n---\rapiVersion: apps/v1 kind: Deployment\rmetadata:\rname: jenkins\rnamespace: devops spec:\rselector:\rmatchLabels:\rapp: jenkins\rreplicas: 1\rtemplate:\rmetadata:\rlabels:\rapp: jenkins\rspec:\rterminationGracePeriodSeconds: 10\rserviceAccount: jenkins-sa\rcontainers:\r- name: jenkins\rimage: jenkins/jenkins:lts\rimagePullPolicy: IfNotPresent\renv:\r- name: JAVA_OPTS\rvalue: -XshowSettings:vm -Dhudson.slaves.NodeProvisioner.initialDelay=0 -Dhudson.slaves.NodeProvisioner.MARGIN=50 -Dhudson.slaves.NodeProvisioner.MARGIN0=0.85 -Duser.timezone=Asia/Shanghai\rports:\r- containerPort: 8080\rname: web\rprotocol: TCP\r- containerPort: 50000\rname: agent\rprotocol: TCP\rresources:\rlimits:\rcpu: 1000m\rmemory: 1Gi\rrequests:\rcpu: 500m\rmemory: 512Mi\rlivenessProbe:\rhttpGet:\rpath: /login\rport: 8080\rinitialDelaySeconds: 60\rtimeoutSeconds: 5\rfailureThreshold: 12\rreadinessProbe:\rhttpGet:\rpath: /login\rport: 8080\rinitialDelaySeconds: 60\rtimeoutSeconds: 5\rfailureThreshold: 12\rvolumeMounts:\r- name: jenkinshome\rmountPath: /var/jenkins_home\rsecurityContext:\rfsGroup: 1000\rvolumes:\r- name: jenkinshome\rpersistentVolumeClaim:\rclaimName: jenkins-pvc\r---\rapiVersion: v1\rkind: Service\rmetadata:\rname: jenkins\rnamespace: devops labels:\rapp: jenkins\rspec:\rselector:\rapp: jenkins\rtype: NodePort\rports:\r- name: web\rport: 8080\rtargetPort: web\rnodePort: 30002\r- name: agent\rport: 50000\rtargetPort: agent\r5、创建上面的资源清单\n# kubectl apply -f jenkins-rbac.yaml\r# kubectl apply -f jenkins-pvc.yaml\r# kubectl apply -f jenkins-deploy.yaml\r启动如果报如下错误（因为我们容器里是以jenkins用户启动，而我们NFS服务器上是root启动，所以没有权限）：\n[root@master manifests]# kubectl logs jenkins-688c6cd5fd-lj6zg -n devops touch: cannot touch '/var/jenkins_home/copy_reference_file.log': Permission denied\rCan not write to /var/jenkins_home/copy_reference_file.log. Wrong volume permissions?\r然后给我们NFS服务器上的目录授权即可：\n# chown -R 1000 /data/k8s/jenkins/jenkins\r然后登录网站，因为我们Service是采用NodePort类型，其端口为30002，我们直接在浏览器用这个端口访问：\n密码可以通过如下命令获得：\n# cat /data/k8s/jenkins/secrets/initialAdminPassword 12b503a274354e09a465b4f76664db70\r然后安装插件到安装完成。\n1.3、配置 1、安装插件kubernetes\n2、填写Kubernetes和Jenkins的配置信息\n配置管理-\u0026gt;系统配置-\u0026gt;新增cloud。\n按照图中红色框中填写，其中Kubernetes命名空间填写我们Jenkins所在的命名空间。\n备注：\n如果连接测试失败，很可能是权限问题，我们就需要把ServiceAccount的凭证jenkins-sa添加进来。\n3、配置Pod模板\n另外需要挂载两个主机目录：\n /var/run/docker.sock：该文件是用于 Pod 中的容器能够共享宿主机的 Docker； /root/.kube：这个目录挂载到容器的/root/.kube目录下面这是为了让我们能够在 Pod 的容器中能够使用 kubectl 工具来访问我们的 Kubernetes 集群，方便我们后面在 Slave Pod 部署 Kubernetes 应用；  避免一些权限不足，需要配置ServiceAccount\n1.4、测试 1、创建一个项目\n2、在标签位置填写我们前面模板中定义的Label\n3、直接在构建处执行shell进行测试\n然后点击构建，在终端可以看到整个过程：\n[root@master manifests]# kubectl get pod -n devops -w\rNAME READY STATUS RESTARTS AGE\rjenkins-6595ddd5d-m5fvd 1/1 Running 0 144m\rjenkins-slave-kkc2b 0/1 Pending 0 0s\rjenkins-slave-kkc2b 0/1 Pending 0 0s\rjenkins-slave-kkc2b 0/1 ContainerCreating 0 0s\rjenkins-slave-kkc2b 1/1 Running 0 3s\rjenkins-slave-kkc2b 1/1 Terminating 0 31s\rjenkins-slave-kkc2b 1/1 Terminating 0 31s\r也可以在jenkins里看日志如下：\n二、Pipeline 2.1、简介 Pipeline，简单来说，就是一套运行在 Jenkins 上的工作流框架，将原来独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排和可视化的工作。\nJenkins Pipeline 有几个核心概念：\n Node：节点，一个 Node 就是一个 Jenkins 节点，Master 或者 Agent，是执行 Step 的具体运行环境，比如我们之前动态运行的 Jenkins Slave 就是一个 Node 节点 Stage：阶段，一个 Pipeline 可以划分为若干个 Stage，每个 Stage 代表一组操作，比如：Build、Test、Deploy，Stage 是一个逻辑分组的概念，可以跨多个 Node Step：步骤，Step 是最基本的操作单元，可以是打印一句话，也可以是构建一个 Docker 镜像，由各类 Jenkins 插件提供，比如命令：sh \u0026lsquo;make\u0026rsquo;，就相当于我们平时 shell 终端中执行 make 命令一样。  Pipeline的使用：\n Pipeline 脚本是由 Groovy 语言实现的 Pipeline 支持两种语法：Declarative(声明式)和 Scripted Pipeline(脚本式)语法 Pipeline 也有两种创建方法：可以直接在 Jenkins 的 Web UI 界面中输入脚本；也可以通过创建一个 Jenkinsfile 脚本文件放入项目源码库中 一般我们都推荐在 Jenkins 中直接从源代码控制(SCMD)中直接载入 Jenkinsfile Pipeline 这种方法  2.2、创建 2.2.1、简单的Pipeline 直接 在Jenkins的WEB UI上输入脚本。\n脚本内容：\nnode {\rstage('Clone') {\recho \u0026quot;1.Clone Stage\u0026quot;\r}\rstage('Test') {\recho \u0026quot;2.Test Stage\u0026quot;\r}\rstage('Build') {\recho \u0026quot;3.Build Stage\u0026quot;\r}\rstage('Deploy') {\recho \u0026quot;4. Deploy Stage\u0026quot;\r}\r}\r然后保存\u0026ndash;\u0026gt; 点击构建\u0026ndash;\u0026gt; 观察日志\n输出符合我们脚本内容。\n2.2.2、在slave中运行Pipeline 上面对Jenkins的Pipeline做了简单的测试，但是其并未在我们的Slave中运行，如果要在Slave中运行，其就要使用我们前面添加的Label，如下：\nnode('joker-jnlp') {\rstage('Clone') {\recho \u0026quot;1.Clone Stage\u0026quot;\r}\rstage('Test') {\recho \u0026quot;2.Test Stage\u0026quot;\r}\rstage('Build') {\recho \u0026quot;3.Build Stage\u0026quot;\r}\rstage('Deploy') {\recho \u0026quot;4. Deploy Stage\u0026quot;\r}\r}\r然后我们保存并点击构建，观察Pod的变化：\n[root@master ~]# kubectl get pod -n devops -w\rNAME READY STATUS RESTARTS AGE\rjenkins-6595ddd5d-m5fvd 1/1 Running 0 2d23h\rjenkins-slave-vq8wf 0/1 Pending 0 0s\rjenkins-slave-vq8wf 0/1 Pending 0 0s\rjenkins-slave-vq8wf 0/1 ContainerCreating 0 0s\rjenkins-slave-vq8wf 1/1 Running 0 2s\rjenkins-slave-vq8wf 1/1 Terminating 0 27s\rjenkins-slave-vq8wf 1/1 Terminating 0 27s\r我们可以看到其依据我们定义的模板动态生成了jenkins-slave的Pod，我们在Jenkins的日志中查看：\n可以看到两个的名字是一样的。\n2.2.3、部署完整应用 部署应用的流程如下：\n 编写代码 测试 编写 Dockerfile 构建打包 Docker 镜像 推送 Docker 镜像到仓库 编写 Kubernetes YAML 文件 更改 YAML 文件中 Docker 镜像 TAG 利用 kubectl 工具部署应用  所以基本的Pipeline脚本框架应该如下：\nnode('joker-jnlp') {\rstage('Clone') {\recho \u0026quot;1.Clone Stage\u0026quot;\r}\rstage('Test') {\recho \u0026quot;2.Test Stage\u0026quot;\r}\rstage('Build') {\recho \u0026quot;3.Build Docker Image Stage\u0026quot;\r}\rstage('Push') {\recho \u0026quot;4.Push Docker Image Stage\u0026quot;\r}\rstage('YAML') {\recho \u0026quot;5. Change YAML File Stage\u0026quot;\r}\rstage('Deploy') {\recho \u0026quot;6. Deploy Stage\u0026quot;\r}\r}\r第一步：克隆代码\nstage('Clone') {\recho \u0026quot;1.Clone Stage\u0026quot;\rgit url: \u0026quot;https://github.com/baidjay/jenkins-demo.git\u0026quot;\rscript {\rbuild_tag = sh(returnStdout: true, script: 'git rev-parse --short HEAD').trim()\r}\recho \u0026quot;${build_tag}\u0026quot;\r}\r我们这里采用和git commit的记录为镜像的 tag，这里有一个好处就是镜像的 tag 可以和 git 提交记录对应起来，也方便日后对应查看。但是由于这个 tag 不只是我们这一个 stage 需要使用，下一个推送镜像是不是也需要，所以这里我们把这个 tag 编写成一个公共的参数，把它放在 Clone 这个 stage 中。\n第二步：测试\nstage('Test') {\recho \u0026quot;2.Test Stage\u0026quot;\r}\r测试可以是单测，也可以是工具，我们这里就简单存在这个步骤。\n第三步：构建镜像\nstage('Build') {\recho \u0026quot;3.Build Docker Image Stage\u0026quot;\rsh \u0026quot;docker build -t registry.cn-hangzhou.aliyuncs.com/ik9s/jenkins-demo:${build_tag} .\u0026quot;\r}\r这一步我们就使用到上面定义的build_tag变量。\n第四步：推送镜像\nstage('Push') {\recho \u0026quot;4.Push Docker Image Stage\u0026quot;\rsh \u0026quot;docker login --username=www.565361785@qq.com registry.cn-hangzhou.aliyuncs.com -p xxxxx\u0026quot;\rsh \u0026quot;docker push registry.cn-hangzhou.aliyuncs.com/ik9s/jenkins-demo:${build_tag}\u0026quot;\r}\r配置Jenkins，隐藏用户名密码信息：\n其中ID：AliRegistry 是我们后面要用的值。\n这样我们上面的脚本就可以定义如下：\nstage('Push') {\recho \u0026quot;4.Push Docker Image Stage\u0026quot;\rwithCredentials([usernamePassword(credentialsId: 'AliRegistry', passwordVariable: 'AliRegistryPassword', usernameVariable: 'AliRegistryUser')]) {\rsh \u0026quot;docker login -u ${AliRegistryUser} registry.cn-hangzhou.aliyuncs.com -p ${AliRegistryPassword}\u0026quot;\rsh \u0026quot;docker push registry.cn-hangzhou.aliyuncs.com/ik9s/jenkins-demo:${build_tag}\u0026quot;\r}\r}\r注意我们这里在 stage 中使用了一个新的函数withCredentials，其中有一个 credentialsId 值就是我们刚刚创建的 ID 值，而对应的用户名变量就是 ID 值加上 User，密码变量就是 ID 值加上 Password，然后我们就可以在脚本中直接使用这里两个变量值来直接替换掉之前的登录 docker hub 的用户名和密码，现在是不是就很安全了，我只是传递进去了两个变量而已，别人并不知道我的真正用户名和密码，只有我们自己的 Jenkins 平台上添加的才知道。\n第五步：更改YAML文件\nstage('YAML') {\recho \u0026quot;5. Change YAML File Stage\u0026quot;\rsh \u0026quot;sed -i 's/\u0026lt;BUILD_TAG\u0026gt;/${build_tag}/' k8s.yaml\u0026quot;\rsh \u0026quot;sed -i 's/\u0026lt;BRANCH_NAME\u0026gt;/${env.BRANCH_NAME}/' k8s.yaml\u0026quot;\r}\r其YAML文件为（YAML文件放在项目根目录）：\napiVersion: extensions/v1beta1\rkind: Deployment\rmetadata:\rname: jenkins-demo\rspec:\rtemplate:\rmetadata:\rlabels:\rapp: jenkins-demo\rspec:\rcontainers:\r- image: registry.cn-hangzhou.aliyuncs.com/ik9s/jenkins-demo:\u0026lt;BUILD_TAG\u0026gt;\rimagePullPolicy: IfNotPresent\rname: jenkins-demo\renv:\r- name: branch\rvalue: \u0026lt;BRANCH_NAME\u0026gt;\r第六步：部署\n部署阶段我们增加人工干预，可能需要将该版本先发布到测试环境、QA 环境、或者预览环境之类的，总之直接就发布到线上环境去还是挺少见的，所以我们需要增加人工确认的环节，一般都是在 CD 的环节才需要人工干预，比如我们这里的最后两步，我们就可以在前面加上确认，比如：\n我们将YAML这一步改为：\nstage('YAML') {\recho \u0026quot;5. Change YAML File Stage\u0026quot;\rdef userInput = input(\rid: 'userInput',\rmessage: 'Choose a deploy environment',\rparameters: [\r[\r$class: 'ChoiceParameterDefinition',\rchoices: \u0026quot;Dev\\nQA\\nProd\u0026quot;,\rname: 'Env'\r]\r]\r)\recho \u0026quot;This is a deploy step to ${userInput.Env}\u0026quot;\rsh \u0026quot;sed -i 's/\u0026lt;BUILD_TAG\u0026gt;/${build_tag}/' k8s.yaml\u0026quot;\rsh \u0026quot;sed -i 's/\u0026lt;BRANCH_NAME\u0026gt;/${env.BRANCH_NAME}/' k8s.yaml\u0026quot;\rsh \u0026quot;sed -i 's#cnych/jenkins-demo#registry.cn-hangzhou.aliyuncs.com/ik9s/jenkins-demo#' k8s.yaml\u0026quot;\r}\r然后再部署阶段：\nstage('Deploy') {\recho \u0026quot;6. Deploy Stage\u0026quot;\rif (userInput.Env == \u0026quot;Dev\u0026quot;) {\r// deploy dev stuff\r} else if (userInput.Env == \u0026quot;QA\u0026quot;){\r// deploy qa stuff\r} else {\r// deploy prod stuff\r}\rsh \u0026quot;kubectl apply -f k8s.yaml\u0026quot;\r}\r由于这一步也属于部署的范畴，所以我们可以将最后两步都合并成一步,我们最终的Pipeline脚本如下：\nnode('joker-jnlp') {\rstage('Clone') {\recho \u0026quot;1.Clone Stage\u0026quot;\rgit url: \u0026quot;https://github.com/baidjay/jenkins-demo.git\u0026quot;\rscript {\rbuild_tag = sh(returnStdout: true, script: 'git rev-parse --short HEAD').trim()\r}\recho \u0026quot;${build_tag}\u0026quot;\r}\rstage('Test') {\recho \u0026quot;2.Test Stage\u0026quot;\r}\rstage('Build') {\recho \u0026quot;3.Build Docker Image Stage\u0026quot;\rsh \u0026quot;docker build -t registry.cn-hangzhou.aliyuncs.com/ik9s/jenkins-demo:${build_tag} .\u0026quot;\r}\rstage('Push') {\recho \u0026quot;4.Push Docker Image Stage\u0026quot;\rwithCredentials([usernamePassword(credentialsId: 'AliRegistry', passwordVariable: 'AliRegistryPassword', usernameVariable: 'AliRegistryUser')]) {\rsh \u0026quot;docker login -u ${AliRegistryUser} registry.cn-hangzhou.aliyuncs.com -p ${AliRegistryPassword}\u0026quot;\rsh \u0026quot;docker push registry.cn-hangzhou.aliyuncs.com/ik9s/jenkins-demo:${build_tag}\u0026quot;\r}\r}\rstage('Deploy') {\recho \u0026quot;5. Change YAML File Stage\u0026quot;\rdef userInput = input(\rid: 'userInput',\rmessage: 'Choose a deploy environment',\rparameters: [\r[\r$class: 'ChoiceParameterDefinition',\rchoices: \u0026quot;Dev\\nQA\\nProd\u0026quot;,\rname: 'Env'\r]\r]\r)\recho \u0026quot;This is a deploy step to ${userInput}\u0026quot;\rsh \u0026quot;sed -i 's/\u0026lt;BUILD_TAG\u0026gt;/${build_tag}/' k8s.yaml\u0026quot;\rsh \u0026quot;sed -i 's/\u0026lt;BRANCH_NAME\u0026gt;/${env.BRANCH_NAME}/' k8s.yaml\u0026quot;\recho \u0026quot;6. Deploy Stage\u0026quot;\rif (userInput == \u0026quot;Dev\u0026quot;) {\r// deploy dev stuff\r} else if (userInput == \u0026quot;QA\u0026quot;){\r// deploy qa stuff\r} else {\r// deploy prod stuff\r}\rsh \u0026quot;kubectl apply -f k8s.yaml -n default\u0026quot;\r}\r}\r然后构建面板如下：\n然后查看Pod日志如下：\n[root@master jenkins]# kubectl logs jenkins-demo-789fdc6878-5pzbx\rHello, Kubernetes！I'm from Jenkins CI！\r2.2.4、Jenkinsfile 万里长征，貌似我们的任务完成了，其实不然，我们这里只是完成了一次手动的添加任务的构建过程，在实际的工作实践中，我们更多的是将 Pipeline 脚本写入到 Jenkinsfile 文件中，然后和代码一起提交到代码仓库中进行版本管理。现在我们将上面的 Pipeline 脚本拷贝到一个 Jenkinsfile 中，将该文件放入上面的 git 仓库中，但是要注意的是，现在既然我们已经在 git 仓库中了，是不是就不需要 git clone 这一步骤了，所以我们需要将第一步 Clone 操作中的 git clone 这一步去掉。\n如下：\nnode('joker-jnlp') {\rstage('Prepare') {\recho \u0026quot;1.Prepare Stage\u0026quot;\rscript {\rbuild_tag = sh(returnStdout: true, script: 'git rev-parse --short HEAD').trim()\r}\recho \u0026quot;${build_tag}\u0026quot;\r}\rstage('Test') {\recho \u0026quot;2.Test Stage\u0026quot;\r}\rstage('Build') {\recho \u0026quot;3.Build Docker Image Stage\u0026quot;\rsh \u0026quot;docker build -t registry.cn-hangzhou.aliyuncs.com/ik9s/jenkins-demo:${build_tag} .\u0026quot;\r}\rstage('Push') {\recho \u0026quot;4.Push Docker Image Stage\u0026quot;\rwithCredentials([usernamePassword(credentialsId: 'AliRegistry', passwordVariable: 'AliRegistryPassword', usernameVariable: 'AliRegistryUser')]) {\rsh \u0026quot;docker login -u ${AliRegistryUser} registry.cn-hangzhou.aliyuncs.com -p ${AliRegistryPassword}\u0026quot;\rsh \u0026quot;docker push registry.cn-hangzhou.aliyuncs.com/ik9s/jenkins-demo:${build_tag}\u0026quot;\r}\r}\rstage('Deploy') {\recho \u0026quot;5. Change YAML File Stage\u0026quot;\rdef userInput = input(\rid: 'userInput',\rmessage: 'Choose a deploy environment',\rparameters: [\r[\r$class: 'ChoiceParameterDefinition',\rchoices: \u0026quot;Dev\\nQA\\nProd\u0026quot;,\rname: 'Env'\r]\r]\r)\recho \u0026quot;This is a deploy step to ${userInput}\u0026quot;\rsh \u0026quot;sed -i 's/\u0026lt;BUILD_TAG\u0026gt;/${build_tag}/' k8s.yaml\u0026quot;\rsh \u0026quot;sed -i 's/\u0026lt;BRANCH_NAME\u0026gt;/${env.BRANCH_NAME}/' k8s.yaml\u0026quot;\recho \u0026quot;6. Deploy Stage\u0026quot;\rif (userInput == \u0026quot;Dev\u0026quot;) {\r// deploy dev stuff\r} else if (userInput == \u0026quot;QA\u0026quot;){\r// deploy qa stuff\r} else {\r// deploy prod stuff\r}\rsh \u0026quot;kubectl apply -f k8s.yaml -n default\u0026quot;\r}\r}\r然后我们更改上面的 jenkins-demo 这个任务，点击 Configure -\u0026gt; 最下方的 Pipeline 区域 -\u0026gt; 将之前的 Pipeline Script 更改成 Pipeline Script from SCM，然后根据我们的实际情况填写上对应的仓库配置，要注意 Jenkinsfile 脚本路径。\n在实际的项目中，往往一个代码仓库都会有很多分支的，比如开发、测试、线上这些分支都是分开的，一般情况下开发或者测试的分支我们希望提交代码后就直接进行 CI/CD 操作，而线上的话最好增加一个人工干预的步骤，这就需要 Jenkins 对代码仓库有多分支的支持，当然这个特性是被 Jenkins 支持的。\n然后新建一个 Jenkinsfile 文件，配置如下：\nnode('joker-jnlp') {\rstage('Prepare') {\recho \u0026quot;1.Prepare Stage\u0026quot;\rcheckout scm\rscript {\rbuild_tag = sh(returnStdout: true, script: 'git rev-parse --short HEAD').trim()\rif (env.BRANCH_NAME != 'master') {\rbuild_tag = \u0026quot;${env.BRANCH_NAME}-${build_tag}\u0026quot;\r}\r}\r}\rstage('Test') {\recho \u0026quot;2.Test Stage\u0026quot;\r}\rstage('Build') {\recho \u0026quot;3.Build Docker Image Stage\u0026quot;\rsh \u0026quot;docker build -t registry.cn-hangzhou.aliyuncs.com/ik9s/jenkins-demo:${build_tag} .\u0026quot;\r}\rstage('Push') {\recho \u0026quot;4.Push Docker Image Stage\u0026quot;\rwithCredentials([usernamePassword(credentialsId: 'AliRegistry', passwordVariable: 'AliRegistryPassword', usernameVariable: 'AliRegistryUser')]) {\rsh \u0026quot;docker login -u ${AliRegistryUser} registry.cn-hangzhou.aliyuncs.com -p ${AliRegistryPassword}\u0026quot;\rsh \u0026quot;docker push registry.cn-hangzhou.aliyuncs.com/ik9s/jenkins-demo:${build_tag}\u0026quot;\r}\r}\rstage('Deploy') {\recho \u0026quot;5. Deploy Stage\u0026quot;\rif (env.BRANCH_NAME == 'master') {\rinput \u0026quot;确认要部署线上环境吗？\u0026quot;\r}\rsh \u0026quot;sed -i 's/\u0026lt;BUILD_TAG\u0026gt;/${build_tag}/' k8s.yaml\u0026quot;\rsh \u0026quot;sed -i 's/\u0026lt;BRANCH_NAME\u0026gt;/${env.BRANCH_NAME}/' k8s.yaml\u0026quot;\rsh \u0026quot;kubectl apply -f k8s.yaml --record\u0026quot;\r}\r}\r在第一步中我们增加了checkout scm命令，用来检出代码仓库中当前分支的代码，为了避免各个环境的镜像 tag 产生冲突，我们为非 master 分支的代码构建的镜像增加了一个分支的前缀，在第五步中如果是 master 分支的话我们才增加一个确认部署的流程，其他分支都自动部署，并且还需要替换 k8s.yaml 文件中的环境变量的值。\n三、BlueOcean 我们这里使用 BlueOcean 这种方式来完成此处 CI/CD 的工作，BlueOcean 是 Jenkins 团队从用户体验角度出发，专为 Jenkins Pipeline 重新设计的一套 UI 界面，仍然兼容以前的 fressstyle 类型的 job，BlueOcean 具有以下的一些特性：\n 连续交付（CD）Pipeline 的复杂可视化，允许快速直观的了解 Pipeline 的状态 可以通过 Pipeline 编辑器直观的创建 Pipeline 需要干预或者出现问题时快速定位，BlueOcean 显示了 Pipeline 需要注意的地方，便于异常处理和提高生产力 用于分支和拉取请求的本地集成可以在 GitHub 或者 Bitbucket 中与其他人进行代码协作时最大限度提高开发人员的生产力。  BlueOcean 可以安装在现有的 Jenkins 环境中，也可以使用 Docker 镜像的方式直接运行，我们这里直接在现有的 Jenkins 环境中安装 BlueOcean 插件：登录 Jenkins Web UI -\u0026gt; 点击左侧的 Manage Jenkins -\u0026gt; Manage Plugins -\u0026gt; Available -\u0026gt; 搜索查找 BlueOcean -\u0026gt; 点击下载安装并重启\n3.1、创建Pipeline 点击创建：\n获取Token的步骤：\n然后获取Token：\n创建完成如下所示：\n","description":"本文主要介绍如何在kubernetes中部署Jenkins以及如何配置动态生成slave","id":18,"section":"posts","tags":["kubernetes","jenkins","devops","CI/CD"],"title":"在kubernetes中部署Jenkins并配置动态生成slave","uri":"https://www.coolops.cn/posts/kubernetes-jenkins-install/"},{"content":"什么是ArgoCD  Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes.\n Argo CD是一个基于Kubernetes的声明式的GitOps工具。\n在说Argo CD之前，我们先来了解一下什么是GitOps。\n什么是GitOps GitOps是以Git为基础，使用CI/CD来更新运行在云原生环境的应用，它秉承了DevOps的核心理念\u0026ndash;“构建它并交付它(you built it you ship it)”。\n概念说起来有点虚，我画了张图，看了你就明白了。\n 当开发人员将开发完成的代码推送到git仓库会触发CI制作镜像并推送到镜像仓库 CI处理完成后，可以手动或者自动修改应用配置，再将其推送到git仓库 GitOps会同时对比目标状态和当前状态，如果两者不一致会触发CD将新的配置部署到集群中  其中，目标状态是Git中的状态，现有状态是集群的里的应用状态。\n不用GitOps可以么？\n当然可以，我们可以使用kubectl、helm等工具直接发布配置，但这会存在一个很严重的安全问题，那就是密钥共享。\n为了让CI系统能够自动的部署应用，我们需要将集群的访问密钥共享给它，这会带来潜在的安全问题。\nArgoCD Argo CD遵循GitOps模式，使用Git存储库存储所需应用程序的配置。\nKubernetes清单可以通过以下几种方式指定:\n kustomize应用程序 helm图表 ksonnet应用程序 jsonnet文件 基于YAML/json配置 配置管理插件配置的任何自定义配置管理工具  Argo CD实现为kubernetes控制器，它持续监视运行中的应用程序，并将当前的活动状态与期望的目标状态进行比较(如Git repo中指定的那样)。如果已部署的应用程序的活动状态偏离了目标状态，则认为是OutOfSync。Argo CD报告和可视化这些差异，同时提供了方法，可以自动或手动将活动状态同步回所需的目标状态。在Git repo中对所需目标状态所做的任何修改都可以自动应用并反映到指定的目标环境中。\nArgo CD就处在如下位置：\n它的优势总结如下：\n 应用定义、配置和环境信息是声明式的，并且可以进行版本控制； 应用部署和生命周期管理是全自动化的，是可审计的，清晰易懂； Argo CD是一个独立的部署工具，支持对多个环境、多个Kubernetes集群上的应用进行统一部署和管理  实践  前提：有一个可用的Kubernetes集群。\n 实验环境：\n kubernetes：1.17.2 argo cd：latest  安装Argo CD 安装很简单，不过在实际使用中需要对数据进行持久化。\n我这里直接使用官方文档的安装命令：\nkubectl create namespace argocd\rkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\r执行成功后会在argocd的namespace下创建如下资源。\n# kubectl get all -n argocd NAME READY STATUS RESTARTS AGE\rpod/argocd-application-controller-0 1/1 Running 0 16h\rpod/argocd-dex-server-74d9998fdb-mvpmh 1/1 Running 0 16h\rpod/argocd-redis-59dbdbb8f9-msxrp 1/1 Running 0 16h\rpod/argocd-repo-server-599bdc7cf5-ccv8l 1/1 Running 0 16h\rpod/argocd-server-576b4c7ff4-cnp9d 1/1 Running 0 16h\rNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rservice/argocd-dex-server ClusterIP 10.105.217.139 \u0026lt;none\u0026gt; 5556/TCP,5557/TCP,5558/TCP 16h\rservice/argocd-metrics ClusterIP 10.97.116.36 \u0026lt;none\u0026gt; 8082/TCP 16h\rservice/argocd-redis ClusterIP 10.105.63.34 \u0026lt;none\u0026gt; 6379/TCP 16h\rservice/argocd-repo-server ClusterIP 10.111.153.131 \u0026lt;none\u0026gt; 8081/TCP,8084/TCP 16h\rservice/argocd-server ClusterIP 10.105.229.250 \u0026lt;none\u0026gt; 80/TCP,443/TCP 16h\rservice/argocd-server-metrics ClusterIP 10.104.8.45 \u0026lt;none\u0026gt; 8083/TCP 16h\rNAME READY UP-TO-DATE AVAILABLE AGE\rdeployment.apps/argocd-dex-server 1/1 1 1 16h\rdeployment.apps/argocd-redis 1/1 1 1 16h\rdeployment.apps/argocd-repo-server 1/1 1 1 16h\rdeployment.apps/argocd-server 1/1 1 1 16h\rNAME DESIRED CURRENT READY AGE\rreplicaset.apps/argocd-dex-server-74d9998fdb 1 1 1 16h\rreplicaset.apps/argocd-redis-59dbdbb8f9 1 1 1 16h\rreplicaset.apps/argocd-repo-server-599bdc7cf5 1 1 1 16h\rreplicaset.apps/argocd-server-576b4c7ff4 1 1 1 16h\rNAME READY AGE\rstatefulset.apps/argocd-application-controller 1/1 16h\r访问Argo server的方式有两种：\n 通过web ui 使用argocd 客户端工具  我这里直接使用web ui进行管理。\n通过kubectl edit -n argocd svc argocd-server将service的type类型改为NodePort。改完后通过以下命令查看端口：\n# kubectl get svc -n argocd NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rargocd-dex-server ClusterIP 10.105.217.139 \u0026lt;none\u0026gt; 5556/TCP,5557/TCP,5558/TCP 17h\rargocd-metrics ClusterIP 10.97.116.36 \u0026lt;none\u0026gt; 8082/TCP 17h\rargocd-redis ClusterIP 10.105.63.34 \u0026lt;none\u0026gt; 6379/TCP 17h\rargocd-repo-server ClusterIP 10.111.153.131 \u0026lt;none\u0026gt; 8081/TCP,8084/TCP 17h\rargocd-server NodePort 10.105.229.250 \u0026lt;none\u0026gt; 80:32109/TCP,443:30149/TCP 17h\rargocd-server-metrics ClusterIP 10.104.8.45 \u0026lt;none\u0026gt; 8083/TCP 17h\r然后通过http://IP:32109访问页面，如下：\n登录账号为admin，密码通过以下命令获取。\nkubectl get pods -n argocd -l app.kubernetes.io/name=argocd-server -o name | cut -d'/' -f 2\r然后进入如下界面。\n更改密码 默认密码是pod的名字，使用客户端修改密码。\n（1）下载客户端，在github上下载即可，https://github.com/argoproj/argo-cd/releases\n（2）用客户端登录\n# argocd login 172.17.100.50:32109\rWARNING: server certificate had error: x509: cannot validate certificate for 172.17.100.50 because it doesn't contain any IP SANs. Proceed insecurely (y/n)? y\rUsername: admin\rPassword: 'admin' logged in successfully\rContext '172.17.100.50:32109' updated\r（3）修改密码\n# argocd account update-password \\\r--account admin \\\r--current-password argocd-server-5dcc6878cf-75j94 \\\r--new-password admin\rPassword updated\rContext '172.17.100.50:32109' updated\r创建应用  这里仅仅是为了测试argo，所以并没有做ci部分。\n 我在gitlab上准备了一个仓库，仓库里的文件很简单，如下：\n其中manifests下就是一个deployment文件，内容如下：\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rlabels:\rapp: devops-argocd-test\rname: devops-argocd-test\rnamespace: default\rspec:\rminReadySeconds: 60\rprogressDeadlineSeconds: 600\rreplicas: 1\rrevisionHistoryLimit: 10\rselector:\rmatchLabels:\rapp: devops-argocd-test\rtemplate:\rmetadata:\rlabels:\rapp: devops-argocd-test\rspec:\rcontainers:\r- name: devops-argocd-test\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/argocd-test-app:v1\rimagePullPolicy: Always\rports:\r- containerPort: 8080\rname: tcp-8080\rprotocol: TCP\r---\rapiVersion: v1\rkind: Service\rmetadata:\rlabels:\rapp: devops-argocd-test\rname: devops-argocd-test\rnamespace: default\rspec:\rports:\r- name: tcp-8080\rport: 8080\rprotocol: TCP\rtargetPort: 8080\rselector:\rapp: devops-argocd-test\rsessionAffinity: None\rtype: NodePort\r现在我们在Argo里创建应用，步骤如下：\n（1）添加仓库地址，Settings → Repositories，点击 Connect Repo using HTTPS 按钮：\n填入以下信息。\n验证通过后显示如下：\n（2）创建应用\n创建完成后如下所示：\n由于我设置的是手动SYNC，所以需要点以下下面的SYNC进行同步。\n然后可以看到状态都变成正常。\n这时候我们在集群里可以看到创建了v1版本的应用了。\n# kubectl get pod | grep devops-argocd-test\rdevops-argocd-test-7f5fdd9fcf-xbzmp 1/1 Running 0 118s\r# kubectl get svc | grep devops-argocd-test\rdevops-argocd-test NodePort 10.97.159.140 \u0026lt;none\u0026gt; 8080:31980/TCP 2m6s\r这时候访问应用，如下：\n配置变更 接下来我手动进行配置变更，修改manifests下的deploymeny.yaml文件中的镜像为v2版本，如下：\n然后提交到仓库。\n这是到ArgoCD中可以看到状态变成了OutOfSync\n这时候再手动sync以下，直到状态都变正常。再访问上面的应用。\n可以看到应用已经更新部署了。\n我们可以看到整个应用的关系状态，如下：\n还可以看到部署历史。\n也可以通过这个界面进行回滚。\n不过这个回滚并不会回改gitlab上的代码哈。\n我上面设置的是手动，你可以设置为自动，自己动手测试一番吧。\n 官方文档：https://argoproj.github.io/argo-cd/#features\n ","description":"本文主要介绍gitops中的一个软件argocd，并附带安装","id":19,"section":"posts","tags":["kubernetes","argocd","gitops"],"title":"Argocd是什么？","uri":"https://www.coolops.cn/posts/argocd/"},{"content":"本章介绍 Pod 运行异常的排错方法。\n一般来说，无论 Pod 处于什么异常状态，都可以执行以下命令来查看 Pod 的状态\n kubectl get pod \u0026lt;pod-name\u0026gt; -o yaml 查看 Pod 的配置是否正确 kubectl describe pod \u0026lt;pod-name\u0026gt; 查看 Pod 的事件 kubectl logs \u0026lt;pod-name\u0026gt; [-c \u0026lt;container-name\u0026gt;] 查看容器日志  这些事件和日志通常都会有助于排查 Pod 发生的问题。\nPod 一直处于 Pending 状态 Pending 说明 Pod 还没有调度到某个 Node 上面。可以通过 kubectl describe pod \u0026lt;pod-name\u0026gt; 命令查看到当前 Pod 的事件，进而判断为什么没有调度。可能的原因包括\n 资源不足，集群内所有的 Node 都不满足该 Pod 请求的 CPU、内存、GPU 等资源 HostPort 已被占用，通常推荐使用 Service 对外开放服务端口  Pod 一直处于 Waiting 或 ContainerCreating 状态 首先还是通过 kubectl describe pod \u0026lt;pod-name\u0026gt; 命令查看到当前 Pod 的事件。可能的原因包括\n  镜像拉取失败，比如\n   配置了错误的镜像 Kubelet 无法访问镜像（国内环境访问 gcr.io 需要特殊处理） 私有镜像的密钥配置错误 镜像太大，拉取超时（可以适当调整 kubelet 的 --image-pull-progress-deadline 和 --runtime-request-timeout 选项）    CNI 网络错误，一般需要检查 CNI 网络插件的配置，比如\n   无法配置 Pod 网络 无法分配 IP 地址    容器无法启动，需要检查是否打包了正确的镜像或者是否配置了正确的容器参数\n  Pod 处于 ImagePullBackOff 状态 这通常是镜像名称配置错误或者私有镜像的密钥配置错误导致。这种情况可以使用 docker pull \u0026lt;image\u0026gt; 来验证镜像是否可以正常拉取。\n如果是私有镜像，需要首先创建一个 docker-registry 类型的 Secret\nkubectl create secret docker-registry my-secret --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL\r然后在容器中引用这个 Secret\nspec:\rcontainers:\r- name: private-reg-container\rimage: \u0026lt;your-private-image\u0026gt;\rimagePullSecrets:\r- name: my-secret\rPod 一直处于 CrashLoopBackOff 状态 CrashLoopBackOff 状态说明容器曾经启动了，但又异常退出了。此时可以先查看一下容器的日志\nkubectl logs \u0026lt;pod-name\u0026gt;\rkubectl logs --previous \u0026lt;pod-name\u0026gt;\r这里可以发现一些容器退出的原因，比如\n 容器进程退出 健康检查失败退出  此时如果还未发现线索，还可以到容器内执行命令来进一步查看退出原因\nkubectl exec cassandra -- cat /var/log/cassandra/system.log\r如果还是没有线索，那就需要 SSH 登录该 Pod 所在的 Node 上，查看 Kubelet 或者 Docker 的日志进一步排查了\n# 查询 Node\rkubectl get pod \u0026lt;pod-name\u0026gt; -o wide\rPod 处于 Error 状态 通常处于 Error 状态说明 Pod 启动过程中发生了错误。常见的原因包括\n 依赖的 ConfigMap、Secret 或者 PV 等不存在 请求的资源超过了管理员设置的限制，比如超过了 LimitRange 等 违反集群的安全策略，比如违反了 PodSecurityPolicy 等 容器无权操作集群内的资源，比如开启 RBAC 后，需要为 ServiceAccount 配置角色绑定  Pod 处于 Terminating 或 Unknown 状态 从 v1.5 开始，Kubernetes 不会因为 Node 失联而删除其上正在运行的 Pod，而是将其标记为 Terminating 或 Unknown 状态。想要删除这些状态的 Pod 有三种方法：\n 从集群中删除该 Node。使用公有云时，kube-controller-manager 会在 VM 删除后自动删除对应的 Node。而在物理机部署的集群中，需要管理员手动删除 Node（如 kubectl delete node \u0026lt;node-name\u0026gt;。 Node 恢复正常。Kubelet 会重新跟 kube-apiserver 通信确认这些 Pod 的期待状态，进而再决定删除或者继续运行这些 Pod。 用户强制删除。用户可以执行 kubectl delete pods \u0026lt;pod\u0026gt; --grace-period=0 --force 强制删除 Pod。除非明确知道 Pod 的确处于停止状态（比如 Node 所在 VM 或物理机已经关机），否则不建议使用该方法。特别是 StatefulSet 管理的 Pod，强制删除容易导致脑裂或者数据丢失等问题。  Pod 行为异常 这里所说的行为异常是指 Pod 没有按预期的行为执行，比如没有运行 podSpec 里面设置的命令行参数。这一般是 podSpec yaml 文件内容有误，可以尝试使用 --validate 参数重建容器，比如\nkubectl delete pod mypod\rkubectl create --validate -f mypod.yaml\r也可以查看创建后的 podSpec 是否是对的，比如\nkubectl get pod mypod -o yaml\r修改静态 Pod 的 Manifest 后未自动重建 Kubelet 使用 inotify 机制检测 /etc/kubernetes/manifests 目录（可通过 Kubelet 的 --pod-manifest-path 选项指定）中静态 Pod 的变化，并在文件发生变化后重新创建相应的 Pod。但有时也会发生修改静态 Pod 的 Manifest 后未自动创建新 Pod 的情景，此时一个简单的修复方法是重启 Kubelet。\n","description":"本文主要介绍在kubernetes中pod的一些场景错误","id":20,"section":"posts","tags":["kubernetes","pod"],"title":"Kubernetes中Pod的一些常见错误","uri":"https://www.coolops.cn/posts/kubernetes-pod-common-error/"},{"content":"我们通过如下命令可以看到当前Kubernetes集群的管理用户是什么：\n[root@master ssl]# kubectl config view apiVersion: v1\rclusters:\r- cluster:\rcertificate-authority-data: DATA+OMITTED\rserver: https://172.16.1.128:6443\rname: cluster1\rcontexts:\r- context:\rcluster: cluster1\ruser: admin\rname: context-cluster1-admin\r- context:\rcluster: kubernetes\rnamespace: kube-system\ruser: joker\rname: joker-context\rcurrent-context: context-cluster1-admin\rkind: Config\rpreferences: {}\rusers:\r- name: admin\ruser:\rclient-certificate-data: REDACTED\rclient-key-data: REDACTED\r- name: joker\ruser:\rclient-certificate: /root/k8s/rbac/joker.crt\rclient-key: /root/k8s/rbac/joker.key\r下面我们自定义证书文件，然后自定义用户来作为集群的管理用户：\n1、我们用的ca证书还是集群的原有证书，我的证书位置在如下位置：\n[root@master ssl]# pwd\r/etc/kubernetes/ssl\rYou have new mail in /var/spool/mail/root\r[root@master ssl]# ll\rtotal 72\r-rw-r--r-- 1 root root 1679 Sep 4 18:17 admin-key.pem\r-rw-r--r-- 1 root root 1391 Sep 4 18:17 admin.pem\r-rw-r--r-- 1 root root 997 Sep 4 18:22 aggregator-proxy.csr\r-rw-r--r-- 1 root root 219 Sep 4 16:54 aggregator-proxy-csr.json\r-rw------- 1 root root 1679 Sep 4 18:22 aggregator-proxy-key.pem\r-rw-r--r-- 1 root root 1383 Sep 4 18:22 aggregator-proxy.pem\r-rw-r--r-- 1 root root 294 Sep 4 16:42 ca-config.json\r-rw-r--r-- 1 root root 1679 Sep 4 16:42 ca-key.pem\r-rw-r--r-- 1 root root 1350 Sep 4 16:42 ca.pem\r-rw-r--r-- 1 root root 17 Sep 9 07:35 ca.srl\r-rw-r--r-- 1 root root 1082 Sep 4 18:23 kubelet.csr\r-rw-r--r-- 1 root root 281 Sep 4 16:56 kubelet-csr.json\r-rw------- 1 root root 1679 Sep 4 18:23 kubelet-key.pem\r-rw-r--r-- 1 root root 1448 Sep 4 18:23 kubelet.pem\r-rw-r--r-- 1 root root 1265 Sep 4 18:22 kubernetes.csr\r-rw-r--r-- 1 root root 466 Sep 4 16:54 kubernetes-csr.json\r-rw------- 1 root root 1675 Sep 4 18:22 kubernetes-key.pem\r-rw-r--r-- 1 root root 1631 Sep 4 18:22 kubernetes.pem\r2、自定义证书\n# 生成自定义key\r# (umask 077; openssl genrsa -out unclejoker.key 2048)\r# 生成csr签名请求文件\r# openssl req -new -key unclejoker.key -out unclejoker.csr -subj \u0026quot;/CN=unclejoker\u0026quot;\r# 用CA证书进行签名\r# openssl x509 -req -days 365 -in unclejoker.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out unclejoker.crt\r3、配置集群config，添加集群（由于我们这里只有一个集群，所以这步不操作）\n命令如下：\n# kubectl config set-cluster joker-kubernetes --server=https://172.16.1.128:6443 --certificate-authority=./ca.pem --embed-certs=true\r4、配置集群config，添加用户\n命令如下：\n# kubectl config set-credentials unclejoker --client-certificate=./unclejoker.crt --client-key=./unclejoker.key --embed-certs=true\r5、添加上下文关联contexts，命令如下：\n# kubectl config set-context unclejoker@joker-kubernetes --cluster=joker-kubernetes --user=unclejoker --namespace=default\r到此就配置完成，如果要使用这个用户，则配置current-context\n# kubectl config use-context unclejoker@joker-kubernetes\r这是我们使用kubectl等操作就是在unclejoker用户下执行了。\n","description":"本文主要介绍如何自定义用户管理集群，适合在权限分配比较严谨的场景","id":21,"section":"posts","tags":["kubernetes"],"title":"Kubernetes自定义管理集群的用户","uri":"https://www.coolops.cn/posts/kubernetes-costom-user/"},{"content":"你是不是和我有同样的感觉，那就是有些知识你明明知道，但是却没办法将其表述出来，确切的说是不怎么从何说起，如何去说。可能在你的脑袋里对每一个组件，每一个模块都有概念，但是却没办法将其完完整整的说出来，用大佬的话说是知识太片面了，或者是太零碎了，对其没有一个整体的把握，也就是全局观。\n我们在运维的时候，从发现问题到定位问题，其实是需要我们对自己的业务逻辑熟悉，对架构熟悉，而且处理问题的思路要清晰，这样才能更快的解决问题。如果没有一个全局的认识，那么处理问题的速度可能会大打折扣。今天我就尝试一下如何说出一套容器云平台。\n 以下内容纯个人理解，如果有不对的地方还请指出。\n 在说之前先考虑几个问题。\n1、为什么要用容器？它能给我们带来什么？和直接用虚拟机有何不同？\n2、如何选择容器云？用私有云自己搭建维护还是用公有云现有的容器云平台？\n3、怎么完成一个完整的架子？\n为什么用容器？ 接触过容器的朋友可能会很清楚的意识到，容器云其实也没那么神秘，它能实现的虚拟机依然可以实现，而且稳定性可能比容器云平台还高。既然这样为什么还有这么多公司用容器呢？\n其实，不论是公有云还是私有云，技术已经很成熟了，基本上可以满足我们所有的需求，但是容器也有其得天独厚的优势。\n1、启动速度快。容器的启动速度和虚拟机不是一个量级，容器可以达到秒级，而虚拟机基本都是分钟级别。\n2、资源利用率高。一个容器镜像最多几百MB，而一个虚拟机镜像基本都是已GB为单位了。再则，使用虚拟机我们需要在底层系统之上再创建一个虚拟机，而容器仅仅是一个进程，可以直接运行在底层系统之上。\n3、环境一致性。用虚拟机的时候经常会遇到环境不一致导致应用各种问题，而容器我们只需要将其打包成一个镜像，这个应用运行所需的基础环境都放在这个镜像中了，那么我们就可以在\u0026quot;任何\u0026quot;地方运行。\n4、故障迁移、自愈能力高。以kubernetes为例，kubernetes有各种controller来保证实际状态和期望状态高度一致，一个实例在某个节点故障可以快速另起一个实例，基本上无需人为干预，而且故障实例的资源会被回收。\n下面简单总结以下容器和虚拟机的区别。\n   特性 容器 虚拟机     启动 秒级 分钟级   硬盘使用 一般为 MB 一般为 GB   性能 接近原生 弱于   系统支持量 单机支持上千个容器 一般几十个    如何选择容器云平台？ 这个其实没有一个标准，主要还是看公司现在用的是哪一种。比如公司现在就用的公有云，那么直接用公有云就好，用公有云的好处是底层的网络、存储等等不需要自己去考虑、去维护，基本上只要出钱就可以了。但是如果直接用公有云的服务其实是存在一个依赖问题，对于以后要换云或者下云是有一定的阻碍的。\n私有云平台的话定制性高，很多都可以自我控制，但是维护成本大，而且底层的网络、存储这些专业的东西需要专业的人士来处理，稍有不慎就万劫不复~\n如何搭架子？  下面以私有云平台中使用kubernetes为例。\n 在选的好使用哪种方式过后，一般情况下会评估以下几个问题。\n1、集群会有多大的规模\n2、集群的高可用怎么保证\n3、存储如何选择\n4、集群如何监控\n5、应用如何部署\n6、日志如何收集\n7、如何演练故障\n1、集群会有多大规模 这个根据自己目前的业务情况来看，我个人认为上下浮动20%，而且集群本身可以快速的扩缩容。\n2、集群高可用如何保证 对于kubernetes来说，需要高可用就是master组件和etcd存储。对于etcd来说，本身就支持高可用部署，建议基数节点，可以和master组件部署在相同节点也可以部署在集群外。我个人建议部署在集群外，这样和集群是解耦合的。\n对于master组件来说，需要我们自己去做高可用的组件只有apiserver，其他组件可以在配置文件中配置。对于apiserver我们可以使用HAProxy+keepalived，也可以使用NGINX+keepalived等等。其他应用的高可用完全有kubernetes自己控制了。\n3、存储如何选择 存储的选择面其实非常多，有商业版的也有开源版的，商业版的好处就太明显了，一切问题找厂家。开源版的维护成本和学习成本比较高，比如ceph，它的维护成本和学习成本就很高。如果公司有现成的存储工程师，那么这都不是问题了，都找他来给你解决。如果用开源产品并且自己维护，就要好好考虑一下了，这个存储软件我能hold住么？如果故障了，我能解决么？它在业界的使用情况和相关的文档丰富么（方便自己处理问题~~）？\n4、集群如何监控 对于监控来说，目前容器的监控主流的就是Prometheus，可以说它是为容器而生的。\n它主要有以下几个组件。\n Prometheus Server exporters alter pushGateway Prometheus UI  其中Prometheus Server就是中枢神经，exporters就是负责收集监控信息的，alter负责处理报警相关，pushGateway用于接收定制监控信息的，UI就是一个简单的UI界面。整个监控系统看起来并不复杂，复杂的地方在于我们收集到了各种信息，如何提取我们需要的，如果制定相应的规则，说白了就是如何去提高监控的质量，具体的规则其实是需要研发，测试，运维共同制定的。\n应用如何部署 我个人觉得这主要是咱们CI/CD部分了，目前做CI/CD的开源软件很多，而且各有所长。目前做CI的开源软件活跃度比较高的就是Jenkins，我们也是使用的Jenkins。Jenkins社区的活跃度很高，而且基本上遇到的问题也都有答案。CD部分，因为咱们用的是容器，我们的制品是镜像，那么只要是可以部署镜像的方法都可以，比如调API，使用helm，或者直接用kubectl命令。\n这中间其实主要考虑几个问题：\n1、pipeline的规范，共享库的建立、管理等\n2、制品库的管理，包括备份、晋级、回滚等\n3、使用Jenkinsfile的话是放在共享库还是代码库\n日志如何收集 目前容器日志收集方案常用的就是EFK，根据日志量来确定需不需要加kafka或者redis。\n日志收集的方式大概有三种：\n 日志是stdout,stderr，直接用daemonSet方式收集 日志输出到文件，可以使用sidecar方式收集 日志直接输出到es、kafka或者redis中  收集完日志就需要对日志进行一系列的处理，比如清洗、聚合、告警等等。\n故障演练 主要有：\n1、集群故障演练，包括存储、网络、集群节点、数据库等等\n2、应用故障演练\n","description":"本文主要从个人角度看为什么要用Kubernetes，如何使用","id":22,"section":"posts","tags":["kubernetes"],"title":"如何使用Kubernetes","uri":"https://www.coolops.cn/posts/kubernetes-why-we-used/"},{"content":"解决方法  可使用kubectl中的强制删除命令  # 删除POD\rkubectl delete pod PODNAME --force --grace-period=0\r# 删除NAMESPACE\rkubectl delete namespace NAMESPACENAME --force --grace-period=0\r 若以上方法无法删除，可使用第二种方法，直接从ETCD中删除源数据  # 删除default namespace下的pod名为pod-to-be-deleted-0\rETCDCTL_API=3 etcdctl del /registry/pods/default/pod-to-be-deleted-0\r# 删除需要删除的NAMESPACE\retcdctl del /registry/namespaces/NAMESPACENAME\rkubernetes 有时候在K8S中删除一个 namespace 会卡住，强制删除也没用，前面我们介绍了可以去 etcd 里面去删除对应的数据，这种方式比较暴力，除此之外我们也可以通过 API 去删除。\n首先执行如下命令开启 API 代理：\nkubectl proxy\r然后在另外一个终端中执行如下所示的命令：(将 monitoring 替换成你要删除的 namespace 即可)\nkubectl get namespace monitoring -o json | jq 'del(.spec.finalizers[] | select(\u0026quot;kubernetes\u0026quot;))' | curl -s -k -H \u0026quot;Content-Type: application/json\u0026quot; -X PUT -o /dev/null --data-binary @- http://localhost:8001/api/v1/namespaces/monitoring/finalize\r如果这样还是不行，就手动去edit namespace，如下：\n kubectl edit ns \u0026lt;NAMESPACE\u0026gt; -o json\r然后找到里面的\u0026quot;finalizers\u0026quot;，把它的值设置成一个空数组。\n","description":"本文介绍如何强制删除pod和namespace","id":23,"section":"posts","tags":["kubernetes"],"title":"Kubernetes中强制删除Pod、namespace","uri":"https://www.coolops.cn/posts/kubernetes-delete-namespace/"},{"content":"设置一个preStop hook，在hook中指定怎么优雅停止容器在K8S中，创建pod、删除pod是最频繁的操作，不论是新增还是升级都会触发。对于新增或者重建我们最关心的是什么时候提供服务，对于删除我们关心的是什么时候不提供服务。那么对于这个临界点在K8S中是如何判定的呢？\n在讨论这个临界点之前，我们先看看创建或删除pod的流程。\n创建Pod的过程 当API收到创建Pod的请求，然后会将Pod的定义存储到etcd中，然后scheduler会将pod加入到调度队列中（如果没有做调度优先级配置，默认是放在队列最后），然后scheduler会根据预选、优选策略给pod分配一个最有的node节点，然后这个pod会被标记为Scheduled，并将其状态存储到etcd中。\n到目前为止pod还并没有被创建，因为创建Pod需要通过kubelet组件来完成。kubelet组件会通过apiserver来获取pod的状态，同样也会上报pod的状态。当某个节点检测到该pod是调度到自己节点的时候，就会在本节点创建这个pod，不过创建pod并不是kubelet自己动手，而是交给下面三个组件来完成。\n 容器运行时接口（CRI）：为 Pod 创建容器的组件。 容器网络接口（CNI）：将容器连接到集群网络并分配 IP 地址的组件。 容器存储接口（CSI）：在容器中装载卷的组件。  到现在pod创建完成了，然后会将该pod的状态上报给apiserver并存储在etcd中。\n现在pod创建完成了，但是在k8s中，pod并不适合直接提供服务，如果在集群内部是通过service来提供服务，如果集群外部需要访问，是通过ingress来提供访问入口。那如果我ingress以及service的某个pod发生了变化，它们又该如何更新呢？\n在这之前先简单介绍一下service和pod的关系。\nservice和pod是通过label selector（标签选择器）来进行关联的，只要符合service中定义的label selector，就会将其地址和端口维护到Endpoints中，如下：\n# kubectl describe svc website Name: website\rNamespace: default\rLabels: \u0026lt;none\u0026gt;\rAnnotations: kubectl.kubernetes.io/last-applied-configuration:\r{\u0026quot;apiVersion\u0026quot;:\u0026quot;v1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;Service\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;website\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;ports\u0026quot;:[{\u0026quot;name\u0026quot;:\u0026quot;http\u0026quot;,\u0026quot;...\rSelector: app=website\rType: ClusterIP\rIP: 10.101.58.163\rPort: http 80/TCP\rTargetPort: 80/TCP\rEndpoints: 192.168.4.9:80\rSession Affinity: None\rEvents: \u0026lt;none\u0026gt;\rEndpoints 对象会从 Pod 中收集所有的 IP 地址和端口，而且不仅一次。在以下情况中，Endpoint 对象将更新一个 endpiont 新列表：\n Pod 创建时。 Pod 删除时。 在 Pod 上修改标签时。  当pod通过Readiness探针后，才标识这个pod真正可用。当pod可用过后，service会通过label selector找到所有匹配的Pod，然后通过k8s更新endpoint，Endpoints也会做相应的更新。\n除了service，还有kube-proxy，ingress都会使用到endpoint，它们也会进行相应的更新，kube-proxy会通过endpoint来更新iptables或者ipvs规则，ingress更新endpoint是为了让pod接入外部流量。\n所以创建pod的过程以及pod创建完成后的一系列变化可以总结如下：\n1、apiserver收到创建pod的请求（可以是直接创建pod的定义，也可以是通过其他控制器来完成的）。\n2、Pod 的定义存储在 etcd 中。\n3、scheduler参与调度Pod，为其分配最优节点，并把相关信息存储到etcd中。\n4、kubelet监听到pod的信息，在节点上创建pod，分配资源以及IP等，将信息存储到etcd中。\n5、kubelet等待pod的Readiness探针成功，并对相关的Endpoints对象更改进行通知。\n6、Endpoints将新的endpoint添加到列表中。\n7、其他组件控制器根据Endpoints做相应的更改配置，比如kube-proxy会重新创建或者更改iptables/ipvs规则等。\n**\n**\n删除Pod的过程 删除pod的主要流程如下：\n  用户发送命令删除 Pod，使用的是默认的宽限期（30秒）\n  API 服务器中的 Pod 会随着宽限期规定的时间进行更新，过了这个时间 Pod 就会被认为已 “死亡”。\n  当使用客户端命令查询 Pod 状态时，Pod 显示为 “Terminating”。\n  （和第 3 步同步进行）当 Kubelet 看到 Pod 由于步骤 2 中设置的时间而被标记为 terminating 状态时，它就开始执行关闭 Pod 流程。\n   如果 Pod 定义了 preStop 钩子，就在 Pod 内部调用它。如果宽限期结束了，但是 preStop 钩子还在运行，那么就用小的（2 秒）扩展宽限期调用步骤 2。 给 Pod 内的进程发送 TERM 信号。请注意，并不是所有 Pod 中的容器都会同时收到 TERM 信号，如果它们关闭的顺序很重要，则每个容器可能都需要一个 preStop 钩子。    （和第 3 步同步进行）从服务的端点列表中删除 Pod，Pod 也不再被视为副本控制器的运行状态的 Pod 集的一部分。因为负载均衡器（如服务代理）会将其从轮换中删除，所以缓慢关闭的 Pod 无法继续为流量提供服务。\n  当宽限期到期时，仍在 Pod 中运行的所有进程都会被 SIGKILL 信号杀死。\n  kubelet 将通过设置宽限期为 0 （立即删除）来完成在 API 服务器上删除 Pod 的操作。该 Pod 从 API 服务器中消失，并且在客户端中不再可见。\n  在这里就有一个不确定因素，那就是你无法判断到底是pod先终止还是endpoints列表先更新。这里简单说两种情况。\n1、pod删除了endpoints还未更新，这种情况下会导致丢包。不管是从ingress还是从service来的流量，由于它们的endpoints并未及时更新，就会导致调度到已经不存在的Pod上，这样就会导致请求丢失。\n2、endpoints更新了，pod还未删除，这种情况下也会丢包。虽然前面流量进不来了，但是自己还未处理完的请求也响应不了。当然，如果pod的停止时间超过了默认的宽限期，就会被强制终止。\n鉴于此，就需要使用优雅退出来处理这种情况。\n优雅退出 优雅退出有两种常见的解决方法：\n 应用本身可以处理SIGTERM信号。 设置一个preStop hook，在hook中指定怎么优雅停止容器  在这之前先简单介绍一下SIGTERM和SIGKILL这两个信号。\n SIGKILL：立刻结束程序。该信号不能被阻塞、处理和忽略，不能在程序中被获取到。 SIGTERM：程序结束(Terminate)信号，又叫请求退出信号，与SIGKILL不同的是该信号可以被阻塞和处理，我们可以通过在程序中注册该信号来实现服务的优雅停止。使用kill命令缺省会发出这个信号。  那么具体应该如何做呢？\n其大概思路如下：当Pod收到SIGTERM信号的时候，先等待一段时间再退出。在这等待的过程中可以继续处理流量，等待时间过后再关闭长连接，关闭进程退出。当然如果超过等待时间，会直接被kill。\n 上面提到了等待时间时间，如果我们不设置，默认为30秒。如果设置为0，将立刻发送SIGKILL信号来杀死Pod内所有进程。如果要设置的话，请根据服务情况酌情设置，避免因为程序内有死锁或者其他原因带来的其他问题。\n 应用处理SIGTERM信号 在应用中处理SIGTERM信号的思路如下：程序在启动过后，会一直阻塞并监听系统信号，直到监测到对应的系统信号后，输出到控制台并退出执行。\n我们知道在容器中pid为1的进程是容器的主进程，这个进程退出则代表容器就退出了。\n在这我们需要注意一个问题，通过在Dockerfle中使用CMD、ENTRYPOINT命令可以定义容器启动命令，关于这两个命令的区别这里就不讲了，我们只讲在使用时候一定要注意的问题。\n这两个命令都支持下面几种格式：\n shell 格式：CMD \u0026lt;命令\u0026gt; exec 格式：CMD [\u0026ldquo;可执行文件\u0026rdquo;, \u0026ldquo;参数1\u0026rdquo;, \u0026ldquo;参数2\u0026rdquo;\u0026hellip;] 参数列表格式：CMD [\u0026ldquo;参数1\u0026rdquo;, \u0026ldquo;参数2\u0026rdquo;\u0026hellip;]。在指定了 ENTRYPOINT 指令后，用 CMD 指定具体的参数。  一般推荐使用 exec格式，这类格式在解析时会被解析为 JSON 数组，因此一定要使用双引号 \u0026quot;，而不要使用单引号'。\n如果使用 shell 格式的话，实际的命令会被包装为 sh -c 的参数的形式进行执行。比如：\nCMD java -jar demo.jar\r在实际执行中，会将其变更为：\nCMD [ \u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;java -jar demo.jar\u0026quot; ]\r因此容器的主进程是sh，当给容器发送信号，接收信号的是sh进程，sh进程收到信号后会直接退出，自然就会令容器退出。我们的程序永远收不到信号。\n使用preStop Hook来停止服务 preStop Hook是Pod资源定义中的一个参数，它支持http和exec，简单的demo如下：\nspec:\rcontaienrs:\r- name: my-container\rlifecycle:\rpreStop:\rexec:\rcommand: [\u0026quot;/bin/sh\u0026quot;，\u0026quot;-c\u0026quot;，\u0026quot;sleep 20\u0026quot;]\r用该方法的主要思路如下：当pod收到SIGTERM信号后，会调用preStop然后等待一段时间，比如15s，这15s的时间留给kube-proxy、ingress等来更新endpoints，等它们更新完后再开始停pod。\n","description":"本文主要介绍在kubernetes中如何使业务进行零宕机部署","id":24,"section":"posts","tags":["kubernetes"],"title":"Kubernetes 中优雅停机和零宕机部署","uri":"https://www.coolops.cn/posts/kubernetes-pod-grace-deploy/"},{"content":"以下文章来源于崔秀龙的博客，文章地址：https://blog.fleeto.us/post/tips-for-kubectl/\n原文：Ready-to-use commands and tips for kubectl\n作者：Flant staff\nKubectl 是 Kubernetes 最重要的命令行工具。在 Flant，我们会在 Wiki 和 Slack 上相互分享 Kubectl 的妙用（其实我们还有个搜索引擎，不过那就是另外一回事了）。多年以来，我们在 kubectl 方面积累了很多技巧，现在想要将其中的部分分享给社区。\n我相信很多读者对这些命令都非常熟悉；然而我还是希望读者能够从本文中有所获益，进而提高生产力。\n 下列内容有的是来自我们的工程师，还有的是来自互联网。我们对后者也进行了测试，并且确认其有效性。\n 现在开始吧。\n获取 Pod 和节点 （1）、我猜你知道如何获取 Kubernetes 集群中所有 Namespace 的 Pod——使用 \u0026ndash;all-namepsaces 就可以。然而不少朋友还不知道，现在这一开关还有了 -A 的缩写。\n（2）、如何查找非 running 状态的 Pod 呢？\nkubectl get pods -A --field-selector=status.phase!=Running | grep -v Complete\r顺便一说，\u0026ndash;field-selector 是个值得深入一点的参数。\n（3）、如何获取节点列表及其内存容量：\nkubectl get no -o json | \\\rjq -r '.items | sort_by(.status.capacity.memory)[]|[.metadata.name,.status.capacity.memory]| @tsv'\r（4）、获取节点列表，其中包含运行在每个节点上的 Pod 数量：\nkubectl get po -o json --all-namespaces | \\\rjq '.items | group_by(.spec.nodeName) | map({\u0026quot;nodeName\u0026quot;: .[0].spec.nodeName, \u0026quot;count\u0026quot;: length}) | sort_by(.count)'\r（5）、有时候 DaemonSet 因为某种原因没能在某个节点上启动。手动搜索会有点麻烦：\n$ ns=my-namespace\r$ pod_template=my-pod\r$ kubectl get node | grep -v \\\u0026quot;$(kubectl -n ${ns} get pod --all-namespaces -o wide | fgrep ${pod_template} | awk '{print $8}' | xargs -n 1 echo -n \u0026quot;\\|\u0026quot; | sed 's/[[:space:]]*//g')\\\u0026quot;\r（6）、使用 kubectl top 获取 Pod 列表并根据其消耗的 CPU 或 内存进行排序：\n# cpu\r$ kubectl top pods -A | sort --reverse --key 3 --numeric\r# memory\r$ kubectl top pods -A | sort --reverse --key 4 --numeric\r（7）、获取 Pod 列表，并根据重启次数进行排序：\nkubectl get pods –sort-by=.status.containerStatuses[0].restartCount\r当然也可以使用 PodStatus 以及 ContainerStatus 的其它字段进行排序。\n获取其它数据 （1）、运行 Ingress 时，经常要获取 Service 对象的 selector 字段，用来查找 Pod。过去要打开 Service 的清单才能完成这个任务，现在使用 -o wide 参数也可以：\n$ kubectl -n jaeger get svc -o wide\rNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR\rjaeger-cassandra ClusterIP None \u0026lt;none\u0026gt; 9042/TCP 77d app=cassandracluster,cassandracluster=jaeger-cassandra,cluster=jaeger-cassandra\r（2）、如何输出 Pod 的 requests 和 limits：\n$ kubectl get pods -A -o=custom-columns='NAME:spec.containers[*].name,MEMREQ:spec.containers[*].resources.requests.memory,MEMLIM:spec.containers[*].resources.limits.memory,CPUREQ:spec.containers[*].resources.requests.cpu,CPULIM:spec.containers[*].resources.limits.cpu'\rNAME MEMREQ MEMLIM CPUREQ CPULIM\rcoredns 70Mi 170Mi 100m \u0026lt;none\u0026gt;\rcoredns 70Mi 170Mi 100m \u0026lt;none\u0026gt;\r...\r（3）、kubectl run（以及 create、apply、patch）命令有个厉害的参数 \u0026ndash;dry-run，该参数让用户无需真正操作集群就能观察集群的行为，如果配合 -o yaml，就能输出命令对应的 YAML：\n$ kubectl run test --image=grafana/grafana --dry-run -o yaml\rapiVersion: apps/v1\rkind: Deployment\rmetadata:\rcreationTimestamp: null\rlabels:\rrun: test\rname: test\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rrun: test\r简单的把输出内容保存到文件，删除无用字段就可以使用了。\n 1.18 开始 kubectl run 生成的是 Pod 而非 Deployment。\n （4）、获取指定资源的描述清单：\nkubectl explain hpa\rKIND: HorizontalPodAutoscaler\rVERSION: autoscaling/v1\rDESCRIPTION:\rconfiguration of a horizontal pod autoscaler.\rFIELDS:\rapiVersion \u0026lt;string\u0026gt;\r...\r网络 （1）、获取集群节点的内部 IP：\n$ kubectl get nodes -o json | jq -r '.items[].status.addresses[]? | select (.type == \u0026quot;InternalIP\u0026quot;) | .address' | \\\rpaste -sd \u0026quot;\\n\u0026quot; -\r9.134.14.252\r（2）、获取所有的 Service 对象以及其 nodePort：\n$ kubectl get -A svc -o json | jq -r '.items[] | [.metadata.name,([.spec.ports[].nodePort | tostring ] | join(\u0026quot;|\u0026quot;))]| @tsv'\rkubernetes null\r...\r（3）、在排除 CNI（例如 Flannel）故障的时候，经常会需要检查路由来识别故障 Pod。Pod 子网在这里非常有用：\n$ kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' | tr \u0026quot; \u0026quot; \u0026quot;\\n\u0026quot; fix-doc-azure-container-registry-config ✭\r10.120.0.0/24\r10.120.1.0/24\r10.120.2.0/24\r日志 （1）、使用可读的时间格式输出日志：\n$ kubectl logs -f fluentbit-gke-qq9w9 -c fluentbit --timestamps\r2020-09-10T13:10:49.822321364Z Fluent Bit v1.3.11\r2020-09-10T13:10:49.822373900Z Copyright (C) Treasure Data\r2020-09-10T13:10:49.822379743Z\r2020-09-10T13:10:49.822383264Z [2020/09/10 13:10:49] [ info] Configuration:\r（2）、只输出尾部日志：\nkubectl logs -f fluentbit-gke-qq9w9 -c fluentbit --tail=10\r[2020/09/10 13:10:49] [ info] ___________\r[2020/09/10 13:10:49] [ info] filters:\r[2020/09/10 13:10:49] [ info] parser.0\r...\r（3）、输出一个 Pod 中所有容器的日志：\nkubectl -n my-namespace logs -f my-pod –all-containers\r（4）、使用标签选择器输出多个 Pod 的日志：\nkubectl -n my-namespace logs -f -l app=nginx\r（5）、获取“前一个”容器的日志（例如崩溃的情况）：\nkubectl -n my-namespace logs my-pod –previous\r其它 （1）、把 Secret 复制到其它命名空间：\nkubectl get secrets -o json --namespace namespace-old | \\\rjq '.items[].metadata.namespace = \u0026quot;namespace-new\u0026quot;' | \\\rkubectl create-f -\r（2）、下面两个命令可以生成一个用于测试的自签发证书：\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026quot;/CN=grafana.mysite.ru/O=MyOrganization\u0026quot;\rkubectl -n myapp create secret tls selfsecret --key tls.key --cert tls.crt\r相关链接 本文没什么结论，但是可以提供一个小列表，其中包含本文相关的有用链接。\n Kubernetes 官方文档：https://kubernetes.io/docs/reference/kubectl/cheatsheet/ Linux Academy 的入门参考：https://linuxacademy.com/blog/containers/kubernetes-cheat-sheet/ Blue Matador 分类整理的命令列表：https://www.bluematador.com/learn/kubectl-cheatsheet 另一个命令指南，部分内容和本文重复：https://gist.github.com/pydevops/0efd399befd960b5eb18d40adb68ef83 kubectl 别名搜集：https://github.com/ahmetb/kubectl-aliases  ","description":"本文主要介绍kubectl不一样的用法，值得尝试","id":25,"section":"posts","tags":["kubernetes","kubectl"],"title":"kubectl的奇巧淫技","uri":"https://www.coolops.cn/posts/kubernetes-kubectl-other-use/"},{"content":"将应用部署到Kubernetes中的方式有很多，目前主流是就是使用kubectl和Helm，不过其先决条件都需要YAML清单文件。\n不同由于部署环境的多样化，比如有开发环境、测试环境、预生产环境、生产环境，我们就会针对不同的环境定制各种YAML文件，但是在很多情况下同一个应用在不同的环境可能只做了简单的更改，这样就会导致YAML泛滥。\n而**Kustomize 就是用于帮助解决这些问题的开源配置管理工具。**从 Kubernetes v1.14 开始，kubectl 就完全支持 Kustomize 和 kustomization 文件。\nkustomize是什么？  kustomize lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is.\n 上面是官方对于kustomize的定义。大致是说：kustomize允许您自定义无模板的原始YAML文件来用于多种目的，而原始的YAML则保持不变并可以使用。\nkustomize的作用 当我们在K8S中有多套环境的时候，就会面临如下问题：\n 多环境多团队多个YAML资源清单 不同环境差异微小，但是不得不copy and change helm稍显复杂，需要额外的学习投入  而kustomize可以很好的解决这些问题：\n kustomize 通过 Base \u0026amp; Overlays 方式方式维护不同环境的应用配置 kustomize 使用 patch 方式复用 Base 配置，并在 Overlay 描述与 Base 应用配置的差异部分来实现资源复用 kustomize 管理的都是 Kubernetes 原生 YAML 文件，不需要学习额外的 DSL 语法  安装 在kubernetes 1.14版本以上，已经集成到kubectl中了，你可以通过kubectl --help来进行查看命令。\n如果需要额外安装，直接到https://github.com/kubernetes-sigs/kustomize/releases里进行下载对应的版本。\n比如：\nwget https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize%2Fv3.8.7/kustomize_v3.8.7_linux_amd64.tar.gz\rtar xf kustomize_v3.8.7_linux_amd64.tar.gz\rcp kustomize/kustomize /usr/local/bin\r这样就完成了简单的安装了。\n实践测试 背景   版本信息\n   kubernetes：1.17.9    集群信息，由于在一个环境中进行测试，所以采用不同的namespace进行分开\n   开发环境：dev 预发环境：stag 生产环境：prod    测试用例：一个简单的hello world示例\n  创建基础模板 首先创建一个helloworld目录，表示应用，然后在里面创建一个base目录，如下：\nmkdir helloworld/base -p\r然后在base目录下创建以下配置清单：\nbase/\r├── configMap.yaml\r├── deployment.yaml\r├── ingress.yaml\r├── kustomization.yaml\r└── service.yaml\r他们的清单内容分别如下：\nconfigMap.yaml\napiVersion: v1\rkind: ConfigMap\rmetadata:\rname: the-map\rnamespace: default\rdata:\raltGreeting: \u0026quot;Hello World!\u0026quot;\renableRisky: \u0026quot;false\u0026quot;\rdeployment.yaml\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: the-deployment\rnamespace: default\rspec:\rreplicas: 3\rselector:\rmatchLabels:\rdeployment: hello\rtemplate:\rmetadata:\rlabels:\rdeployment: hello\rspec:\rcontainers:\r- name: the-container\rimage: monopole/hello:1\rcommand: [\u0026quot;/hello\u0026quot;,\r\u0026quot;--port=8080\u0026quot;,\r\u0026quot;--enableRiskyFeature=$(ENABLE_RISKY)\u0026quot;]\rports:\r- containerPort: 8080\renv:\r- name: ALT_GREETING\rvalueFrom:\rconfigMapKeyRef:\rname: the-map\rkey: altGreeting\r- name: ENABLE_RISKY\rvalueFrom:\rconfigMapKeyRef:\rname: the-map\rkey: enableRisky\rservice.yaml\nkind: Service\rapiVersion: v1\rmetadata:\rname: the-service\rnamespace: default\rspec:\rselector:\rdeployment: hello\rtype: NodePort\rports:\r- protocol: TCP\rport: 8080\rtargetPort: 8080\ringress.yaml\napiVersion: extensions/v1beta1\rkind: Ingress\rmetadata:\rname: the-ingress namespace: default\rspec:\rrules:\r- host: test.coolops.cn http:\rpaths:\r- backend:\rserviceName: the-service servicePort: 8080 path: /\rkustomization.yaml\n# Example configuration for the webserver\r# at https://github.com/monopole/hello\rcommonLabels:\rapp: hello\rresources:\r- deployment.yaml\r- service.yaml\r- configMap.yaml\r- ingress.yaml\r这样基础模板就创建好了，我们可以使用如下命令将所有文件连在一起。\nkustomize build ../base\r然后如果想创建应用可以用以下方式。\n# 直接使用kubectl apply -k （集群版本要高于1.14）\rkubectl apply -k ../base/\r# 还可以通过kustomize命令\rkustomize build ../base | kubectl apply -f -\r删除应用命令类似，可以自行尝试。\n根据不同环境创建overlays 上面的是基础模板，所有的配置都是基于它，现在我们根据不同的环境进行定制。\n首先创建如下目录结构\n.\r├── base\r│ ├── configMap.yaml\r│ ├── deployment.yaml\r│ ├── ingress.yaml\r│ ├── kustomization.yaml\r│ └── service.yaml\r└── overlays\r├── dev\r├── prod\r└── stag\r其中：\n dev目录下存放开发环境定制清单 stag目录下存放预发环境定制清单 prod目录下存放生产环境定制清单  配置开发环境 在dev目录下创建以下文件：\n../dev/\r├── ingress.yaml\r├── kustomization.yaml\r└── map.yaml\r其中ingress.yaml如下：\napiVersion: extensions/v1beta1\rkind: Ingress\rmetadata:\rname: the-ingress namespace: default\rspec:\rrules:\r- host: hello-dev.coolops.cn http:\rpaths:\r- backend:\rserviceName: the-service servicePort: 8080 path: /\rmap.yaml\napiVersion: v1\rkind: ConfigMap\rmetadata:\rname: the-map\rdata:\raltGreeting: \u0026quot;Hello,This is Dev!\u0026quot;\renableRisky: \u0026quot;true\u0026quot;\rkustomization.yaml\nnamePrefix: dev-\rcommonLabels:\rorg: acmeCorporation\rvariant: dev\rcommonAnnotations:\rnote: Hello, This is dev!\rpatchesStrategicMerge:\r- map.yaml\r- ingress.yaml\rapiVersion: kustomize.config.k8s.io/v1beta1\rkind: Kustomization\rresources:\r- ../../base\rnamespace: dev\r开发环境更改了configmap的内容、ingress的host，还有namespace。\n然后可以通过kustomize build .测试配置是否正确。\n配置预发环境 在stag目录下创建以下文件：\n../stag/\r├── kustomization.yaml\r└── map.yaml\r其中map.yaml内容如下：\napiVersion: v1\rkind: ConfigMap\rmetadata:\rname: the-map\rdata:\raltGreeting: \u0026quot;Hello,This is Stag!\u0026quot;\renableRisky: \u0026quot;true\u0026quot;\rkustomization.yaml\nnamePrefix: stag-\rcommonLabels:\rorg: acmeCorporation\rvariant: stag\rcommonAnnotations:\rnote: Hello, This is stag!\rpatchesStrategicMerge:\r- map.yaml\rapiVersion: kustomize.config.k8s.io/v1beta1\rkind: Kustomization\rresources:\r- ../../base\rnamespace: stag\r预发环境更改了configmap和namespace。\n配置生产环境 在prod目录下创建以下文件：\n../prod/\r├── deployment.yaml\r├── kustomization.yaml\r└── map.yaml\r其中deployment.yaml配置如下：\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: the-deployment\rspec:\rreplicas: 3\rmap.yaml\napiVersion: v1\rkind: ConfigMap\rmetadata:\rname: the-map\rdata:\raltGreeting: \u0026quot;Hello,This is prod!\u0026quot;\renableRisky: \u0026quot;true\u0026quot;\rkustomization.yaml\nnamePrefix: prod-\rcommonLabels:\rorg: acmeCorporation\rvariant: prod\rcommonAnnotations:\rnote: Hello, This is prod!\rpatchesStrategicMerge:\r- deployment.yaml\r- map.yaml\rapiVersion: kustomize.config.k8s.io/v1beta1\rkind: Kustomization\rresources:\r- ../../base\rnamespace: prod\r生产环境更改了configmap、deploy副本数、namspace。\n发布使用 上面我们已经将整个需要的配置定制好了。现在就可以进行发布了。\n如果要发布开发环境，则使用：\ncd helloworld/\rkustomize build overlays/dev/ | kubectl apply -f -\r然后我们可以看到发布完成，如下：\n# kubectl get all -n dev\rNAME READY STATUS RESTARTS AGE\rpod/dev-the-deployment-6cdcbbc878-27n5g 1/1 Running 0 50s\rpod/dev-the-deployment-6cdcbbc878-fgx89 1/1 Running 0 50s\rpod/dev-the-deployment-6cdcbbc878-xz5q2 1/1 Running 0 50s\rNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rservice/dev-the-service NodePort 10.103.77.190 \u0026lt;none\u0026gt; 8080:32414/TCP 50s\rNAME READY UP-TO-DATE AVAILABLE AGE\rdeployment.apps/dev-the-deployment 3/3 3 3 50s\rNAME DESIRED CURRENT READY AGE\rreplicaset.apps/dev-the-deployment-6cdcbbc878 3 3 3 50s\r然后通过域名访问如下：\n其他环境是类似的操作，这里不再赘述。\n结合CD使用 在进行持续部署的时候每次都需要修改镜像地址为最新的版本，使用kustomize也可以简单的实现。\n加入我们要修改dev环境下的镜像地址为nginx，命令如下\ncd overlays/dev\rkustomize edit set image monopole/hello=nginx:latest\r说明：\n monopole/hello是原来的镜像名字 nginx:latest是新的镜像+标签  然后可以看到kustomization.yaml下的镜像地址已经变成了nginx，如下：\nnamePrefix: dev-\rcommonLabels:\rorg: acmeCorporation\rvariant: dev\rcommonAnnotations:\rnote: Hello, This is dev!\rpatchesStrategicMerge:\r- map.yaml\r- ingress.yaml\rapiVersion: kustomize.config.k8s.io/v1beta1\rkind: Kustomization\rresources:\r- ../../base\rnamespace: dev\rimages:\r- name: monopole/hello\rnewName: nginx\rnewTag: latest\r我们这时候再发布，就是nginx的镜像了。\n# kustomize build . | kubectl apply -f -\rconfigmap/dev-the-map unchanged\rservice/dev-the-service unchanged\rdeployment.apps/dev-the-deployment configured\ringress.extensions/dev-the-ingress unchanged\r同样，修改namespace可以使用如下命令。\nkustomize edit set namespace test\r更多操作可以查看官方文档：https://kubernetes-sigs.github.io/kustomize/zh/guides/\n写在最后 使用 Kustomize 简化了针对不同环境的应用程序配置的管理。我们将一组几乎重复的 YAML 文件重组为一个分层模型，这将减少错误，减少手动配置，并使代码更易于识别和维护。\n参考文档  https://kubernetes-sigs.github.io/kustomize/zh/guides/ https://github.com/kubernetes-sigs/kustomize https://kubernetes.io/zh/docs/tasks/manage-kubernetes-objects/kustomization/#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B  ","description":"本文主要介绍神马是kustomize，如何用其管理K8S的yaml清单","id":26,"section":"posts","tags":["kubernetes","kustomize"],"title":"使用Kustomize管理K8S的yaml清单","uri":"https://www.coolops.cn/posts/kubernetes-kustomize/"},{"content":"Kubedog 是一个开源的 Golang 项目，使用 watch 方式对 Kubernetes 资源进行跟踪，能够方便的用于日常运维和 CI/CD 过程之中，项目中除了一个 CLI 小工具之外，还提供了一组 SDK，用户可以将其中的 Watch 功能集成到自己的系统之中。安装过程非常简单，在项目网页直接下载即可。\n源码地址：https://github.com/werf/kubedog.git\nkubedog主要使用以下三种方式进行资源跟踪：\n follow rollout multitrack  分别对应三个命令：\n kubedog follow kubedog rollout track kubedog multitrack  rollout track 在 Kubernetes 上运行应用时，通常的做法是使用 kubectl apply 提交 YAML 之后，使用 kubectl get -w 或者 watch kubectl get 之类的命令等待 Pod 启动。如果启动成功，则进行测试等后续动作；如果启动失败，就需要用 kubectl logs、kubectl describe 等命令来查看失败原因。kubedog 能在一定程度上简化这一过程。\n例如使用 kubectl run 命令创建一个新的 Deployment 资源，并使用 kubedog 跟进创建进程：\n$ kubectl run nginx --image=nginx22\r...\rdeployment.apps/nginx created\r$ kubedog rollout track deployment nginx\r# deploy/nginx added\r# deploy/nginx rs/nginx-6cc78cbf64 added\r# deploy/nginx po/nginx-6cc78cbf64-8pnjz added\r# deploy/nginx po/nginx-6cc78cbf64-8pnjz nginx error: ImagePullBackOff: Back-off pulling image \u0026quot;nginx22\u0026quot;\rdeploy/nginx po/nginx-6cc78cbf64-8pnjz nginx failed: ImagePullBackOff: Back-off pulling image \u0026quot;nginx22\u0026quot;\r$ echo $?\r130\r很方便的看出，运行失败的状态及其原因，并且可以使用返回码来进行判断，方便在 Pipeline 中的运行。接下来可以使用 kubectl edit 命令编辑 Deployment，修改正确的镜像名称。然后再次进行验证：\n$ kubectl edit deployment nginx\rdeployment.extensions/nginx edited\r$ kubedog rollout track deployment nginx\r# deploy/nginx added\r# deploy/nginx rs/nginx-dbddb74b8 added\r# deploy/nginx po/nginx-dbddb74b8-x4nkm added\r# deploy/nginx event: po/nginx-dbddb74b8-x4nkm Pulled: Successfully pulled image \u0026quot;nginx\u0026quot;\r# deploy/nginx event: po/nginx-dbddb74b8-x4nkm Created: Created container\r# deploy/nginx event: po/nginx-dbddb74b8-x4nkm Started: Started container\r# deploy/nginx event: ScalingReplicaSet: Scaled down replica set nginx-6cc78cbf64 to 0\r# deploy/nginx become READY\r$ echo $?\r0\r修改完成，重新运行 kubedog，会看到成功运行的情况，并且返回值也变成了 0。\nfollow follow 命令的功能和 kubetail 的功能有少量重叠，可以用 Deployment/Job/Daemonset 等为单位，查看其中所有 Pod 的日志，例如前面用的 Nginx，如果有访问的话，就会看到如下结果：\n$ kubedog follow deployment nginx\r# deploy/nginx appears to be ready\r# deploy/nginx rs/nginx-6cc78cbf64 added\r# deploy/nginx new rs/nginx-dbddb74b8 added\r# deploy/nginx rs/nginx-dbddb74b8(new) po/nginx-dbddb74b8-x4nkm added\r# deploy/nginx rs/nginx-6cc54845d9 added\r# deploy/nginx event: ScalingReplicaSet: Scaled up replica set nginx-6cc54845d9 to 1\r# deploy/nginx rs/nginx-6cc54845d9(new) po/nginx-6cc54845d9-nhlvs added\r# deploy/nginx event: po/nginx-6cc54845d9-nhlvs Pulling: pulling image \u0026quot;nginx:alpine\u0026quot;\r# deploy/nginx event: po/nginx-6cc54845d9-nhlvs Pulled: Successfully pulled image \u0026quot;nginx:alpine\u0026quot;\r# deploy/nginx event: po/nginx-6cc54845d9-nhlvs Created: Created container\r# deploy/nginx event: po/nginx-6cc54845d9-nhlvs Started: Started container\r# deploy/nginx event: ScalingReplicaSet: Scaled down replica set nginx-dbddb74b8 to 0\r# deploy/nginx become READY\r# deploy/nginx event: po/nginx-dbddb74b8-x4nkm Killing: Killing container with id docker://nginx:Need to kill Pod\r\u0026gt;\u0026gt; deploy/nginx rs/nginx-dbddb74b8 po/nginx-dbddb74b8-x4nkm nginx\r\u0026gt;\u0026gt; deploy/nginx rs/nginx-6cc54845d9(new) po/nginx-6cc54845d9-nhlvs nginx\r127.0.0.1 - - [02/Jun/2019:11:35:08 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 612 \u0026quot;-\u0026quot; \u0026quot;Wget\u0026quot; \u0026quot;-\u0026quot;\r127.0.0.1 - - [02/Jun/2019:11:35:11 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 612 \u0026quot;-\u0026quot; \u0026quot;Wget\u0026quot; \u0026quot;-\u0026quot;\rmultitrack 官方不建议使用follow和rollout模式，后续也不会更新；而推荐使用multitrack则直观、易读的方式来展示状态信息。\n要使用multitrack，您需要将JSON结构传递给kubedog的stdin，如下：\ncat \u0026lt;\u0026lt; EOF | kubedog multitrack\r{\r\u0026quot;Deployments\u0026quot;: [\r{\r\u0026quot;ResourceName\u0026quot;: \u0026quot;helloworld\u0026quot;,\r\u0026quot;Namespace\u0026quot;: \u0026quot;test\u0026quot;\r}\r],\r\u0026quot;Deployments\u0026quot;: [\r{\r\u0026quot;ResourceName\u0026quot;: \u0026quot;helloworld1\u0026quot;,\r\u0026quot;Namespace\u0026quot;: \u0026quot;test\u0026quot;\r}\r]\r}\rEOF\r或\recho '{\u0026quot;Deployments\u0026quot;: [{\u0026quot;ResourceName\u0026quot;: \u0026quot;helloworld\u0026quot;,\u0026quot;Namespace\u0026quot;: \u0026quot;test\u0026quot;}],\u0026quot;Deployments\u0026quot;: [{\u0026quot;ResourceName\u0026quot;: \u0026quot;helloworld1\u0026quot;,\u0026quot;Namespace\u0026quot;: \u0026quot;test\u0026quot;}]}' | kubedog multitrack\r我们在做CI/CD的时候，无论是通过Kubernetes CLI还是Kubernetes Continuous Deploy插件，在应用yaml后无法检查资源是否部署成功，只能通过kubectl手动检查，因此我们可以使用kubedog在pipeline完成这最后一步验证。\n","description":"本文主要介绍如何使用kubedog解决kubernetes中资源跟踪问题","id":27,"section":"posts","tags":["kubernetes","kubedog"],"title":"kubedog：解决K8S中资源跟踪问题","uri":"https://www.coolops.cn/posts/kubernetes-kubedog/"},{"content":"kubectl-debug是K8S中Pod的诊断工具，它通过启动一个排错工具容器，并将其加入到目标业务容器的pid, network, user 以及 ipc namespace 中，这时我们就可以在新容器中直接用 netstat, tcpdump 这些熟悉的工具来解决问题了, 而业务容器可以保持最小化, 不需要预装任何额外的排障工具。\n其主要有两部分组成：\n kubectl-debug工具，是一个二进制文件 debug-agent，部署在node节点，用于启动关联的排错容器  部署 1、部署kubectl-debug工具 由于其仅仅是一个二进制文件，所以部署很简单，不过由于集群是通过kubeconfig进行授权访问的，所以我们将kubectl-debug工具部署在master上。\n项目地址：https://github.com/aylei/kubectl-debug\n# install kubectl-debug\rexport PLUGIN_VERSION=0.1.1\r#linux x86_64\rcurl -Lo kubectl-debug.tar.gz https://github.com/aylei/kubectl-debug/releases/download/v${PLUGIN_VERSION}/kubectl-debug_${PLUGIN_VERSION}_linux_amd64.tar.gz\rtar -zxvf kubectl-debug.tar.gz kubectl-debug\rsudo mv kubectl-debug /usr/local/bin/\r2、部署debug-agent debug-agent以daemonSet的形式部署。直接执行下面命令即可\nkubectl apply -f https://raw.githubusercontent.com/aylei/kubectl-debug/master/scripts/agent_daemonset.yml\r然后查看pod的状态，直到其变为running\nkubectl get pod\rNAME READY STATUS RESTARTS AGE\rdebug-agent-5prws 1/1 Running 0 27d\rdebug-agent-5qm2w 1/1 Running 1 23d\rdebug-agent-dd7bt 1/1 Running 1 27d\rdebug-agent-nfzqp 1/1 Running 0 27d\rdebug-agent-wx4qp 1/1 Running 0 27d\r使用 1、pod里只有一个contaienr\nkubectl-debug POD_NAME\r2、pod里有多个container\nkubectl-debug POD_NAME -c CONTAINER_NAME\r3、指定namespace\nkubectl-debug POD_NAME -n test\r4、自定义排除的容器镜像\nkubectl-debug POD_NAME --image aylei/debug-jvm\r更多可以通过kubectl-debug -h来查看帮助文档。\n上面的debug-agent是部署在集群中的，这种方式对于调试频率不高的环境容易造成资源浪费，所以debug-agent还提供一种agentless的模式，kubectl-debug执行命令后，才创建agent pod和排错工具容器，并在退出后删除工具容器和agent pod。由于每次执行都要重新拉起agent，启动会比daemon-set模式稍慢。使用-a, \u0026ndash;agentless开启agentless模式。\n比如：\nkubectl-debug POD_NAME -n NAMESPACE -a --image aylei/debug-jvm --agent-image nicolaka/netshoot:latest\r","description":"本文简单介绍如何使用kubectl-debug诊断工具","id":28,"section":"posts","tags":["kubernetes","kubectl-debug"],"title":"使用kubectl-debug诊断kubernetes集群","uri":"https://www.coolops.cn/posts/kubernetes-kubectl-debug/"},{"content":"nginx-ingress-controller的日志\nnginx-ingress-controller的日志包括三个部分：\n controller日志： 输出到stdout，通过启动参数中的–log_dir可已配置输出到文件，重定向到文件后会自动轮转，但不会自动清理 accesslog：输出到stdout，通过nginx-configuration中的字段可以配置输出到哪个文件。输出到文件后不会自动轮转或清理 errorlog：输出到stderr，配置方式与accesslog类似。  给controller日志落盘\n 给nginx-ingress-controller挂一个hostpath： /data/log/nginx/ 映射到容器里的/var/log/nginx/ ， 给nginx-ingress-controller配置log-dir和logtostderr参数，将日志重定向到/var/log/nginx/中。  controller的日志需要做定时清理。由于controller的日志是通过klog(k8s.io/klog)输出的，会进行日志滚动，所以我们通过脚本定时清理一定时间之前的日志文件即可。\n给nginx日志落盘\n修改configmap： nginx-configuration。配置accesslog和errorlog的输出路径，替换默认的stdout和stderr。输出路径我们可以与controller一致，便于查找。\naccesslog和errorlog都只有一个日志文件，我们可以使用logrotate进行日志轮转，将输出到宿主机上的日志进行轮转和清理。配置如：\n$ cat /etc/logrotate.d/nginx.log\r/data/log/nginx/access.log {\rsu root list\rrotate 7\rdaily\rmaxsize 50M\rcopytruncate\rmissingok\rcreate 0644 www-data root\r}\r官方提供的模板中，nginx-ingress-controller默认都是以33这个用户登录启动容器的，因此挂载hostpath路径时存在权限问题。我们需要手动在机器上执行chown -R 33:33 /data/log/nginx.\n自动化\nnginx日志落盘中，第2、3两点均需要人工运维，有什么解决办法吗？\n问题的关键是：有什么办法可以在nginx-ingress-controller容器启动之前加一个hook，将宿主机的指定目录执行chown呢？\n可以用initContainer。initcontainer必须在containers中的容器运行前运行完毕并成功退出。再说第二点，我们注意到nginx-ingress-controller的基础镜像中就自带了logrotate，那么问题就简单了，我们将写好的logrotate配置文件以configmap的形式挂载到容器中就可以了。\n完整的yaml文件如下：\napiVersion: v1\rkind: Namespace\rmetadata:\rname: ingress-nginx\rlabels:\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\r---\r# Source: ingress-nginx/templates/controller-serviceaccount.yaml\rapiVersion: v1\rkind: ServiceAccount\rmetadata:\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rapp.kubernetes.io/component: controller\rname: ingress-nginx\rnamespace: ingress-nginx\r---\r# Source: ingress-nginx/templates/controller-configmap.yaml\rapiVersion: v1\rkind: ConfigMap\rmetadata:\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rapp.kubernetes.io/component: controller\rname: ingress-nginx-controller\rnamespace: ingress-nginx\rdata:\rclient_max_body_size: \u0026quot;100m\u0026quot;\rproxy_body_size: \u0026quot;100m\u0026quot;\raccess-log-path: /var/log/nginx/access.log\rerror-log-path: /var/log/nginx/erroes.log\r---\r# 创建一个configmap，配置nginx日志的轮转策略，对应的是nginx日志在容器内的日志文件\rapiVersion: v1\rdata:\rnginx.log: |\r/var/log/nginx/access.log {\rrotate 7 daily\rmaxsize 200M minsize 10M\rcopytruncate\rmissingok\rcreate 0644 root root\r}\r/var/log/nginx/error.log {\rrotate 7 daily\rmaxsize 200M minsize 10M\rcopytruncate\rmissingok\rcreate 0644 root root\r}\rkind: ConfigMap\rmetadata:\rname: nginx-ingress-logrotate\rnamespace: ingress-nginx\r---\r# Source: ingress-nginx/templates/clusterrole.yaml\rapiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRole\rmetadata:\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rname: ingress-nginx\rnamespace: ingress-nginx\rrules:\r- apiGroups:\r- ''\rresources:\r- configmaps\r- endpoints\r- nodes\r- pods\r- secrets\rverbs:\r- list\r- watch\r- apiGroups:\r- ''\rresources:\r- nodes\rverbs:\r- get\r- apiGroups:\r- ''\rresources:\r- services\rverbs:\r- get\r- list\r- update\r- watch\r- apiGroups:\r- extensions\r- networking.k8s.io # k8s 1.14+\rresources:\r- ingresses\rverbs:\r- get\r- list\r- watch\r- apiGroups:\r- ''\rresources:\r- events\rverbs:\r- create\r- patch\r- apiGroups:\r- extensions\r- networking.k8s.io # k8s 1.14+\rresources:\r- ingresses/status\rverbs:\r- update\r- apiGroups:\r- networking.k8s.io # k8s 1.14+\rresources:\r- ingressclasses\rverbs:\r- get\r- list\r- watch\r---\r# Source: ingress-nginx/templates/clusterrolebinding.yaml\rapiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRoleBinding\rmetadata:\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rname: ingress-nginx\rnamespace: ingress-nginx\rroleRef:\rapiGroup: rbac.authorization.k8s.io\rkind: ClusterRole\rname: ingress-nginx\rsubjects:\r- kind: ServiceAccount\rname: ingress-nginx\rnamespace: ingress-nginx\r---\r# Source: ingress-nginx/templates/controller-role.yaml\rapiVersion: rbac.authorization.k8s.io/v1\rkind: Role\rmetadata:\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rapp.kubernetes.io/component: controller\rname: ingress-nginx\rnamespace: ingress-nginx\rrules:\r- apiGroups:\r- ''\rresources:\r- namespaces\rverbs:\r- get\r- apiGroups:\r- ''\rresources:\r- configmaps\r- pods\r- secrets\r- endpoints\rverbs:\r- get\r- list\r- watch\r- apiGroups:\r- ''\rresources:\r- services\rverbs:\r- get\r- list\r- update\r- watch\r- apiGroups:\r- extensions\r- networking.k8s.io # k8s 1.14+\rresources:\r- ingresses\rverbs:\r- get\r- list\r- watch\r- apiGroups:\r- extensions\r- networking.k8s.io # k8s 1.14+\rresources:\r- ingresses/status\rverbs:\r- update\r- apiGroups:\r- networking.k8s.io # k8s 1.14+\rresources:\r- ingressclasses\rverbs:\r- get\r- list\r- watch\r- apiGroups:\r- ''\rresources:\r- configmaps\rresourceNames:\r- ingress-controller-leader-nginx\rverbs:\r- get\r- update\r- apiGroups:\r- ''\rresources:\r- configmaps\rverbs:\r- create\r- apiGroups:\r- ''\rresources:\r- endpoints\rverbs:\r- create\r- get\r- update\r- apiGroups:\r- ''\rresources:\r- events\rverbs:\r- create\r- patch\r---\r# Source: ingress-nginx/templates/controller-rolebinding.yaml\rapiVersion: rbac.authorization.k8s.io/v1\rkind: RoleBinding\rmetadata:\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rapp.kubernetes.io/component: controller\rname: ingress-nginx\rnamespace: ingress-nginx\rroleRef:\rapiGroup: rbac.authorization.k8s.io\rkind: Role\rname: ingress-nginx\rsubjects:\r- kind: ServiceAccount\rname: ingress-nginx\rnamespace: ingress-nginx\r---\r# Source: ingress-nginx/templates/controller-service-webhook.yaml\rapiVersion: v1\rkind: Service\rmetadata:\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rapp.kubernetes.io/component: controller\rname: ingress-nginx-controller-admission\rnamespace: ingress-nginx\rspec:\rtype: ClusterIP\rports:\r- name: https-webhook\rport: 443\rtargetPort: webhook\rselector:\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/component: controller\r---\r# Source: ingress-nginx/templates/controller-service.yaml\rapiVersion: v1\rkind: Service\rmetadata:\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rapp.kubernetes.io/component: controller\rname: ingress-nginx-controller\rnamespace: ingress-nginx\rspec:\rtype: LoadBalancer\rexternalTrafficPolicy: Local\rports:\r- name: http\rport: 80\rprotocol: TCP\rtargetPort: http\r- name: https\rport: 443\rprotocol: TCP\rtargetPort: https\rselector:\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/component: controller\r---\r# Source: ingress-nginx/templates/controller-deployment.yaml\rapiVersion: apps/v1\rkind: Deployment\rmetadata:\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rapp.kubernetes.io/component: controller\rname: ingress-nginx-controller\rnamespace: ingress-nginx\rspec:\rselector:\rmatchLabels:\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/component: controller\rrevisionHistoryLimit: 10\rminReadySeconds: 0\rreplicas: 1\rtemplate:\rmetadata:\rlabels:\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/component: controller\rspec:\rdnsPolicy: ClusterFirst\rhostNetwork: true\rtolerations:\r- operator: \u0026quot;Exists\u0026quot;\rnodeSelector:\rkubernetes.io/hostname: k8s-master-134\rinitContainers:\r- name: adddirperm\rimage: busybox\rcommand:\r- /bin/sh\r- -c\r- chown -R ${USER_ID}:${USER_ID} ${LOG_DIR} env:\r- name: LOG_DIR\rvalue: /var/log/nginx\r- name: USER_ID\rvalue: \u0026quot;101\u0026quot;\rvolumeMounts:\r- name: logdir\rmountPath: /var/log/nginx\rcontainers:\r- name: controller\rimage: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.32.0\rimagePullPolicy: IfNotPresent\rlifecycle:\rpreStop:\rexec:\rcommand:\r- /wait-shutdown\rargs:\r- /nginx-ingress-controller\r- --publish-service=ingress-nginx/ingress-nginx-controller\r- --election-id=ingress-controller-leader\r- --ingress-class=nginx\r- --configmap=ingress-nginx/ingress-nginx-controller\r- --validating-webhook=:8443\r- --validating-webhook-certificate=/usr/local/certificates/cert\r- --validating-webhook-key=/usr/local/certificates/key\r- --log_dir=/var/log/nginx\r- --logtostderr=false\rsecurityContext:\rcapabilities:\rdrop:\r- ALL\radd:\r- NET_BIND_SERVICE\rrunAsUser: 101\rallowPrivilegeEscalation: true\renv:\r- name: POD_NAME\rvalueFrom:\rfieldRef:\rfieldPath: metadata.name\r- name: POD_NAMESPACE\rvalueFrom:\rfieldRef:\rfieldPath: metadata.namespace\rlivenessProbe:\rhttpGet:\rpath: /healthz\rport: 10254\rscheme: HTTP\rinitialDelaySeconds: 10\rperiodSeconds: 10\rtimeoutSeconds: 1\rsuccessThreshold: 1\rfailureThreshold: 3\rreadinessProbe:\rhttpGet:\rpath: /healthz\rport: 10254\rscheme: HTTP\rinitialDelaySeconds: 10\rperiodSeconds: 10\rtimeoutSeconds: 1\rsuccessThreshold: 1\rfailureThreshold: 3\rports:\r- name: http\rcontainerPort: 80\rprotocol: TCP\r- name: https\rcontainerPort: 443\rprotocol: TCP\r- name: webhook\rcontainerPort: 8443\rprotocol: TCP\rvolumeMounts:\r- name: webhook-cert\rmountPath: /usr/local/certificates/\rreadOnly: true\r- name: logdir\rmountPath: /var/log/nginx\r- name: logrotateconf\rmountPath: /etc/logrotate.d/nginx.log\rsubPath: nginx.log\rresources:\rrequests:\rcpu: 100m\rmemory: 90Mi\rserviceAccountName: ingress-nginx\rterminationGracePeriodSeconds: 300\rvolumes:\r- name: webhook-cert\rsecret:\rsecretName: ingress-nginx-admission\r- name: logdir\rhostPath:\rpath: /var/log/nginx\rtype: DirectoryOrCreate\r- name: logrotateconf\rconfigMap:\rname: nginx-ingress-logrotate\ritems:\r- key: nginx.log\rpath: nginx.log\r---\r# Source: ingress-nginx/templates/admission-webhooks/validating-webhook.yaml\rapiVersion: admissionregistration.k8s.io/v1beta1\rkind: ValidatingWebhookConfiguration\rmetadata:\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rapp.kubernetes.io/component: admission-webhook\rname: ingress-nginx-admission\rnamespace: ingress-nginx\rwebhooks:\r- name: validate.nginx.ingress.kubernetes.io\rrules:\r- apiGroups:\r- extensions\r- networking.k8s.io\rapiVersions:\r- v1beta1\roperations:\r- CREATE\r- UPDATE\rresources:\r- ingresses\rfailurePolicy: Fail\rclientConfig:\rservice:\rnamespace: ingress-nginx\rname: ingress-nginx-controller-admission\rpath: /extensions/v1beta1/ingresses\r---\r# Source: ingress-nginx/templates/admission-webhooks/job-patch/clusterrole.yaml\rapiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRole\rmetadata:\rname: ingress-nginx-admission\rannotations:\rhelm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade\rhelm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rapp.kubernetes.io/component: admission-webhook\rnamespace: ingress-nginx\rrules:\r- apiGroups:\r- admissionregistration.k8s.io\rresources:\r- validatingwebhookconfigurations\rverbs:\r- get\r- update\r---\r# Source: ingress-nginx/templates/admission-webhooks/job-patch/clusterrolebinding.yaml\rapiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRoleBinding\rmetadata:\rname: ingress-nginx-admission\rannotations:\rhelm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade\rhelm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rapp.kubernetes.io/component: admission-webhook\rnamespace: ingress-nginx\rroleRef:\rapiGroup: rbac.authorization.k8s.io\rkind: ClusterRole\rname: ingress-nginx-admission\rsubjects:\r- kind: ServiceAccount\rname: ingress-nginx-admission\rnamespace: ingress-nginx\r---\r# Source: ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml\rapiVersion: batch/v1\rkind: Job\rmetadata:\rname: ingress-nginx-admission-create\rannotations:\rhelm.sh/hook: pre-install,pre-upgrade\rhelm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rapp.kubernetes.io/component: admission-webhook\rnamespace: ingress-nginx\rspec:\rtemplate:\rmetadata:\rname: ingress-nginx-admission-create\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rapp.kubernetes.io/component: admission-webhook\rspec:\rcontainers:\r- name: create\rimage: jettech/kube-webhook-certgen:v1.2.0\rimagePullPolicy: IfNotPresent\rargs:\r- create\r- --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.ingress-nginx.svc\r- --namespace=ingress-nginx\r- --secret-name=ingress-nginx-admission\rrestartPolicy: OnFailure\rserviceAccountName: ingress-nginx-admission\rsecurityContext:\rrunAsNonRoot: true\rrunAsUser: 2000\r---\r# Source: ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml\rapiVersion: batch/v1\rkind: Job\rmetadata:\rname: ingress-nginx-admission-patch\rannotations:\rhelm.sh/hook: post-install,post-upgrade\rhelm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rapp.kubernetes.io/component: admission-webhook\rnamespace: ingress-nginx\rspec:\rtemplate:\rmetadata:\rname: ingress-nginx-admission-patch\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rapp.kubernetes.io/component: admission-webhook\rspec:\rcontainers:\r- name: patch\rimage: jettech/kube-webhook-certgen:v1.2.0\rimagePullPolicy:\rargs:\r- patch\r- --webhook-name=ingress-nginx-admission\r- --namespace=ingress-nginx\r- --patch-mutating=false\r- --secret-name=ingress-nginx-admission\r- --patch-failure-policy=Fail\rrestartPolicy: OnFailure\rserviceAccountName: ingress-nginx-admission\rsecurityContext:\rrunAsNonRoot: true\rrunAsUser: 2000\r---\r# Source: ingress-nginx/templates/admission-webhooks/job-patch/role.yaml\rapiVersion: rbac.authorization.k8s.io/v1\rkind: Role\rmetadata:\rname: ingress-nginx-admission\rannotations:\rhelm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade\rhelm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rapp.kubernetes.io/component: admission-webhook\rnamespace: ingress-nginx\rrules:\r- apiGroups:\r- ''\rresources:\r- secrets\rverbs:\r- get\r- create\r---\r# Source: ingress-nginx/templates/admission-webhooks/job-patch/rolebinding.yaml\rapiVersion: rbac.authorization.k8s.io/v1\rkind: RoleBinding\rmetadata:\rname: ingress-nginx-admission\rannotations:\rhelm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade\rhelm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rapp.kubernetes.io/component: admission-webhook\rnamespace: ingress-nginx\rroleRef:\rapiGroup: rbac.authorization.k8s.io\rkind: Role\rname: ingress-nginx-admission\rsubjects:\r- kind: ServiceAccount\rname: ingress-nginx-admission\rnamespace: ingress-nginx\r---\r# Source: ingress-nginx/templates/admission-webhooks/job-patch/serviceaccount.yaml\rapiVersion: v1\rkind: ServiceAccount\rmetadata:\rname: ingress-nginx-admission\rannotations:\rhelm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade\rhelm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\rlabels:\rhelm.sh/chart: ingress-nginx-2.0.3\rapp.kubernetes.io/name: ingress-nginx\rapp.kubernetes.io/instance: ingress-nginx\rapp.kubernetes.io/version: 0.32.0\rapp.kubernetes.io/managed-by: Helm\rapp.kubernetes.io/component: admission-webhook\rnamespace: ingress-nginx\r","description":"本文主要介绍如何对nginx ingress的日志进行落盘操作","id":29,"section":"posts","tags":["kubernetes","ingress-nginx"],"title":"如何给nginx-ingress进行日志落盘","uri":"https://www.coolops.cn/posts/kubernetes-update-nginx-ingress-log/"},{"content":"（1）、查看当前的证书时间\n# kubeadm alpha certs check-expiration\r[check-expiration] Reading configuration from the cluster...\r[check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'\rCERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED\radmin.conf Jun 20, 2021 11:21 UTC 364d no apiserver Jun 20, 2021 11:21 UTC 364d ca no apiserver-etcd-client Jun 20, 2021 11:21 UTC 364d etcd-ca no apiserver-kubelet-client Jun 20, 2021 11:21 UTC 364d ca no controller-manager.conf Jun 20, 2021 11:21 UTC 364d no etcd-healthcheck-client Jun 20, 2021 11:21 UTC 364d etcd-ca no etcd-peer Jun 20, 2021 11:21 UTC 364d etcd-ca no etcd-server Jun 20, 2021 11:21 UTC 364d etcd-ca no front-proxy-client Jun 20, 2021 11:21 UTC 364d front-proxy-ca no scheduler.conf Jun 20, 2021 11:21 UTC 364d no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED\rca Jun 18, 2030 11:21 UTC 9y no etcd-ca Jun 18, 2030 11:21 UTC 9y no front-proxy-ca Jun 18, 2030 11:21 UTC 9y no （2）、下载源码\ngit clone https://github.com/kubernetes/kubernetes.git\r（3）、切换到自己的版本，修改源码，比如我的是v1.17.2版本\ncd kubernetes\rgit checkout v1.17.2\rvim cmd/kubeadm/app/constants/constants.go，找到CertificateValidity，修改如下\n....\rconst (\r// KubernetesDir is the directory Kubernetes owns for storing various configuration files\rKubernetesDir = \u0026quot;/etc/kubernetes\u0026quot;\r// ManifestsSubDirName defines directory name to store manifests\rManifestsSubDirName = \u0026quot;manifests\u0026quot;\r// TempDirForKubeadm defines temporary directory for kubeadm\r// should be joined with KubernetesDir.\rTempDirForKubeadm = \u0026quot;tmp\u0026quot;\r// CertificateValidity defines the validity for all the signed certificates generated by kubeadm\rCertificateValidity = time.Hour * 24 * 365 * 100\r....\r（4）、编译kubeadm\nmake WHAT=cmd/kubeadm\r编译完生成如下目录和二进制文件\n# ll _output/bin/\rtotal 76172\r-rwxr-xr-x 1 root root 6799360 Jun 20 21:08 conversion-gen\r-rwxr-xr-x 1 root root 6778880 Jun 20 21:08 deepcopy-gen\r-rwxr-xr-x 1 root root 6750208 Jun 20 21:08 defaulter-gen\r-rwxr-xr-x 1 root root 4883629 Jun 20 21:08 go2make\r-rwxr-xr-x 1 root root 2109440 Jun 20 21:09 go-bindata\r-rwxr-xr-x 1 root root 39256064 Jun 20 21:11 kubeadm\r-rwxr-xr-x 1 root root 11419648 Jun 20 21:09 openapi-gen\r（5）、备份原kubeadm和证书文件\ncp /usr/bin/kubeadm{,.bak20200620}\rcp -r /etc/kubernetes/pki{,.bak20200620}\r（7）、将新生成的kubeadm进行替换\ncp _output/bin/kubeadm /usr/bin/kubeadm\r（8）、生成新的证书\ncd /etc/kubernetes/pki\rkubeadm alpha certs renew all\r输出如下\n[renew] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'\rcertificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed\rcertificate for serving the Kubernetes API renewed\rcertificate the apiserver uses to access etcd renewed\rcertificate for the API server to connect to kubelet renewed\rcertificate embedded in the kubeconfig file for the controller manager to use renewed\rcertificate for liveness probes to healthcheck etcd renewed\rcertificate for etcd nodes to communicate with each other renewed\rcertificate for serving etcd renewed\rcertificate for the front proxy client renewed\rcertificate embedded in the kubeconfig file for the scheduler manager to use renewed\r（9）、验证结果\nkubeadm alpha certs check-expiration\r输出如下\n[root@k8s-master pki]# kubeadm alpha certs check-expiration\r[check-expiration] Reading configuration from the cluster...\r[check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'\rCERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED\radmin.conf May 27, 2120 13:25 UTC 99y no apiserver May 27, 2120 13:25 UTC 99y ca no apiserver-etcd-client May 27, 2120 13:25 UTC 99y etcd-ca no apiserver-kubelet-client May 27, 2120 13:25 UTC 99y ca no controller-manager.conf May 27, 2120 13:25 UTC 99y no etcd-healthcheck-client May 27, 2120 13:25 UTC 99y etcd-ca no etcd-peer May 27, 2120 13:25 UTC 99y etcd-ca no etcd-server May 27, 2120 13:25 UTC 99y etcd-ca no front-proxy-client May 27, 2120 13:25 UTC 99y front-proxy-ca no scheduler.conf May 27, 2120 13:25 UTC 99y no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED\rca Jun 18, 2030 11:21 UTC 9y no etcd-ca Jun 18, 2030 11:21 UTC 9y no front-proxy-ca Jun 18, 2030 11:21 UTC 9y no 查看集群状态是否OK。\n[root@k8s-master pki]# kubectl get node\rNAME STATUS ROLES AGE VERSION\rk8s-master Ready master 127m v1.17.2\rk8s-node01 Ready \u0026lt;none\u0026gt; 94m v1.17.2\rk8s-node02 Ready \u0026lt;none\u0026gt; 95m v1.17.2\r[root@k8s-master pki]# kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE\rcalico-kube-controllers-589b5f594b-76vwr 1/1 Running 0 93m\rcalico-node-4qvfj 1/1 Running 0 93m\rcalico-node-cn79s 1/1 Running 0 93m\rcalico-node-sppn9 1/1 Running 0 93m\rcoredns-7f9c544f75-hc5q5 1/1 Running 0 127m\rcoredns-7f9c544f75-z77s8 1/1 Running 0 127m\retcd-k8s-master 1/1 Running 0 114m\rkube-apiserver-k8s-master 1/1 Running 0 115m\rkube-controller-manager-k8s-master 1/1 Running 0 114m\rkube-proxy-6kckk 1/1 Running 0 94m\rkube-proxy-r7mn2 1/1 Running 0 127m\rkube-proxy-zf48c 1/1 Running 0 95m\rkube-scheduler-k8s-master 1/1 Running 0 114m\r更新kubeconfig\nkubeadm init phase kubeconfig all --config kubeadm.yaml\r[kubeconfig] Using kubeconfig folder \u0026quot;/etc/kubernetes\u0026quot;\r[kubeconfig] Using existing kubeconfig file: \u0026quot;/etc/kubernetes/admin.conf\u0026quot;\r[kubeconfig] Using existing kubeconfig file: \u0026quot;/etc/kubernetes/kubelet.conf\u0026quot;\r[kubeconfig] Using existing kubeconfig file: \u0026quot;/etc/kubernetes/controller-manager.conf\u0026quot;\r[kubeconfig] Using existing kubeconfig file: \u0026quot;/etc/kubernetes/scheduler.conf\u0026quot;\r将新生成的 admin 配置文件覆盖掉原本的 admin 文件:\nmv $HOME/.kube/config $HOME/.kube/config.old\rcp -i /etc/kubernetes/admin.conf $HOME/.kube/config\rchown $(id -u):$(id -g) $HOME/.kube/config\r完成后重启 kube-apiserver、kube-controller、kube-scheduler、etcd 这4个容器即可，我们可以查看 apiserver 的证书的有效期来验证是否更新成功：\n$ echo | openssl s_client -showcerts -connect 127.0.0.1:6443 -servername api 2\u0026gt;/dev/null | openssl x509 -noout -enddate\rnotAfter=Aug 26 03:47:23 2021 GMT\r到此证书修改完成。\n 如果github上下载很慢的话可以到gitee上下载，地址：https://gitee.com/mirrors/Kubernetes/tree/master/\n 不过证书修改虽然完成了，但是kubelet的证书并没有更新，这时候我们可以开启证书自动轮转。\n（1）增加 kubelet 参数\n修改/usr/lib/systemd/system/kubelet.service\n--feature-gates=RotateKubeletServerCertificate=true\r（2）增加 controller-manager 参数\n修改controller-manager的yaml文件\n--experimental-cluster-signing-duration=87600h0m0s\r--feature-gates=RotateKubeletServerCertificate=true\r（3）创建 rbac 对象\n创建rbac对象，允许节点轮换kubelet server证书：\napiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRole\rmetadata:\rannotations:\rrbac.authorization.kubernetes.io/autoupdate: \u0026quot;true\u0026quot;\rlabels:\rkubernetes.io/bootstrapping: rbac-defaults\rname: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver\rrules:\r- apiGroups:\r- certificates.k8s.io\rresources:\r- certificatesigningrequests/selfnodeserver\rverbs:\r- create\r---\rapiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRoleBinding\rmetadata:\rname: kubeadm:node-autoapprove-certificate-server\rroleRef:\rapiGroup: rbac.authorization.k8s.io\rkind: ClusterRole\rname: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver\rsubjects:\r- apiGroup: rbac.authorization.k8s.io\rkind: Group\rname: system:nodes\r查看证书时间\nopenssl x509 -in ca.crt -noout -text | grep \u0026quot;Not\u0026quot;\r","description":"本文主要介绍如何修改kubeadm搭建集群的默认证书时间","id":30,"section":"posts","tags":["kubernetes","kubeadm"],"title":"修改kubeadm搭建集群的证书时间","uri":"https://www.coolops.cn/posts/kubernetes-update-tls-time/"},{"content":"kubeadm搭建的集群 修改之前节点信息如下：\n# kubectl get node\rNAME STATUS ROLES AGE VERSION\rk8s-master Ready master 95d v1.17.2\rk8s-node01 Ready node01 95d v1.17.2\rk8s-node02 Ready node02 95d v1.17.2\r（1）、修改主机名\nhostnamectl set-hostname k8s-node03\r（2）、删除node节点\nkubectl delete nodes k8s-node02\r（3）、在删除的Node节点上重置节点\nkubeadm reset\r（4）、在master节点上查看token是否存在（默认24小时过期）\nkubeadm token list\r（5）、如果token不存在则创建token\n# kubeadm token create\r4u4w7j.qv34axysi783i7wg\r（6）、获取ca证书sha256编码hash值\n# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'\rd0d8bc0728a15007638f3ff4f047b3ef8b6359fd0dc3cf409efe94147cb32e32\r（7）、在删除的Node节点执行kubeadm join加入集群\nkubeadm join 10.1.10.128:6443 --token cq7ufd.t05znkzrnrinosjn \\\r--discovery-token-ca-cert-hash sha256:b7f7676bf5af4ce251f96390d70e1158f01ddfaa2767f9734b03e238a5b6a798 \\\r--node-name k8s-node03\r（8）、在master上查看是否已经加入\n# kubectl get node\rNAME STATUS ROLES AGE VERSION\rk8s-master Ready master 95d v1.17.2\rk8s-node01 Ready node01 95d v1.17.2\rk8s-node03 Ready \u0026lt;none\u0026gt; 3m19s v1.17.2\r（9）、给node节点加标签（按需）\nkubectl label nodes k8s-node03 node-role.kubernetes.io/node03=\r（10）、查看集群Node信息\n# kubectl get node\rNAME STATUS ROLES AGE VERSION\rk8s-master Ready master 95d v1.17.2\rk8s-node01 Ready node01 95d v1.17.2\rk8s-node03 Ready node03 4m12s v1.17.2\r二进制搭建的集群  将host1改为master\n （1）、修改系统主机名\nhostnamectl set-hostname master\r（2）、修改kubelet启动参数\n--hostname-override=master\r重启kubelet服务\nsystemctl restart kubelet\r查看kubelet日志\njournalctl -xe -u kubelet\r会看到如下报错\nMar 23 13:15:27 master kubelet[13508]: E0323 13:15:27.320556 13508 kubelet_node_status.go:106] Unable to register node \u0026quot;master\u0026quot; with API server: nodes \u0026quot;master\u0026quot; is forbidden: node \u0026quot;host1\u0026quot; cannot modify node \u0026quot;master\u0026quot;\r（3）、停止kubelet服务并删除节点\nsystemctl stop kubelet\rkubectl delete node host1\r（4）、删除kubelet.kubeconfig,kubelet.key,kubelet.crt,kubelet-client.key和kubelet-client.crt\nrm -f /etc/kubernetes/kubelet.kubeconfig\rrm -f /etc/kubernetes/ssl/kubelet*\r（5）、重启kubelet\nystemctl restart kubelet\r（6）、查看证书状态\n# kubectl get csr\rNAME AGE REQUESTOR CONDITION\rnode-csr-GIAqC5LBI_7c6TlMW8wugv_TlHfs1CShZhnEyLgxvSI 1m kubelet-bootstrap Pending\r（7）、允许证书\nkubectl certificate approve node-csr-GIAqC5LBI_7c6TlMW8wugv_TlHfs1CShZhnEyLgxvSI\r（8）、再次查看证书状态\n# kubectl get csr\rNAME AGE REQUESTOR CONDITION\rnode-csr-GIAqC5LBI_7c6TlMW8wugv_TlHfs1CShZhnEyLgxvSI 1m kubelet-bootstrap Approved,Issued\r（9）、查看节点状态\nkubectl get node\r","description":"本文主要介绍如何为kubernetes集群中的node更改名字。","id":31,"section":"posts","tags":["kubernetes"],"title":"修改kubernetes集群中Node的名字","uri":"https://www.coolops.cn/posts/kubernetes-update-node-name/"},{"content":"Nginx作为WEB服务器被广泛使用。其自身支持热更新，在修改配置文件后，使用nginx -s reload命令可以不停服务重新加载配置。然而对于Dockerize的Nginx来说，如果每次都进到容器里执行对应命令去实现配置重载，这个过程是很痛苦的。本文介绍了一种kubernetes集群下nginx的热更新方案。\n首先我们创建正常的一个nginx资源，资源清单如下：\napiVersion: v1\rkind: ConfigMap\rmetadata:\rname: nginx-config\rdata:\rdefault.conf: |-\rserver {\rserver_name localhost;\rlisten 80 default_server;\rlocation = /healthz {\radd_header Content-Type text/plain;\rreturn 200 'ok';\r}\rlocation / {\rroot /usr/share/nginx/html;\rindex index.html index.htm;\r}\rerror_page 500 502 503 504 /50x.html;\rlocation = /50x.html {\rroot /usr/share/nginx/html;\r}\r}\r---\rapiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: my-app\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: my-app\rtemplate:\rmetadata:\rlabels:\rapp: my-app\rspec:\rcontainers:\r- name: my-app\rimage: nginx\rimagePullPolicy: IfNotPresent\rvolumeMounts:\r- name: nginx-config\rmountPath: /etc/nginx/conf.d\rvolumes:\r- name: nginx-config\rconfigMap:\rname: nginx-config\r然后创建资源对象。\n# kubectl get pod -o wide\rNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES\rmy-app-9bdd6cbbc-x9gnt 1/1 Running 0 112s 192.168.58.197 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\r然后我们访问pod资源，如下：\n# curl -I 192.168.58.197\rHTTP/1.1 200 OK\rServer: nginx/1.17.10\rDate: Tue, 26 May 2020 06:18:18 GMT\rContent-Type: text/html\rContent-Length: 612\rLast-Modified: Tue, 14 Apr 2020 14:19:26 GMT\rConnection: keep-alive\rETag: \u0026quot;5e95c66e-264\u0026quot;\rAccept-Ranges: bytes\r现在我们来更新一下ConfigMap，也就是更改配置文件如下：\napiVersion: v1\rkind: ConfigMap\rmetadata:\rname: nginx-config\rdata:\rdefault.conf: |-\rserver {\rserver_name localhost;\rlisten 8080 default_server;\rlocation = /healthz {\radd_header Content-Type text/plain;\rreturn 200 'ok';\r}\rlocation / {\rroot /usr/share/nginx/html;\rindex index.html index.htm;\r}\rerror_page 500 502 503 504 /50x.html;\rlocation = /50x.html {\rroot /usr/share/nginx/html;\r}\r}\r等待数秒\u0026hellip;..\n然后我们可以看到nginx pod里的配置信息已经更改为如下：\n# kubectl exec -it my-app-9bdd6cbbc-x9gnt -- /bin/bash\rroot@my-app-9bdd6cbbc-x9gnt:/# cat /etc/nginx/conf.d/default.conf server {\rserver_name localhost;\rlisten 8080 default_server;\rlocation = /healthz {\radd_header Content-Type text/plain;\rreturn 200 'ok';\r}\rlocation / {\rroot /usr/share/nginx/html;\rindex index.html index.htm;\r}\rerror_page 500 502 503 504 /50x.html;\rlocation = /50x.html {\rroot /usr/share/nginx/html;\r}\r}\rroot@my-app-9bdd6cbbc-x9gnt:/# 这时候我们访问8080是不通的，访问80是没问题的，如下：\n[root@k8s-master nginx]# curl -I 192.168.58.197\rHTTP/1.1 200 OK\rServer: nginx/1.17.10\rDate: Tue, 26 May 2020 06:21:05 GMT\rContent-Type: text/html\rContent-Length: 612\rLast-Modified: Tue, 14 Apr 2020 14:19:26 GMT\rConnection: keep-alive\rETag: \u0026quot;5e95c66e-264\u0026quot;\rAccept-Ranges: bytes\r[root@k8s-master nginx]# curl -I 192.168.58.197:8080\rcurl: (7) Failed connect to 192.168.58.197:8080; Connection refused\r我们可以看到，我们需要的配置文件已经更新的，但是并没有使用上，pod里的nginx也没有重载配置文件，这时候如果我们重新部署Pod，资源对象肯定就生效了。\n但是这并不我们想要的效果，我们希望配置文件更改了，服务也跟着reload，并不需要我们手动的去干预。\n目前有三种方法：\n 应用本身可以检测配置文件，然后自动reload 给Pod增加一个sidecar，用它来检测配置文件 第三方组件reloader，在deployment的annotations增加字段reloader.stakater.com/auto: \u0026quot;true\u0026quot;,即可检测configmap的更改来重启pod  应用本身检测的话这里就不做介绍了。这里主要来实验一下第2，3种方法\n一、以sidecar形式 1.1、方法  Kubernetes集群中部署Nginx Pod。该Pod包含两个Container，一个是nginx container，实现nginx自身的功能；另一个是nginx-reloader container，负责实时监测目标configmap的变化，当发现configmap更新以后，会主动向nginx的master进程发送HUP信号，实现配置的热加载。 配置文件是通过ConfigMap的形式挂载到Nginx Pod上，两个Container共享该ConfigMap。 依赖K8s集群的shareProcessNamespace特性（版本需在1.12之后），两个Container需要在Pod中共享进程名字空间。  1.2、实现 1.2.1、镜像制作 （1）、主容器使用官方NG容器即可\n（2）、sidecar容器制作\nDockerfile如下：\nFROM golang:1.12.0 as build\rRUN go get github.com/fsnotify/fsnotify\rRUN go get github.com/shirou/gopsutil/process\rRUN mkdir -p /go/src/app\rADD main.go /go/src/app/\rWORKDIR /go/src/app\rRUN CGO_ENABLED=0 GOOS=linux go build -a -o nginx-reloader .\r# main image\rFROM nginx:1.14.2-alpine\rCOPY --from=build /go/src/app/nginx-reloader /\rCMD [\u0026quot;/nginx-reloader\u0026quot;]\rmain.go脚本如下：\npackage main\rimport (\r\u0026quot;log\u0026quot;\r\u0026quot;os\u0026quot;\r\u0026quot;path/filepath\u0026quot;\r\u0026quot;syscall\u0026quot;\r\u0026quot;github.com/fsnotify/fsnotify\u0026quot;\rproc \u0026quot;github.com/shirou/gopsutil/process\u0026quot;\r)\rconst (\rnginxProcessName = \u0026quot;nginx\u0026quot;\rdefaultNginxConfPath = \u0026quot;/etc/nginx\u0026quot;\rwatchPathEnvVarName = \u0026quot;WATCH_NGINX_CONF_PATH\u0026quot;\r)\rvar stderrLogger = log.New(os.Stderr, \u0026quot;error: \u0026quot;, log.Lshortfile)\rvar stdoutLogger = log.New(os.Stdout, \u0026quot;\u0026quot;, log.Lshortfile)\rfunc getMasterNginxPid() (int, error) {\rprocesses, processesErr := proc.Processes()\rif processesErr != nil {\rreturn 0, processesErr\r}\rnginxProcesses := map[int32]int32{}\rfor _, process := range processes {\rprocessName, processNameErr := process.Name()\rif processNameErr != nil {\rreturn 0, processNameErr\r}\rif processName == nginxProcessName {\rppid, ppidErr := process.Ppid()\rif ppidErr != nil {\rreturn 0, ppidErr\r}\rnginxProcesses[process.Pid] = ppid\r}\r}\rvar masterNginxPid int32\rfor pid, ppid := range nginxProcesses {\rif ppid == 0 {\rmasterNginxPid = pid\rbreak\r}\r}\rstdoutLogger.Println(\u0026quot;found master nginx pid:\u0026quot;, masterNginxPid)\rreturn int(masterNginxPid), nil\r}\rfunc signalNginxReload(pid int) error {\rstdoutLogger.Printf(\u0026quot;signaling master nginx process (pid: %d) -\u0026gt; SIGHUP\\n\u0026quot;, pid)\rnginxProcess, nginxProcessErr := os.FindProcess(pid)\rif nginxProcessErr != nil {\rreturn nginxProcessErr\r}\rreturn nginxProcess.Signal(syscall.SIGHUP)\r}\rfunc main() {\rwatcher, watcherErr := fsnotify.NewWatcher()\rif watcherErr != nil {\rstderrLogger.Fatal(watcherErr)\r}\rdefer watcher.Close()\rdone := make(chan bool)\rgo func() {\rfor {\rselect {\rcase event, ok := \u0026lt;-watcher.Events:\rif !ok {\rreturn\r}\rif event.Op\u0026amp;fsnotify.Create == fsnotify.Create {\rif filepath.Base(event.Name) == \u0026quot;..data\u0026quot; {\rstdoutLogger.Println(\u0026quot;config map updated\u0026quot;)\rnginxPid, nginxPidErr := getMasterNginxPid()\rif nginxPidErr != nil {\rstderrLogger.Printf(\u0026quot;getting master nginx pid failed: %s\u0026quot;, nginxPidErr.Error())\rcontinue\r}\rif err := signalNginxReload(nginxPid); err != nil {\rstderrLogger.Printf(\u0026quot;signaling master nginx process failed: %s\u0026quot;, err)\r}\r}\r}\rcase err, ok := \u0026lt;-watcher.Errors:\rif !ok {\rreturn\r}\rstderrLogger.Printf(\u0026quot;received watcher.Error: %s\u0026quot;, err)\r}\r}\r}()\rpathToWatch, ok := os.LookupEnv(watchPathEnvVarName)\rif !ok {\rpathToWatch = defaultNginxConfPath\r}\rstdoutLogger.Printf(\u0026quot;adding path: `%s` to watch\\n\u0026quot;, pathToWatch)\rif err := watcher.Add(pathToWatch); err != nil {\rstderrLogger.Fatal(err)\r}\r\u0026lt;-done\r}\r1.2.2、部署NG （1）、NG的配置以configMap进行部署：\nnginx-config.yaml\n// nginx-config.yaml\rapiVersion: v1\rkind: ConfigMap\rmetadata:\rname: nginx-config\rdata:\rdefault.conf: |-\rserver {\rserver_name localhost;\rlisten 80 default_server;\rlocation = /healthz {\radd_header Content-Type text/plain;\rreturn 200 'ok';\r}\rlocation / {\rroot /usr/share/nginx/html;\rindex index.html index.htm;\r}\rerror_page 500 502 503 504 /50x.html;\rlocation = /50x.html {\rroot /usr/share/nginx/html;\r}\r}\r（2）、NG的Deployment清单(需打开共享进程命名空间特性：shareProcessNamespace: true)：\nnginx-deploy.yaml\n---\rapiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: nginx\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: nginx\rtemplate:\rmetadata:\rname: nginx\rlabels:\rapp: nginx\rspec:\rshareProcessNamespace: true\rcontainers:\r- name: nginx\rimage: nginx\rimagePullPolicy: IfNotPresent\rvolumeMounts:\r- name: nginx-config\rmountPath: /etc/nginx/conf.d\rreadOnly: true - name: nginx-reloader\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/nginx-reloader:v1\rimagePullPolicy: IfNotPresent\renv:\r- name: WATCH_NGINX_CONF_PATH\rvalue: /etc/nginx/conf.d\rvolumeMounts:\r- name: nginx-config\rmountPath: /etc/nginx/conf.d\rreadOnly: true\rvolumes:\r- name: nginx-config\rconfigMap:\rname: nginx-config\r手动修改configmap后，reloader监测到configmap变化，会主动向nginx主进程发起HUP信号，实现配置热更新。\n二、第三方插件reloader 项目地址：https://github.com/stakater/Reloader\n资源清单如下，我修改了镜像地址：\n---\r# Source: reloader/templates/clusterrole.yaml\rapiVersion: rbac.authorization.k8s.io/v1beta1\rkind: ClusterRole\rmetadata:\rlabels:\rapp: reloader-reloader\rchart: \u0026quot;reloader-v0.0.58\u0026quot;\rrelease: \u0026quot;reloader\u0026quot;\rheritage: \u0026quot;Tiller\u0026quot;\rname: reloader-reloader-role\rnamespace: default\rrules:\r- apiGroups:\r- \u0026quot;\u0026quot;\rresources:\r- secrets\r- configmaps\rverbs:\r- list\r- get\r- watch\r- apiGroups:\r- \u0026quot;apps\u0026quot;\rresources:\r- deployments\r- daemonsets\r- statefulsets\rverbs:\r- list\r- get\r- update\r- patch\r- apiGroups:\r- \u0026quot;extensions\u0026quot;\rresources:\r- deployments\r- daemonsets\rverbs:\r- list\r- get\r- update\r- patch\r---\r# Source: reloader/templates/clusterrolebinding.yaml\rapiVersion: rbac.authorization.k8s.io/v1beta1\rkind: ClusterRoleBinding\rmetadata:\rlabels:\rapp: reloader-reloader\rchart: \u0026quot;reloader-v0.0.58\u0026quot;\rrelease: \u0026quot;reloader\u0026quot;\rheritage: \u0026quot;Tiller\u0026quot;\rname: reloader-reloader-role-binding\rnamespace: default\rroleRef:\rapiGroup: rbac.authorization.k8s.io\rkind: ClusterRole\rname: reloader-reloader-role\rsubjects:\r- kind: ServiceAccount\rname: reloader-reloader\rnamespace: default\r---\r# Source: reloader/templates/deployment.yaml\rapiVersion: apps/v1\rkind: Deployment\rmetadata:\rlabels:\rapp: reloader-reloader\rchart: \u0026quot;reloader-v0.0.58\u0026quot;\rrelease: \u0026quot;reloader\u0026quot;\rheritage: \u0026quot;Tiller\u0026quot;\rgroup: com.stakater.platform\rprovider: stakater\rversion: v0.0.58\rname: reloader-reloader\rspec:\rreplicas: 1\rrevisionHistoryLimit: 2\rselector:\rmatchLabels:\rapp: reloader-reloader\rrelease: \u0026quot;reloader\u0026quot;\rtemplate:\rmetadata:\rlabels:\rapp: reloader-reloader\rchart: \u0026quot;reloader-v0.0.58\u0026quot;\rrelease: \u0026quot;reloader\u0026quot;\rheritage: \u0026quot;Tiller\u0026quot;\rgroup: com.stakater.platform\rprovider: stakater\rversion: v0.0.58\rspec:\rcontainers:\r- env:\rimage: \u0026quot;registry.cn-hangzhou.aliyuncs.com/rookieops/stakater-reloader:v0.0.58\u0026quot;\rimagePullPolicy: IfNotPresent\rname: reloader-reloader\rargs:\rserviceAccountName: reloader-reloader\r---\r# Source: reloader/templates/role.yaml\r---\r# Source: reloader/templates/rolebinding.yaml\r---\r# Source: reloader/templates/service.yaml\r---\r# Source: reloader/templates/serviceaccount.yaml\rapiVersion: v1\rkind: ServiceAccount\rmetadata:\rlabels:\rapp: reloader-reloader\rchart: \u0026quot;reloader-v0.0.58\u0026quot;\rrelease: \u0026quot;reloader\u0026quot;\rheritage: \u0026quot;Tiller\u0026quot;\rname: reloader-reloader\r然后部署资源，结果如下：\n kubectl get pod\rNAME READY STATUS RESTARTS AGE\rmy-app-9bdd6cbbc-x9gnt 1/1 Running 0 38m\rreloader-reloader-ff767bb8-cpzgz 1/1 Running 0 56s\r然后给deployment增加一个annotations。如下：\nkubectl patch deployments.apps my-app -p '{\u0026quot;metadata\u0026quot;: {\u0026quot;annotations\u0026quot;: {\u0026quot;reloader.stakater.com/auto\u0026quot;: \u0026quot;true\u0026quot;}}}'\r然后我们更改configMap清单，重新apply过后，我们可以看到pod会删除重启，如下：\n kubectl get pod -o wide\rNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES\rmy-app-7c4fc77f5f-w4mbn 1/1 Running 0 3s 192.168.58.202 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rmy-app-df6fbdb67-bnftb 1/1 Terminating 0 35s 192.168.58.201 k8s-node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rreloader-reloader-ff767bb8-cpzgz 1/1 Running 0 3m47s 192.168.85.195 k8s-node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\r然后我们curl pod也可以通了，如下：\n# curl 192.168.58.202:8080 -I\rHTTP/1.1 200 OK\rServer: nginx/1.17.10\rDate: Tue, 26 May 2020 06:58:38 GMT\rContent-Type: text/html\rContent-Length: 612\rLast-Modified: Tue, 14 Apr 2020 14:19:26 GMT\rConnection: keep-alive\rETag: \u0026quot;5e95c66e-264\u0026quot;\rAccept-Ranges: bytes\r三、附加 附加一个sidecar形式的python脚本\n#!/usr/bin/env python\r# -*- encoding: utf8 -*-\r\u0026quot;\u0026quot;\u0026quot;\r需求：nginx配置文件变化，自动更新配置文件，类似nginx -s reload\r实现：\r1、用pyinotify实时监控nginx配置文件变化\r2、如果配置文件变化，给系统发送HUP来reload nginx\r\u0026quot;\u0026quot;\u0026quot;\rimport os\rimport re\rimport pyinotify\rimport logging\rfrom threading import Timer\r# Param\rLOG_PATH = \u0026quot;/root/python/log\u0026quot;\rCONF_PATHS = [\r\u0026quot;/etc/nginx\u0026quot;,\r]\rDELAY = 5\rSUDO = False\rRELOAD_COMMAND = \u0026quot;nginx -s reload\u0026quot;\rif SUDO:\rRELOAD_COMMAND = \u0026quot;sudo \u0026quot; + RELOAD_COMMAND\r# Log\rlogger = logging.getLogger(__name__)\rlogger.setLevel(level = logging.INFO)\rlog_handler = logging.FileHandler(LOG_PATH)\rlog_handler.setLevel(logging.INFO)\rlog_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\rlog_handler.setFormatter(log_formatter)\rlogger.addHandler(log_handler)\r# Reloader\rdef reload_nginx():\ros.system(RELOAD_COMMAND)\rlogger.info(\u0026quot;nginx is reloaded\u0026quot;)\rt = Timer(DELAY, reload_nginx)\rdef trigger_reload_nginx(pathname, action):\rlogger.info(\u0026quot;nginx monitor is triggered because %s is %s\u0026quot; % (pathname, action))\rglobal t\rif t.is_alive():\rt.cancel()\rt = Timer(DELAY, reload_nginx)\rt.start()\relse:\rt = Timer(DELAY, reload_nginx)\rt.start()\revents = pyinotify.IN_MODIFY | pyinotify.IN_CREATE | pyinotify.IN_DELETE\rwatcher = pyinotify.WatchManager()\rwatcher.add_watch(CONF_PATHS, events, rec=True, auto_add=True)\rclass EventHandler(pyinotify.ProcessEvent):\rdef process_default(self, event):\rif event.name.endswith(\u0026quot;.conf\u0026quot;):\rif event.mask == pyinotify.IN_CREATE:\raction = \u0026quot;created\u0026quot;\rif event.mask == pyinotify.IN_MODIFY:\raction = \u0026quot;modified\u0026quot;\rif event.mask == pyinotify.IN_DELETE:\raction = \u0026quot;deleted\u0026quot;\rtrigger_reload_nginx(event.pathname, action)\rhandler = EventHandler()\rnotifier = pyinotify.Notifier(watcher, handler)\r# Start\rlogger.info(\u0026quot;Start Monitoring\u0026quot;)\rnotifier.loop()\r","description":"本文主要介绍使用sidecar的方式使部署在kubernetes中的nginx容器实现热加载配置文件。","id":32,"section":"posts","tags":["kubernetes","nginx"],"title":"在Kubernetes中让nginx容器热加载配置文件","uri":"https://www.coolops.cn/posts/kubernetes-nginx-hot-update/"},{"content":" 内容主要源自邹佳在云原生社区的分享。\n Harbor 是一个用于存储和分发Docker 镜像的企业级Registry 服务器，由vmware开源，是一个可信的云原生制品仓库，用来存储、签名、管理相关的内容。\nHarbor的一切设计都是围绕了云原生展开的，并且会在这个方向一直坚持下去。\n在云原生下，镜像就是命脉，一切应用都是围绕着镜像，可以说镜像技术加速了云原生的发展。\n那么Harbor在镜像方面做了哪些呢？\n让镜像分发更高效 （1）基于策略的内容复制机制 Harbor支持多种过滤器（镜像库、标签等）与多种触发模式（手动、定时等）来实现镜像的推送和拉取。\n 初始的时候进行全量拉取 然后再通过增量拉取  在大集群，多机房的情况下，可以使用主从模式（中心-边缘模式）来进行镜像的分发。\n（2）提供项目级别的缓存能力 最近开源圈热论的话题就是DockerHub限速问题s，不过“上有政策，下有对策”，Harbor就可以有效解决这个问题。\n通过Harbor缓存下来的制品与“本地”制品无异，而且Harbor方面相关的管理策略也可以应用到缓存的镜像上，比如配额、扫描等。目前仅支持上游的DockerHub和其他的Harbor。\n在配置缓存时要注意几点：\n 要使用缓存功能，则必须在新建项目的时候选择启用，切该项目不可推送 已创建的普通项目无法直接转为缓存项目 Pull镜像的路径有专门的格式。docker pull \u0026lt;harbor-host\u0026gt;/[cache-project-name]/\u0026lt;repository\u0026gt;_path，比如docker pull goharbor.io/my_cache_pro/library/nginx:latest  （3）可以使用P2P进行镜像预热  PS：这里的P2P不是网贷机构，不会暴雷的。\n 那什么是P2P技术呢？\n在C/S模式中，数据的分发采用专门的服务器，多个客户端都从此服务器获取数据。这种模式的优点是：数据的一致性容易控制，系统也容易管理。但是此种模式的缺点是：因为服务器的个数只有一个(即便有多个也非常有限)，系统容易出现单一失效点；单一服务器面对众多的客户端，由于CPU能力、内存大小、网络带宽的限制，可同时服务的客户端非常有限，可扩展性差。\nP2P技术正是为了解决这些问题而提出来的一种对等网络结构。在P2P网络中，每个节点既可以从其他节点得到服务，也可以向其他节点提供服务。这样，庞大的终端资源被利用起来，一举解决了C/S模式中的两个弊端。\nHarbor也充分利用了这种技术，将所选镜像提前分发到P2P网络中，以便客户端拉取的时候直接从P2P网络中拉取。\n  基于策略实现自动化\n   Repository过滤器 Tag过滤器 标签（Label）过滤器 漏洞状态条件 签名状态条件    基于事件触发或定时触发\n  Harbor目前仅支持：\n Dragonfly。是阿里自研并捐献给 CNCF 的 P2P 文件分发系统。 Kraken。是 Uber 开源的点对点（P2P）Docker 容器仓库。  让镜像分发更安全 容器实际上是不透明的，被封装成一个个繁琐的镜像。当越来越多的镜像被创建时，没有人能确定镜像里到底封装了什么，所以日常使用的镜像都面临着严重的安全问题。\nHarbor在安全方面做了严格的把关。\n（1）对镜像进行签名  基于开源的Notary实现镜像的签名   基于GPG实现对Helm Chart的签名支持  （2）对镜像进行漏洞扫描  通过插件化接入扫描器，对镜像进行漏洞扫描   可以生成相应的扫描报告，以便与管理相关漏洞信息和了解安全威胁程度  （3）通过策略限制不安全镜像分发 可以在项目里设置相关的安全策略，以阻止不合安全规范的镜像分发。\n 基于内容信任，仅允许通过认证的镜像分发 基于危害级别，可以设置危害级别限制镜像分发  （4）通过规则来限制Tag不被覆盖或删除 默认情况下Harbor里的镜像是可以被覆盖和删除的，不过可以添加一些规则来保护一些Tag不被删除，比如latest的tag。\n规则可以通过正则匹配，匹配上的tag会被标记为不可变。\n优雅的资源清理和垃圾回收 犹记Harbor1.x的时候，资源清理和垃圾回收是多么的繁杂。要先调API进行资源清理，然后进重启进行垃圾回收。为此还专门写脚本进行定时清理。\n不过在Harbor2.x就不用这么麻烦了。\n（1）可以通过策略保留需要的TAG 可以在项目仓库里通过策略来保留需要的TAG。\n规则可以自定义，如下\n说明：\n 不可变TAG一定会被保留 该操作不释放存储空间，仅释放配额  （2）可以通过垃圾清理来释放空间 可以通过垃圾清理来释放空间。当然不一定释放很多空间，比如你的这个镜像的底层是链接了一个大的镜像，大镜像没被清理，空间也就释放不到多少。\n多种HA方案 （1）基于内容复制能力的HA 通俗点说就是多套独立的环境，通过Replicate的方式来进行同步数据，如下：\n（2）基于外部共享服务的HA （3）在Kubernetes集群中的多实例HA 原则上和第二种HA差不多。\n（4）Harbor Operator基于Kubernetes的all-in-one HA （5）多数据中心的HA 动手安装Harbor玩玩 “纸上得来终觉浅，绝知此事要躬行”。下面我们就来安装Harbor来玩玩。\nHarbor的安装方式主要有以下几种。\n 离线安装包 在线安装包 Helm Chart Harbor Operator（开发中）  这里主要尝试前三种安装方式。\n系统：Centos 7.4\n内核：Kernel 3.10\nHarbor：2.1.1\ndocker-compose：1.27.4\nhelm：v3\nkubernetes：1.17.9\n下载地址：\nhttps://github.com/goharbor/harbor/releases\nhttps://github.com/docker/compose/releases\n 前提：系统做好了初始化\n 01 系统初始化以及安装所需软件 （1）关闭防火墙、SeLinux、修改主机名\n# systemctl stop firewalld\r# systemctl disable firewalld\r# setenforce 0\r# vim /etc/sysconfig/selinux\rSELINUX=disabeld\r# hostnamectl set-hostname harbor\r# hostname\r（2）安装docker-ce\n# yum install -y yum-utils device-mapper-persistent-data lvm2\r# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\r# yum install docker-ce -y\r# systemctl start docker\r# systemctl enable docker\r# curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io\r# systemctl restart docker\r（3）安装docker-compose\n# wget https://github.com/docker/compose/releases/download/1.27.4/docker-compose-Linux-x86_64\r# mv docker-compose-Linux-x86_64 /usr/local/bin/docker-compose\r# chmod +x /usr/local/bin/docker-compose\r# docker-compose version\r02 离线安装Harbor （1）下载Harbor离线安装包并解压到/opt目录下\n# wget https://github.com/goharbor/harbor/releases/download/v2.1.1/harbor-offline-installer-v2.1.1.tgz\r# tar xf harbor-offline-installer-v2.1.1.tgz -C /opt/\r（2）拷贝配置文件，并对起进行修改\n# cd /opt/harbor\r# cp harbor.yml.tmpl harbor.yml\r配置文件主要修改以下几个地方。\n hostname：主机名或域名 https证书 harbor_admin_password：管理员密码 data_volume：数据存放目录  修改后如下：\nhostname: 172.17.100.171,harbor.coolops.cn http:\rport: 80\rhttps:\rport: 443\rcertificate: /opt/harbor/ssl/harbor.coolops.cn.crt\rprivate_key: /opt/harbor/ssl/harbor.coolops.cn.key\rharbor_admin_password: Harbor12345\rdatabase:\rpassword: root123\rmax_idle_conns: 50\rmax_open_conns: 1000\rdata_volume: /data\r......\r（3）生成ssl证书\n这里使用openssl工具生成。\n# yum install -y openssl\r# mkdir /opt/harbor/ssl\r# cd /opt/harbor/ssl\r# openssl genrsa -out ca.key 4096\r# openssl req -x509 -new -nodes -sha512 -days 3650 -subj \u0026quot;/C=CN/ST=Chongqing/L=Chongqing/O=harbor.coolops.cn/OU=harbor.coolops.cn/CN=harbor.coolops.cn\u0026quot; -key ca.key -out ca.crt\r# openssl genrsa -out harbor.coolops.cn.key 4096\r# openssl req -sha512 -new -subj \u0026quot;/C=CN/ST=Chongqing/L=Chongqing/O=harbor.coolops.cn/OU=harbor.coolops.cn/CN=harbor.coolops.cn\u0026quot; -key harbor.coolops.cn.key -out harbor.coolops.cn.csr\r创建一个外部配置文件\n#cat \u0026gt; xexternalfile.ext \u0026lt;\u0026lt;-EOF\rauthorityKeyIdentifier=keyid,issuer\rbasicConstraints=CA:FALSE\rkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment\rextendedKeyUsage = serverAuth\rsubjectAltName = @alt_names\r[alt_names]\rDNS.1=harbor.coolops.cn\rEOF\r通过外部配置文件生成crt和csr\n# openssl x509 -req -sha512 -days 3650 -extfile xexternalfile.ext -CA ca.crt -CAkey ca.key -CAcreateserial -in harbor.coolops.cn.csr -out harbor.coolops.cn.crt\r将crt转为cert\n# openssl x509 -inform PEM -in harbor.coolops.cn.crt -out harbor.coolops.cn.cert\r（4）预装harbor\n# cd /opt/harbor\r# ./prepare\r通过预装可以看看配置是否有问题。\n（5）正式安装\n直接执行目录下install.sh脚本\n# cd /opt/harbor\r# ./install.sh\r看到如下输出表示安装完成。\n[Step 5]: starting Harbor ...\rCreating network \u0026quot;harbor_harbor\u0026quot; with the default driver\rCreating harbor-log ... done\rCreating redis ... done\rCreating registry ... done\rCreating registryctl ... done\rCreating harbor-portal ... done\rCreating harbor-db ... done\rCreating harbor-core ... done\rCreating harbor-jobservice ... done\rCreating nginx ... done\r✔ ----Harbor has been installed and started successfully.----\r然后就可以访问了，如下。\n03 在线安装Harbor 在线安装和离线安装唯一的区别就是在线安装需要从网上自己下载镜像。 依然按照第一步装好必要的基本软件。\n然后下载在线安装包。\n# wget https://github.com/goharbor/harbor/releases/download/v2.1.1/harbor-online-installer-v2.1.1.tgz\r然后按照离线安装的第2、第3步骤安装即可。\n04 Helm Chart （1）安装helm3\nhelm安装在master节点。\n# wget https://get.helm.sh/helm-v3.0.0-linux-amd64.tar.gz\r# tar zxvf helm-v3.0.0-linux-amd64.tar.gz\r# mv linux-amd64/helm /usr/bin/\r# helm version\r（2）添加Harbor仓库\n# helm repo add harbor https://helm.goharbor.io\r（3）查看helm版本，并下载\n# helm search repo harbor\rNAME CHART VERSION APP VERSION DESCRIPTION harbor/harbor 1.5.1 2.1.1 An open source trusted cloud native registry th...\r# helm pull harbor/harbor --version 1.5.1\r# tar xf harbor-1.5.1.tgz\r（4）进入harbor目录，可以看到如下文件以及目录\n# cd harbor/\r# ll\rtotal 128\rdrwxr-xr-x 2 root root 4096 Nov 13 14:07 cert\r-rwxr-xr-x 1 root root 576 Oct 30 10:48 Chart.yaml\rdrwxr-xr-x 2 root root 4096 Nov 13 14:07 conf\r-rwxr-xr-x 1 root root 11357 Oct 30 10:48 LICENSE\r-rwxr-xr-x 1 root root 72587 Oct 30 10:48 README.md\rdrwxr-xr-x 15 root root 4096 Nov 13 14:07 templates\r-rwxr-xr-x 1 root root 25467 Oct 30 10:48 values.yaml\r（5）修改values.yaml配置文件\n主要修改地方如下：\n......\ringress:\rhosts:\rcore: harbor.coolops.cn\rnotary: notary.coolops.cn\r......\rexternalURL: https://harbor.coolops.cn\r......\rharborAdminPassword: \u0026quot;Harbor12345\u0026quot;\r 关于持久化存储，如果自己配置storageclass，就在values.yaml中对应填上名字，如果让起自动创建的话，则默认使用集群default的存储\n （6）安装harbor\n# helm install harbor ./harbor\rNAME: harbor\rLAST DEPLOYED: Fri Nov 13 14:21:06 2020\rNAMESPACE: default\rSTATUS: deployed\rREVISION: 1\rTEST SUITE: None\rNOTES:\rPlease wait for several minutes for Harbor deployment to complete.\rThen you should be able to visit the Harbor portal at https://harbor.coolops.cn\rFor more details, please visit https://github.com/goharbor/harbor\r查看部署情况\n# helm list\rNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION\rharbor default 1 2020-11-13 14:21:06.555605312 +0800 CST deployed harbor-1.5.1 2.1.1 然后等待所有pod变成running，就可以通过Ingress访问了。\n","description":"本文介绍Harbor如何在云原生中使用，并附带一些安装方式","id":33,"section":"posts","tags":["Cloud Native","Harbor","kubernetes","docker-compose","docker"],"title":"Harbor：在云原生中使用，附安装","uri":"https://www.coolops.cn/posts/harbor/"},{"content":"环境准备\n3个节点，都是 Centos 7.6 系统，内核版本：3.10.0-957.12.2.el7.x86_64，在每个节点上添加 hosts 信息：\n$ cat /etc/hosts\n172.16.1.128 k8s-master\r172.16.1.129 k8s-node01\r172.16.1.130 k8s-node02\r禁用防火墙：\n$ systemctl stop firewalld\r$ systemctl disable firewalld\r禁用SELINUX：\n$ setenforce 0\r$ cat /etc/selinux/config\rSELINUX=disabled\r创建/etc/sysctl.d/k8s.conf文件，添加如下内容：\nnet.bridge.bridge-nf-call-ip6tables = 1\rnet.bridge.bridge-nf-call-iptables = 1\rnet.ipv4.ip_forward = 1\r执行如下命令使修改生效：\n$ modprobe br_netfilter\r$ sysctl -p /etc/sysctl.d/k8s.conf\r安装 ipvs\n$ cat \u0026gt; /etc/sysconfig/modules/ipvs.modules \u0026lt;\u0026lt;EOF\r#!/bin/bash\rmodprobe -- ip_vs\rmodprobe -- ip_vs_rr\rmodprobe -- ip_vs_wrr\rmodprobe -- ip_vs_sh\rmodprobe -- nf_conntrack_ipv4\rEOF\r$ chmod 755 /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; bash /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4\r上面脚本创建了的/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。使用lsmod | grep -e ip_vs -e nf_conntrack_ipv4命令查看是否已经正确加载所需的内核模块。\n接下来还需要确保各个节点上已经安装了 ipset 软件包：\n$ yum install ipset -y\r为了便于查看 ipvs 的代理规则，最好安装一下管理工具 ipvsadm：\n$ yum install ipvsadm -y\r同步服务器时间\n$ yum install chrony -y\r$ systemctl enable chronyd\r$ systemctl start chronyd\r$ chronyc sources\r关闭 swap 分区：\n$ swapoff -a\r修改/etc/fstab文件，注释掉 SWAP 的自动挂载，使用free -m确认 swap 已经关闭。swappiness 参数调整，修改/etc/sysctl.d/k8s.conf添加下面一行：\nvm.swappiness=0\r执行sysctl -p /etc/sysctl.d/k8s.conf使修改生效。\n接下来可以安装 Docker\n$ yum install -y yum-utils \\\rdevice-mapper-persistent-data \\\rlvm2\r$ yum-config-manager \\\r--add-repo \\\rhttps://download.docker.com/linux/centos/docker-ce.repo\r$ yum list docker-ce --showduplicates | sort -r\r可以选择安装一个版本，比如我们这里安装最新版本：\n$ yum install docker-ce-18.09.8-3.el7 -y\r配置 Docker 镜像加速器\n$ vi /etc/docker/daemon.json\n{\r\u0026quot;exec-opts\u0026quot;: [\u0026quot;native.cgroupdriver=systemd\u0026quot;],\r\u0026quot;registry-mirrors\u0026quot; : [\r\u0026quot;https://ot2k4d59.mirror.aliyuncs.com/\u0026quot;\r]\r}\r启动 Docker\n$ systemctl start docker\r$ systemctl enable docker\r在确保 Docker 安装完成后，上面的相关环境配置也完成了，现在我们就可以来安装 Kubeadm 了，我们这里是通过指定yum 源的方式来进行安装的：\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo\r[kubernetes]\rname=Kubernetes\rbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64\renabled=1\rgpgcheck=1\rrepo_gpgcheck=1\rgpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg\rhttps://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\rEOF\r当然了，上面的 yum 源是需要科学上网的，如果不能科学上网的话，我们可以使用阿里云的源进行安装：\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo\r[kubernetes]\rname=Kubernetes\rbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\renabled=1\rgpgcheck=0\rrepo_gpgcheck=0\rgpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\rhttp://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\rEOF\r然后安装 kubeadm、kubelet、kubectl（我安装的是最新版，有版本要求自己设定版本）：\n$ yum install -y kubelet-1.17.2 kubeadm-1.17.2 kubectl-1.17.2 --disableexcludes=kubernetes\r可以看到我们这里安装的是 v1.17.2 版本，然后将 kubelet 设置成开机启动：\n$ systemctl enable kubelet.service\r 到这里为止上面所有的操作都需要在所有节点执行配置。\n 初始化集群\n然后接下来在 master 节点配置 kubeadm 初始化文件，可以通过如下命令导出默认的初始化配置：\n$ kubeadm config print init-defaults \u0026gt; kubeadm.yaml\r然后根据我们自己的需求修改配置，比如修改 imageRepository 的值，kube-proxy 的模式为 ipvs，另外需要注意的是我们这里是准备安装 calico 网络插件的，需要将 networking.podSubnet 设置为192.168.0.0/16：\napiVersion: kubeadm.k8s.io/v1beta2\rbootstrapTokens:\r- groups:\r- system:bootstrappers:kubeadm:default-node-token\rtoken: abcdef.0123456789abcdef\rttl: 24h0m0s\rusages:\r- signing\r- authentication\rkind: InitConfiguration\rlocalAPIEndpoint:\radvertiseAddress: 172.16.1.128\rbindPort: 6443\rnodeRegistration:\rcriSocket: /var/run/dockershim.sock\rname: k8s-master\rtaints:\r- effect: NoSchedule\rkey: node-role.kubernetes.io/master\r---\rapiServer:\rtimeoutForControlPlane: 4m0s\rapiVersion: kubeadm.k8s.io/v1beta2\rcertificatesDir: /etc/kubernetes/pki\rclusterName: kubernetes\rcontrollerManager: {}\rdns:\rtype: CoreDNS\retcd:\rlocal:\rdataDir: /var/lib/etcd\rimageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers\rkind: ClusterConfiguration\rkubernetesVersion: v1.17.2\rnetworking:\rdnsDomain: cluster.local\rpodSubnet: 192.168.0.0/16\rserviceSubnet: 10.96.0.0/12\rscheduler: {}\r---\rapiVersion: kubeproxy.config.k8s.io/v1alpha1\rkind: KubeProxyConfiguration\rmode: ipvs\r然后使用上面的配置文件进行初始化：\n$ kubeadm init --config kubeadm.yaml\r[init] Using Kubernetes version: v1.17.2\r[preflight] Running pre-flight checks\r[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 19.03.1. Latest validated version: 18.09\r[preflight] Pulling images required for setting up a Kubernetes cluster\r[preflight] This might take a minute or two, depending on the speed of your internet connection\r[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\r[kubelet-start] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot;\r[kubelet-start] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot;\r[kubelet-start] Activating the kubelet service\r[certs] Using certificateDir folder \u0026quot;/etc/kubernetes/pki\u0026quot;\r[certs] Generating \u0026quot;etcd/ca\u0026quot; certificate and key\r[certs] Generating \u0026quot;etcd/server\u0026quot; certificate and key\r[certs] etcd/server serving cert is signed for DNS names [ydzs-master localhost] and IPs [10.151.30.11 127.0.0.1 ::1]\r[certs] Generating \u0026quot;etcd/peer\u0026quot; certificate and key\r[certs] etcd/peer serving cert is signed for DNS names [ydzs-master localhost] and IPs [10.151.30.11 127.0.0.1 ::1]\r[certs] Generating \u0026quot;apiserver-etcd-client\u0026quot; certificate and key\r[certs] Generating \u0026quot;etcd/healthcheck-client\u0026quot; certificate and key\r[certs] Generating \u0026quot;ca\u0026quot; certificate and key\r[certs] Generating \u0026quot;apiserver\u0026quot; certificate and key\r[certs] apiserver serving cert is signed for DNS names [ydzs-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.151.30.11]\r[certs] Generating \u0026quot;apiserver-kubelet-client\u0026quot; certificate and key\r[certs] Generating \u0026quot;front-proxy-ca\u0026quot; certificate and key\r[certs] Generating \u0026quot;front-proxy-client\u0026quot; certificate and key\r[certs] Generating \u0026quot;sa\u0026quot; key and public key\r[kubeconfig] Using kubeconfig folder \u0026quot;/etc/kubernetes\u0026quot;\r[kubeconfig] Writing \u0026quot;admin.conf\u0026quot; kubeconfig file\r[kubeconfig] Writing \u0026quot;kubelet.conf\u0026quot; kubeconfig file\r[kubeconfig] Writing \u0026quot;controller-manager.conf\u0026quot; kubeconfig file\r[kubeconfig] Writing \u0026quot;scheduler.conf\u0026quot; kubeconfig file\r[control-plane] Using manifest folder \u0026quot;/etc/kubernetes/manifests\u0026quot;\r[control-plane] Creating static Pod manifest for \u0026quot;kube-apiserver\u0026quot;\r[control-plane] Creating static Pod manifest for \u0026quot;kube-controller-manager\u0026quot;\r[control-plane] Creating static Pod manifest for \u0026quot;kube-scheduler\u0026quot;\r[etcd] Creating static Pod manifest for local etcd in \u0026quot;/etc/kubernetes/manifests\u0026quot;\r[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026quot;/etc/kubernetes/manifests\u0026quot;. This can take up to 4m0s\r[kubelet-check] Initial timeout of 40s passed.\r[apiclient] All control plane components are healthy after 42.012149 seconds\r[upload-config] Storing the configuration used in ConfigMap \u0026quot;kubeadm-config\u0026quot; in the \u0026quot;kube-system\u0026quot; Namespace\r[kubelet] Creating a ConfigMap \u0026quot;kubelet-config-1.15\u0026quot; in namespace kube-system with the configuration for the kubelets in the cluster\r[upload-certs] Skipping phase. Please see --upload-certs\r[mark-control-plane] Marking the node ydzs-master as control-plane by adding the label \u0026quot;node-role.kubernetes.io/master=''\u0026quot;\r[mark-control-plane] Marking the node ydzs-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]\r[bootstrap-token] Using token: abcdef.0123456789abcdef\r[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\r[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\r[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\r[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\r[bootstrap-token] Creating the \u0026quot;cluster-info\u0026quot; ConfigMap in the \u0026quot;kube-public\u0026quot; namespace\r[addons] Applied essential addon: CoreDNS\r[addons] Applied essential addon: kube-proxy\rYour Kubernetes control-plane has initialized successfully!\rTo start using your cluster, you need to run the following as a regular user:\rmkdir -p $HOME/.kube\rsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\rsudo chown $(id -u):$(id -g) $HOME/.kube/config\rYou should now deploy a pod network to the cluster.\rRun \u0026quot;kubectl apply -f [podnetwork].yaml\u0026quot; with one of the options listed at:\rhttps://kubernetes.io/docs/concepts/cluster-administration/addons/\rThen you can join any number of worker nodes by running the following on each as root:\rkubeadm join 172.16.1.128:6443 --token abcdef.0123456789abcdef \\\r--discovery-token-ca-cert-hash sha256:deb5158b39948a4592ff48512047ea6e45b288c248872724a28f15008962178b\r 可以看到最新验证的 docker 版本是18.09，虽然是一个 warning，所以最好还是安装18.09版本的 docker。\n 拷贝 kubeconfig 文件\n$ mkdir -p $HOME/.kube\r$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\r$ sudo chown $(id -u):$(id -g) $HOME/.kube/config\r添加节点\n记住初始化集群上面的配置和操作要提前做好，将 master 节点上面的 $HOME/.kube/config 文件拷贝到 node 节点对应的文件中，安装 kubeadm、kubelet、kubectl，然后执行上面初始化完成后提示的 join 命令即可：\nkubeadm join 172.16.1.128:6443 --token abcdef.0123456789abcdef \\\r--discovery-token-ca-cert-hash sha256:deb5158b39948a4592ff48512047ea6e45b288c248872724a28f15008962178b\r[preflight] Reading configuration from the cluster...\r[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'\r[kubelet-start] Downloading configuration for the kubelet from the \u0026quot;kubelet-config-1.15\u0026quot; ConfigMap in the kube-system namespace\r[kubelet-start] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot;\r[kubelet-start] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot;\r[kubelet-start] Activating the kubelet service\r[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...\rThis node has joined the cluster:\r* Certificate signing request was sent to apiserver and a response was received.\r* The Kubelet was informed of the new secure connection details.\rRun 'kubectl get nodes' on the control-plane to see this node join the cluster.\r 如果忘记了上面的 join 命令可以使用命令kubeadm token create \u0026ndash;print-join-command重新获取。\n 执行成功后运行 get nodes 命令：\n# kubectl get nodes NAME STATUS ROLES AGE VERSION\rk8s-master Ready master 3d18h v1.17.2\rk8s-node01 Ready node01 3d18h v1.17.2\rk8s-node02 Ready node02 3d17h v1.17.2\r可以看到是 NotReady 状态，这是因为还没有安装网络插件，接下来安装网络插件，可以在文档 https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/ 中选择我们自己的网络插件，这里我们安装 calio:\n$ wget https://docs.projectcalico.org/v3.8/manifests/calico.yaml\r# 因为有节点是多网卡，所以需要在资源清单文件中指定内网网卡\n$ vi calico.yaml\n......\rspec:\rcontainers:\r- env:\r- name: DATASTORE_TYPE\rvalue: kubernetes\r- name: IP_AUTODETECTION_METHOD # DaemonSet中添加该环境变量\rvalue: interface=ens33 # 指定内网网卡\r- name: WAIT_FOR_DATASTORE\rvalue: \u0026quot;true\u0026quot;\r......\r$ kubectl apply -f calico.yaml # 安装calico网络插件\n隔一会儿查看 Pod 运行状态：\n# kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE\rcalico-kube-controllers-64d7686ff8-q4wwg 1/1 Running 0 10m\rcalico-node-92b29 1/1 Running 0 3d17h\rcalico-node-f8shx 1/1 Running 7 3d17h\rcalico-node-jsk9r 1/1 Running 0 3d17h\rcoredns-6cd559f5d5-448x5 1/1 Running 0 3d18h\rcoredns-6cd559f5d5-tprrj 1/1 Running 0 3d18h\retcd-k8s-master 1/1 Running 1 3d18h\rkube-apiserver-k8s-master 1/1 Running 4 3d18h\rkube-controller-manager-k8s-master 1/1 Running 18 3d18h\rkube-proxy-7hk2n 1/1 Running 1 3d18h\rkube-proxy-ffslj 1/1 Running 1 3d17h\rkube-proxy-txhcq 1/1 Running 1 3d18h\rkube-scheduler-k8s-master 1/1 Running 17 3d18h\r网络插件运行成功了，node 状态也正常了：\n# kubectl get nodes NAME STATUS ROLES AGE VERSION\rk8s-master Ready master 3d18h v1.17.2\rk8s-node01 Ready node01 3d18h v1.17.2\rk8s-node02 Ready node02 3d17h v1.17.2\r用同样的方法添加另外一个节点即可。\n安装 Dashboard\n$ wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml\r$ vi kubernetes-dashboard.yaml\r# 修改镜像名称\n......\rcontainers:\r- args:\r- --auto-generate-certificates\rimage: gcr.azk8s.cn/google_containers/kubernetes-dashboard-amd64:v1.10.1\rimagePullPolicy: IfNotPresent\r......\r# 修改Service为NodePort类型\n......\rselector:\rk8s-app: kubernetes-dashboard\rtype: NodePort\r......\r直接创建：\n$ kubectl apply -f kubernetes-dashboard.yaml\r$ kubectl get pods -n kube-system -l k8s-app=kubernetes-dashboard\rNAME READY STATUS RESTARTS AGE\rkubernetes-dashboard-fcfb4cbc-t462n 1/1 Running 0 50m\r$ kubectl get svc -n kube-system -l k8s-app=kubernetes-dashboard\rNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rkubernetes-dashboard NodePort 10.110.172.49 \u0026lt;none\u0026gt; 443:32497/TCP 55m\r然后可以通过上面的 32497 端口去访问 Dashboard，要记住使用 https，Chrome不生效可以使用Firefox测试。\n然后创建一个具有全局所有权限的用户来登录Dashboard：(admin.yaml)\nkind: ClusterRoleBinding\rapiVersion: rbac.authorization.k8s.io/v1beta1\rmetadata:\rname: admin\rannotations:\rrbac.authorization.kubernetes.io/autoupdate: \u0026quot;true\u0026quot;\rroleRef:\rkind: ClusterRole\rname: cluster-admin\rapiGroup: rbac.authorization.k8s.io\rsubjects:\r- kind: ServiceAccount\rname: admin\rnamespace: kube-system\r---\rapiVersion: v1\rkind: ServiceAccount\rmetadata:\rname: admin\rnamespace: kube-system\rlabels:\rkubernetes.io/cluster-service: \u0026quot;true\u0026quot;\raddonmanager.kubernetes.io/mode: Reconcile\r直接创建：\n$ kubectl apply -f admin.yaml\r$ kubectl get secret -n kube-system|grep admin-token\radmin-token-d5jsg kubernetes.io/service-account-token 3 1d\r$ kubectl get secret admin-token-d5jsg -o jsonpath={.data.token} -n kube-system |base64 -d# 会生成一串很长的base64后的字符串\r然后用上面的base64解码后的字符串作为token登录Dashboard即可：\n最终我们就完成了使用 kubeadm 搭建 v1.17.2 版本的 kubernetes 集群、coredns、ipvs、calico。\n配置命令自动补全\nyum install -y bash-completion\rsource /usr/share/bash-completion/bash_completion\rsource \u0026lt;(kubectl completion bash)\recho \u0026quot;source \u0026lt;(kubectl completion bash)\u0026quot; \u0026gt;\u0026gt; ~/.bashrc\r","description":"本文主要介绍使用kubeadm搭建单master节点的Kubernetes集群","id":34,"section":"posts","tags":["kubernetes","kubeadm"],"title":"Kubeadm搭建单master节点的Kubernetes集群","uri":"https://www.coolops.cn/posts/kubeadm-install-single-master-kubernetes/"},{"content":"PS: 最近经常有朋友问我有没有用kubeadm搭建高可用集群的文档，说实在的我确实没有，我自己测试的话就用kubeadm单master版，公司用的话就用二进制搭建的。所以就找了个下班时间搭建测试了一番。希望对大家有帮助！如果觉得有用的话就帮忙点个关注或转发吧，哈哈~\n节点规划信息    名称 IP     k8s-master01 10.1.10.100   k8s-master02 10.1.10.101   k8s-master03 10.1.10.102   k8s-node01 10.1.10.103   k8s-lb 10.1.10.200    基础环境配置 环境信息    系统 CentOS7.6.1810     内核版本 4.9.220       软件 版本     kubernetes 1.18.2   docker 19.0.3    环境初始化 （1）、配置主机名，以k8s-master01为例\nhostnamectl set-hostname k8s-master01\r（1）、配置主机hosts映射\n10.1.10.100 k8s-master01\r10.1.10.101 k8s-master02\r10.1.10.102 k8s-master03\r10.1.10.103 k8s-node01\r10.1.10.200 k8s-lb\r配置完后可以通过如下命令测试\nfor host in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-lb;do ping -c 1 $host;done\r 这里ping k8s-node01不通，是因为我们还没配置VIP\n （2）、禁用防火墙\nsystemctl stop firewalld\rsystemctl disable firewalld\r（3）、关闭selinux\nsetenforce 0\rsed -i \u0026quot;s/^SELINUX=.*/SELINUX=disabled/g\u0026quot; /etc/sysconfig/selinux\rsed -i \u0026quot;s/^SELINUX=.*/SELINUX=disabled/g\u0026quot; /etc/selinux/config\r（4）、关闭swap分区\nswapoff -a \u0026amp;\u0026amp; sysctl -w vm.swappiness=0\r（5）、时间同步\nyum install chrony -y\rsystemctl enable chronyd\rsystemctl start chronyd\rchronyc sources\r（6）、配置ulimt\nulimit -SHn 65535\r（7）、配置内核参数\ncat \u0026gt;\u0026gt; /etc/sysctl.d/k8s.conf \u0026lt;\u0026lt; EOF\rnet.bridge.bridge-nf-call-ip6tables = 1\rnet.bridge.bridge-nf-call-iptables = 1\rnet.ipv4.ip_forward = 1\rvm.swappiness=0\rEOF\r使之生效\nsysctl -p\r（8）、master之间添加互信（按需）\nssh-keygen\rssh-copy-id 10.1.10.101\rssh-copy-id 10.1.10.102\r内核升级 由于centos7.6的系统默认内核版本是3.10，3.10的内核有很多BUG，最常见的一个就是group memory leak。\n（1）、下载所需要的内核版本，我这里采用rpm安装，所以直接下载的rpm包\nwget https://cbs.centos.org/kojifiles/packages/kernel/4.9.220/37.el7/x86_64/kernel-4.9.220-37.el7.x86_64.rpm\r（2）、执行rpm升级即可\nrpm -ivh kernel-4.9.220-37.el7.x86_64.rpm\r（3）、升级完reboot，然后查看内核是否成功升级\nreboot\runame -r\r组件安装 安装ipvs （1）、安装ipvs需要的软件\n由于我准备使用ipvs作为kube-proxy的代理模式，所以需要安装相应的软件包。\nyum install ipvsadm ipset sysstat conntrack libseccomp -y\r（2）、加载模块\ncat \u0026gt; /etc/sysconfig/modules/ipvs.modules \u0026lt;\u0026lt;EOF\r#!/bin/bash\rmodprobe -- ip_vs\rmodprobe -- ip_vs_rr\rmodprobe -- ip_vs_wrr\rmodprobe -- ip_vs_sh\rmodprobe -- nf_conntrack\rmodprobe -- ip_tables\rmodprobe -- ip_set\rmodprobe -- xt_set\rmodprobe -- ipt_set\rmodprobe -- ipt_rpfilter\rmodprobe -- ipt_REJECT\rmodprobe -- ipip\rEOF\r 注意：在内核4.19版本nf_conntrack_ipv4已经改为nf_conntrack\n 配置重启自动加载\nchmod 755 /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; bash /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; lsmod | grep -e ip_vs -e nf_conntrack\r安装docker-ce # 安装需要的软件\ryum install -y yum-utils device-mapper-persistent-data lvm2\r# 添加yum源\ryum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\r查看是否有docker-ce包\n# yum list | grep docker-ce\rcontainerd.io.x86_64 1.2.13-3.1.el7 docker-ce-stable\rdocker-ce.x86_64 3:19.03.8-3.el7 docker-ce-stable\rdocker-ce-cli.x86_64 1:19.03.8-3.el7 docker-ce-stable\rdocker-ce-selinux.noarch 17.03.3.ce-1.el7 docker-ce-stable\r安装docker-ce\nyum install docker-ce-19.03.8-3.el7 -y\rsystemctl start docker\rsystemctl enable docker\r配置镜像加速\ncurl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io\rsystemctl restart docker\r安装kubernetes组件 添加yum源\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo\r[kubernetes]\rname=Kubernetes\rbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\renabled=1\rgpgcheck=0\rrepo_gpgcheck=0\rgpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\rhttp://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\rEOF\r安装软件\nyum install -y kubelet-1.18.2-0 kubeadm-1.18.2-0 kubectl-1.18.2-0 --disableexcludes=kubernetes\r将kubelet设置为开机自启动\nsystemctl enable kubelet.service\r 以上操作在所有节点执行\n 集群初始化 配置VIP 高可用采用的是HAProxy+Keepalived，HAProxy和KeepAlived以守护进程的方式在所有Master节点部署。\n安装软件 yum install keepalived haproxy -y\r配置haproxy 所有master节点的配置相同，如下：\n#---------------------------------------------------------------------\r# Global settings\r#---------------------------------------------------------------------\rglobal\r# to have these messages end up in /var/log/haproxy.log you will\r# need to:\r#\r# 1) configure syslog to accept network log events. This is done\r# by adding the '-r' option to the SYSLOGD_OPTIONS in\r# /etc/sysconfig/syslog\r#\r# 2) configure local2 events to go to the /var/log/haproxy.log\r# file. A line like the following can be added to\r# /etc/sysconfig/syslog\r#\r# local2.* /var/log/haproxy.log\r#\rlog 127.0.0.1 local2\rchroot /var/lib/haproxy\rpidfile /var/run/haproxy.pid\rmaxconn 4000\ruser haproxy\rgroup haproxy\rdaemon\r# turn on stats unix socket\rstats socket /var/lib/haproxy/stats\r#---------------------------------------------------------------------\r# common defaults that all the 'listen' and 'backend' sections will\r# use if not designated in their block\r#---------------------------------------------------------------------\rdefaults\rmode http\rlog global\roption httplog\roption dontlognull\roption http-server-close\roption redispatch\rretries 3\rtimeout http-request 10s\rtimeout queue 1m\rtimeout connect 10s\rtimeout client 1m\rtimeout server 1m\rtimeout http-keep-alive 10s\rtimeout check 10s\rmaxconn 3000\r#---------------------------------------------------------------------\r# kubernetes apiserver frontend which proxys to the backends\r#---------------------------------------------------------------------\rfrontend kubernetes\rmode tcp\rbind *:16443\roption tcplog\rdefault_backend kubernetes-apiserver\r#---------------------------------------------------------------------\r# round robin balancing between the various backends\r#---------------------------------------------------------------------\rbackend kubernetes-apiserver\rmode tcp\rbalance roundrobin\rserver k8s-master01 10.1.10.100:6443 check\rserver k8s-master02 10.1.10.101:6443 check\rserver k8s-master03 10.1.10.102:6443 check\r#---------------------------------------------------------------------\r# collection haproxy statistics message\r#---------------------------------------------------------------------\rlisten stats\rbind *:9999\rstats auth admin:P@ssW0rd\rstats refresh 5s\rstats realm HAProxy\\ Statistics\rstats uri /admin?stats\r配置keepalived k8s-master01\n! Configuration File for keepalived\rglobal_defs {\rnotification_email {\racassen@firewall.loc\rfailover@firewall.loc\rsysadmin@firewall.loc\r}\rnotification_email_from Alexandre.Cassen@firewall.loc\rsmtp_server 192.168.200.1\rsmtp_connect_timeout 30\rrouter_id LVS_DEVEL\rvrrp_skip_check_adv_addr\rvrrp_garp_interval 0\rvrrp_gna_interval 0\r}\r# 定义脚本\rvrrp_script check_apiserver {\rscript \u0026quot;/etc/keepalived/check_apiserver.sh\u0026quot; interval 2 weight -5 fall 3 rise 2 }\rvrrp_instance VI_1 {\rstate MASTER\rinterface eth33\rvirtual_router_id 51\rpriority 100\radvert_int 1\rauthentication {\rauth_type PASS\rauth_pass 1111\r}\rvirtual_ipaddress {\r10.1.10.200\r}\r# 调用脚本\rtrack_script {\rcheck_apiserver\r}\r}\rk8s-master02\n! Configuration File for keepalived\rglobal_defs {\rnotification_email {\racassen@firewall.loc\rfailover@firewall.loc\rsysadmin@firewall.loc\r}\rnotification_email_from Alexandre.Cassen@firewall.loc\rsmtp_server 192.168.200.1\rsmtp_connect_timeout 30\rrouter_id LVS_DEVEL\rvrrp_skip_check_adv_addr\rvrrp_garp_interval 0\rvrrp_gna_interval 0\r}\r# 定义脚本\rvrrp_script check_apiserver {\rscript \u0026quot;/etc/keepalived/check_apiserver.sh\u0026quot; interval 2 weight -5 fall 3 rise 2 }\rvrrp_instance VI_1 {\rstate MASTER\rinterface eth33\rvirtual_router_id 51\rpriority 99\radvert_int 1\rauthentication {\rauth_type PASS\rauth_pass 1111\r}\rvirtual_ipaddress {\r10.1.10.200\r}\r# 调用脚本\rtrack_script {\rcheck_apiserver\r}\r}\rk8s-master03\n! Configuration File for keepalived\rglobal_defs {\rnotification_email {\racassen@firewall.loc\rfailover@firewall.loc\rsysadmin@firewall.loc\r}\rnotification_email_from Alexandre.Cassen@firewall.loc\rsmtp_server 192.168.200.1\rsmtp_connect_timeout 30\rrouter_id LVS_DEVEL\rvrrp_skip_check_adv_addr\rvrrp_garp_interval 0\rvrrp_gna_interval 0\r}\r# 定义脚本\rvrrp_script check_apiserver {\rscript \u0026quot;/etc/keepalived/check_apiserver.sh\u0026quot; interval 2 weight -5 fall 3 rise 2 }\rvrrp_instance VI_1 {\rstate MASTER\rinterface ens33\rvirtual_router_id 51\rpriority 98 advert_int 1\rauthentication {\rauth_type PASS\rauth_pass 1111\r}\rvirtual_ipaddress {\r10.1.10.200\r}\r# 调用脚本\r#track_script {\r# check_apiserver\r#}\r}\r 先把健康检查关闭，等部署好了过后再打开\n 编写健康检测脚本check-apiserver.sh\n#!/bin/bash\rfunction check_apiserver(){\rfor ((i=0;i\u0026lt;5;i++))\rdo\rapiserver_job_id=${pgrep kube-apiserver}\rif [[ ! -z ${apiserver_job_id} ]];then\rreturn\relse\rsleep 2\rfi\rdone\rapiserver_job_id=0\r}\r# 1-\u0026gt;running 0-\u0026gt;stopped\rcheck_apiserver\rif [[ $apiserver_job_id -eq 0 ]];then\r/usr/bin/systemctl stop keepalived\rexit 1\relse\rexit 0\rfi\r启动haproxy和keepalived\nsystemctl enable --now keepalived\rsystemctl enable --now haproxy\r部署master （1）、在k8s-master01上，编写kubeadm.yaml配置文件，如下：\ncat \u0026gt;\u0026gt; kubeadm.yaml \u0026lt;\u0026lt;EOF\rapiVersion: kubeadm.k8s.io/v1beta2\rkind: ClusterConfiguration\rkubernetesVersion: v1.18.2\rimageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers\rcontrolPlaneEndpoint: \u0026quot;k8s-lb:16443\u0026quot;\rnetworking:\rdnsDomain: cluster.local\rpodSubnet: 192.168.0.0/16\rserviceSubnet: 10.96.0.0/12\r---\rapiVersion: kubeproxy.config.k8s.io/v1alpha1\rkind: KubeProxyConfiguration\rfeatureGates:\rSupportIPVSProxyMode: true\rmode: ipvs\rEOF\r提前下载镜像\nkubeadm config images pull --config kubeadm.yaml\r进行初始化\nkubeadm init --config kubeadm.yaml --upload-certs\rW0509 22:37:40.702752 65728 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]\r[init] Using Kubernetes version: v1.18.2\r[preflight] Running pre-flight checks\r[WARNING IsDockerSystemdCheck]: detected \u0026quot;cgroupfs\u0026quot; as the Docker cgroup driver. The recommended driver is \u0026quot;systemd\u0026quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/\r[preflight] Pulling images required for setting up a Kubernetes cluster\r[preflight] This might take a minute or two, depending on the speed of your internet connection\r[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\r[kubelet-start] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot;\r[kubelet-start] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot;\r[kubelet-start] Starting the kubelet\r[certs] Using certificateDir folder \u0026quot;/etc/kubernetes/pki\u0026quot;\r[certs] Generating \u0026quot;ca\u0026quot; certificate and key\r[certs] Generating \u0026quot;apiserver\u0026quot; certificate and key\r[certs] apiserver serving cert is signed for DNS names [k8s-master01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local k8s-lb] and IPs [10.96.0.1 10.1.10.100]\r[certs] Generating \u0026quot;apiserver-kubelet-client\u0026quot; certificate and key\r[certs] Generating \u0026quot;front-proxy-ca\u0026quot; certificate and key\r[certs] Generating \u0026quot;front-proxy-client\u0026quot; certificate and key\r[certs] Generating \u0026quot;etcd/ca\u0026quot; certificate and key\r[certs] Generating \u0026quot;etcd/server\u0026quot; certificate and key\r[certs] etcd/server serving cert is signed for DNS names [k8s-master01 localhost] and IPs [10.1.10.100 127.0.0.1 ::1]\r[certs] Generating \u0026quot;etcd/peer\u0026quot; certificate and key\r[certs] etcd/peer serving cert is signed for DNS names [k8s-master01 localhost] and IPs [10.1.10.100 127.0.0.1 ::1]\r[certs] Generating \u0026quot;etcd/healthcheck-client\u0026quot; certificate and key\r[certs] Generating \u0026quot;apiserver-etcd-client\u0026quot; certificate and key\r[certs] Generating \u0026quot;sa\u0026quot; key and public key\r[kubeconfig] Using kubeconfig folder \u0026quot;/etc/kubernetes\u0026quot;\r[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address\r[kubeconfig] Writing \u0026quot;admin.conf\u0026quot; kubeconfig file\r[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address\r[kubeconfig] Writing \u0026quot;kubelet.conf\u0026quot; kubeconfig file\r[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address\r[kubeconfig] Writing \u0026quot;controller-manager.conf\u0026quot; kubeconfig file\r[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address\r[kubeconfig] Writing \u0026quot;scheduler.conf\u0026quot; kubeconfig file\r[control-plane] Using manifest folder \u0026quot;/etc/kubernetes/manifests\u0026quot;\r[control-plane] Creating static Pod manifest for \u0026quot;kube-apiserver\u0026quot;\r[control-plane] Creating static Pod manifest for \u0026quot;kube-controller-manager\u0026quot;\rW0509 22:37:47.750722 65728 manifests.go:225] the default kube-apiserver authorization-mode is \u0026quot;Node,RBAC\u0026quot;; using \u0026quot;Node,RBAC\u0026quot;\r[control-plane] Creating static Pod manifest for \u0026quot;kube-scheduler\u0026quot;\rW0509 22:37:47.764989 65728 manifests.go:225] the default kube-apiserver authorization-mode is \u0026quot;Node,RBAC\u0026quot;; using \u0026quot;Node,RBAC\u0026quot;\r[etcd] Creating static Pod manifest for local etcd in \u0026quot;/etc/kubernetes/manifests\u0026quot;\r[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026quot;/etc/kubernetes/manifests\u0026quot;. This can take up to 4m0s\r[apiclient] All control plane components are healthy after 20.024575 seconds\r[upload-config] Storing the configuration used in ConfigMap \u0026quot;kubeadm-config\u0026quot; in the \u0026quot;kube-system\u0026quot; Namespace\r[kubelet] Creating a ConfigMap \u0026quot;kubelet-config-1.18\u0026quot; in namespace kube-system with the configuration for the kubelets in the cluster\r[upload-certs] Storing the certificates in Secret \u0026quot;kubeadm-certs\u0026quot; in the \u0026quot;kube-system\u0026quot; Namespace\r[upload-certs] Using certificate key:\rf25e738324e4f027703f24b55d47d28f692b4edc21c2876171ff87877dc8f2ef\r[mark-control-plane] Marking the node k8s-master01 as control-plane by adding the label \u0026quot;node-role.kubernetes.io/master=''\u0026quot;\r[mark-control-plane] Marking the node k8s-master01 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]\r[bootstrap-token] Using token: 3k4vr0.x3y2nc3ksfnei4y1\r[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\r[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes\r[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\r[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\r[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\r[bootstrap-token] Creating the \u0026quot;cluster-info\u0026quot; ConfigMap in the \u0026quot;kube-public\u0026quot; namespace\r[kubelet-finalize] Updating \u0026quot;/etc/kubernetes/kubelet.conf\u0026quot; to point to a rotatable kubelet client certificate and key\r[addons] Applied essential addon: CoreDNS\r[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address\r[addons] Applied essential addon: kube-proxy\rYour Kubernetes control-plane has initialized successfully!\rTo start using your cluster, you need to run the following as a regular user:\rmkdir -p $HOME/.kube\rsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\rsudo chown $(id -u):$(id -g) $HOME/.kube/config\rYou should now deploy a pod network to the cluster.\rRun \u0026quot;kubectl apply -f [podnetwork].yaml\u0026quot; with one of the options listed at:\rhttps://kubernetes.io/docs/concepts/cluster-administration/addons/\rYou can now join any number of the control-plane node running the following command on each as root:\rkubeadm join k8s-lb:16443 --token 3k4vr0.x3y2nc3ksfnei4y1 \\\r--discovery-token-ca-cert-hash sha256:a5f761f332bd45a199d0676875e7f58c323226df6fb9b4f0b977b6f63b252791 \\\r--control-plane --certificate-key f25e738324e4f027703f24b55d47d28f692b4edc21c2876171ff87877dc8f2ef\rPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!\rAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use\r\u0026quot;kubeadm init phase upload-certs --upload-certs\u0026quot; to reload certs afterward.\rThen you can join any number of worker nodes by running the following on each as root:\rkubeadm join k8s-lb:16443 --token 3k4vr0.x3y2nc3ksfnei4y1 \\\r--discovery-token-ca-cert-hash sha256:a5f761f332bd45a199d0676875e7f58c323226df6fb9b4f0b977b6f63b252791 配置环境变量\ncat \u0026gt;\u0026gt; /root/.bashrc \u0026lt;\u0026lt;EOF\rexport KUBECONFIG=/etc/kubernetes/admin.conf\rEOF\rsource /root/.bashrc\r查看节点状态\n# kubectl get nodes\rNAME STATUS ROLES AGE VERSION\rk8s-master01 NotReady master 3m1s v1.18.2\r安装网络插件\nwget https://docs.projectcalico.org/v3.8/manifests/calico.yaml\r如果有节点是多网卡，所以需要在资源清单文件中指定内网网卡\nvi calico.yaml\n......\rspec:\rcontainers:\r- env:\r- name: DATASTORE_TYPE\rvalue: kubernetes\r- name: IP_AUTODETECTION_METHOD # DaemonSet中添加该环境变量\rvalue: interface=ens33 # 指定内网网卡\r- name: WAIT_FOR_DATASTORE\rvalue: \u0026quot;true\u0026quot;\r......\rkubectl apply -f calico.yaml # 安装calico网络插件\n当网络插件安装完成后，查看node节点信息如下：\n# kubectl get nodes\rNAME STATUS ROLES AGE VERSION\rk8s-master01 Ready master 10m v1.18.2\r可以看到状态已经从NotReady变为ready了。\n（2）、将master02加入集群\n提前下载镜像\nkubeadm config images pull --config kubeadm.yaml\r加入集群\n kubeadm join k8s-lb:16443 --token 3k4vr0.x3y2nc3ksfnei4y1 \\\r--discovery-token-ca-cert-hash sha256:a5f761f332bd45a199d0676875e7f58c323226df6fb9b4f0b977b6f63b252791 \\\r--control-plane --certificate-key f25e738324e4f027703f24b55d47d28f692b4edc21c2876171ff87877dc8f2ef\r输出如下：\n...\rThis node has joined the cluster and a new control plane instance was created:\r* Certificate signing request was sent to apiserver and approval was received.\r* The Kubelet was informed of the new secure connection details.\r* Control plane (master) label and taint were applied to the new node.\r* The Kubernetes control plane instances scaled up.\r* A new etcd member was added to the local/stacked etcd cluster.\rTo start administering your cluster from this node, you need to run the following as a regular user:\rmkdir -p $HOME/.kube\rsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\rsudo chown $(id -u):$(id -g) $HOME/.kube/config\rRun 'kubectl get nodes' to see this node join the cluster.\r...\r配置环境变量\ncat \u0026gt;\u0026gt; /root/.bashrc \u0026lt;\u0026lt;EOF\rexport KUBECONFIG=/etc/kubernetes/admin.conf\rEOF\rsource /root/.bashrc\r另一台的操作一样。\n查看集群状态\n# kubectl get nodes NAME STATUS ROLES AGE VERSION\rk8s-master01 Ready master 41m v1.18.2\rk8s-master02 Ready master 29m v1.18.2\rk8s-master03 Ready master 27m v1.18.2\r查看集群组件状态\n# kubectl get pod -n kube-system -o wide\rNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES\rcalico-kube-controllers-77c5fc8d7f-stl57 1/1 Running 0 26m 192.168.32.130 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rcalico-node-ppsph 1/1 Running 0 26m 10.1.10.100 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rcalico-node-tl6sq 0/1 Init:2/3 0 26m 10.1.10.101 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rcalico-node-w92qh 1/1 Running 0 26m 10.1.10.102 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rcoredns-546565776c-vtlhr 1/1 Running 0 42m 192.168.32.129 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rcoredns-546565776c-wz9bk 1/1 Running 0 42m 192.168.32.131 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\retcd-k8s-master01 1/1 Running 0 42m 10.1.10.100 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\retcd-k8s-master02 1/1 Running 0 30m 10.1.10.101 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\retcd-k8s-master03 1/1 Running 0 28m 10.1.10.102 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-apiserver-k8s-master01 1/1 Running 0 42m 10.1.10.100 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-apiserver-k8s-master02 1/1 Running 0 30m 10.1.10.101 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-apiserver-k8s-master03 1/1 Running 0 28m 10.1.10.102 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-controller-manager-k8s-master01 1/1 Running 1 42m 10.1.10.100 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-controller-manager-k8s-master02 1/1 Running 1 30m 10.1.10.101 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-controller-manager-k8s-master03 1/1 Running 0 28m 10.1.10.102 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-proxy-6sbpp 1/1 Running 0 28m 10.1.10.102 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-proxy-dpppr 1/1 Running 0 42m 10.1.10.100 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-proxy-ln7l7 1/1 Running 0 30m 10.1.10.101 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-scheduler-k8s-master01 1/1 Running 1 42m 10.1.10.100 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-scheduler-k8s-master02 1/1 Running 1 30m 10.1.10.101 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-scheduler-k8s-master03 1/1 Running 0 28m 10.1.10.102 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\r查看CSR\n kubectl get csr\rNAME AGE SIGNERNAME REQUESTOR CONDITION\rcsr-cfl2w 42m kubernetes.io/kube-apiserver-client-kubelet system:node:k8s-master01 Approved,Issued\rcsr-mm7g7 28m kubernetes.io/kube-apiserver-client-kubelet system:bootstrap:3k4vr0 Approved,Issued\rcsr-qzn6r 30m kubernetes.io/kube-apiserver-client-kubelet system:bootstrap:3k4vr0 Approved,Issued\r部署node node节点只需加入集群即可\nkubeadm join k8s-lb:16443 --token 3k4vr0.x3y2nc3ksfnei4y1 \\\r--discovery-token-ca-cert-hash sha256:a5f761f332bd45a199d0676875e7f58c323226df6fb9b4f0b977b6f63b252791 输出日志如下：\nW0509 23:24:12.159733 10635 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.\r[preflight] Running pre-flight checks\r[WARNING IsDockerSystemdCheck]: detected \u0026quot;cgroupfs\u0026quot; as the Docker cgroup driver. The recommended driver is \u0026quot;systemd\u0026quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/\r[preflight] Reading configuration from the cluster...\r[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'\r[kubelet-start] Downloading configuration for the kubelet from the \u0026quot;kubelet-config-1.18\u0026quot; ConfigMap in the kube-system namespace\r[kubelet-start] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot;\r[kubelet-start] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot;\r[kubelet-start] Starting the kubelet\r[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...\rThis node has joined the cluster:\r* Certificate signing request was sent to apiserver and a response was received.\r* The Kubelet was informed of the new secure connection details.\rRun 'kubectl get nodes' on the control-plane to see this node join the cluster.\r然后查看集群节点信息\n# kubectl get nodes NAME STATUS ROLES AGE VERSION\rk8s-master01 Ready master 47m v1.18.2\rk8s-master02 Ready master 35m v1.18.2\rk8s-master03 Ready master 32m v1.18.2\rk8s-node01 Ready node01 55s v1.18.2\r测试切换 关闭一台master主机，看集群是否可用。\n关闭master01主机，然后查看整个集群。\n# 模拟关掉keepalived\rsystemctl stop keepalived\r# 然后查看集群是否可用\r[root@k8s-master03 ~]# kubectl get nodes\rNAME STATUS ROLES AGE VERSION\rk8s-master01 Ready master 64m v1.18.2\rk8s-master02 Ready master 52m v1.18.2\rk8s-master03 Ready master 50m v1.18.2\rk8s-node01 Ready \u0026lt;none\u0026gt; 18m v1.18.2\r[root@k8s-master03 ~]# kubectl get pod -n kube-system\rNAME READY STATUS RESTARTS AGE\rcalico-kube-controllers-77c5fc8d7f-stl57 1/1 Running 0 49m\rcalico-node-8t5ft 1/1 Running 0 19m\rcalico-node-ppsph 1/1 Running 0 49m\rcalico-node-tl6sq 1/1 Running 0 49m\rcalico-node-w92qh 1/1 Running 0 49m\rcoredns-546565776c-vtlhr 1/1 Running 0 65m\rcoredns-546565776c-wz9bk 1/1 Running 0 65m\retcd-k8s-master01 1/1 Running 0 65m\retcd-k8s-master02 1/1 Running 0 53m\retcd-k8s-master03 1/1 Running 0 51m\rkube-apiserver-k8s-master01 1/1 Running 0 65m\rkube-apiserver-k8s-master02 1/1 Running 0 53m\rkube-apiserver-k8s-master03 1/1 Running 0 51m\rkube-controller-manager-k8s-master01 1/1 Running 2 65m\rkube-controller-manager-k8s-master02 1/1 Running 1 53m\rkube-controller-manager-k8s-master03 1/1 Running 0 51m\rkube-proxy-6sbpp 1/1 Running 0 51m\rkube-proxy-dpppr 1/1 Running 0 65m\rkube-proxy-ln7l7 1/1 Running 0 53m\rkube-proxy-r5ltk 1/1 Running 0 19m\rkube-scheduler-k8s-master01 1/1 Running 2 65m\rkube-scheduler-k8s-master02 1/1 Running 1 53m\rkube-scheduler-k8s-master03 1/1 Running 0 51m\r 到此集群搭建完了，然后可以开启keepalived的检查脚本了。另外一些组件就自己自行安装。\n 安装自动补全命令 yum install -y bash-completion\rsource /usr/share/bash-completion/bash_completion\rsource \u0026lt;(kubectl completion bash)\recho \u0026quot;source \u0026lt;(kubectl completion bash)\u0026quot; \u0026gt;\u0026gt; ~/.bashrc\r","description":"使用kubeadm搭建高可用kubernetes集群","id":35,"section":"posts","tags":["kubernetes"],"title":"Kubeadm搭建高可用Kubernetes集群","uri":"https://www.coolops.cn/posts/kubeadm-install-multi-master-kubernetes/"},{"content":"备份 备份策略：\n 每两个小时用命令对etcd进行备份 备份数据保留2天  ETCDCTL_API=3 /opt/etcd/bin/etcdctl snapshot save /root/backup/etcd_$(date \u0026quot;+%Y%m%d%H%M%S\u0026quot;).db 恢复 （1）、停止kube-apiserver，确保不会再写入数据\nsystemctl stop kube-apiserver （2）、停止所有节点etcd\nsystemctl stop etcd （3）、将etcd节点上原有的数据目录备份（具体的目录可以在etcd的配置文件中查看）\nmv /var/lib/etcd/default.etcd{,.bak} （4）、将备份的数据拷贝到所有etcd节点\nscp etcd_20200106152240.db 10.1.10.129:/root/backup/ scp etcd_20200106152240.db 10.1.10.130:/root/backup/ （5）、使用命令进行恢复\n# 在etcd-1上 ETCDCTL_API=3 /opt/etcd/bin/etcdctl snapshot --endpoints=\u0026quot;https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379\u0026quot; --cacert=/opt/etcd/ssl/etcd-ca.pem --cert=/opt/etcd/ssl/etcd-server.pem --key=/opt/etcd/ssl/etcd-server-key.pem restore ~/backup/etcd_20200106152240.db --name=etcd-1 --data-dir=/var/lib/etcd/default.etcd --initial-cluster=\u0026quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380,etcd-3=https://10.1.10.130:2380\u0026quot; --initial-cluster-token=\u0026quot;etcd-cluster\u0026quot; --initial-advertise-peer-urls=https://10.1.10.128:2380 # 在etcd-2上 ETCDCTL_API=3 /opt/etcd/bin/etcdctl snapshot --endpoints=\u0026quot;https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379\u0026quot; --cacert=/opt/etcd/ssl/etcd-ca.pem --cert=/opt/etcd/ssl/etcd-server.pem --key=/opt/etcd/ssl/etcd-server-key.pem restore ~/backup/etcd_20200106152240.db --name=etcd-2 --data-dir=/var/lib/etcd/default.etcd --initial-cluster=\u0026quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380,etcd-3=https://10.1.10.130:2380\u0026quot; --initial-cluster-token=\u0026quot;etcd-cluster\u0026quot; --initial-advertise-peer-urls=https://10.1.10.129:2380 # 在etcd-3上 ETCDCTL_API=3 /opt/etcd/bin/etcdctl snapshot --endpoints=\u0026quot;https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379\u0026quot; --cacert=/opt/etcd/ssl/etcd-ca.pem --cert=/opt/etcd/ssl/etcd-server.pem --key=/opt/etcd/ssl/etcd-server-key.pem restore ~/backup/etcd_20200106152240.db --name=etcd-3 --data-dir=/var/lib/etcd/default.etcd --initial-cluster=\u0026quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380,etcd-3=https://10.1.10.130:2380\u0026quot; --initial-cluster-token=\u0026quot;etcd-cluster\u0026quot; --initial-advertise-peer-urls=https://10.1.10.130:2380 （6）、启动etcd服务\nsystemctl start etcd （7）、启动kube-apiserver\nsystemctl start kube-apiserver （8）、查看集群状态\n# /opt/etcd/bin/etcdctl \\ \u0026gt; --ca-file=/opt/etcd/ssl/etcd-ca.pem --cert-file=/opt/etcd/ssl/etcd-server.pem --key-file=/opt/etcd/ssl/etcd-server-key.pem \\ \u0026gt; --endpoints=\u0026quot;https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379\u0026quot; \\ \u0026gt; cluster-health member a2dba8836695bcf6 is healthy: got healthy result from https://10.1.10.129:2379 member d1272b0b3cb41282 is healthy: got healthy result from https://10.1.10.128:2379 member e4a3a9c93ef84f2d is healthy: got healthy result from https://10.1.10.130:2379 cluster is healthy # kubectl get node NAME STATUS ROLES AGE VERSION master-k8s Ready \u0026lt;none\u0026gt; 4h9m v1.16.4 node01-k8s Ready \u0026lt;none\u0026gt; 40h v1.16.4 node02-k8s Ready \u0026lt;none\u0026gt; 39h v1.16.4 # kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE coredns-9d5b6bdb6-mpwht 1/1 Running 0 38h kube-flannel-ds-amd64-2qkcb 1/1 Running 0 38h kube-flannel-ds-amd64-7nzj5 1/1 Running 0 38h kube-flannel-ds-amd64-hlfdf 1/1 Running 0 4h9m metrics-server-v0.3.6-6c57d48cb4-tzjc7 2/2 Running 0 3h53m traefik-ingress-controller-7758594f89-lwf2t 1/1 Running 0 16h 剔除 如果我们要剔除某个节点，我们可以通过下面步骤进行。\n（1）、查看现有节点的member信息\n /opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/etcd-ca.pem --cert-file=/opt/etcd/ssl/etcd-server.pem --key-file=/opt/etcd/ssl/etcd-server-key.pem member list a2dba8836695bcf6: name=etcd-2 peerURLs=https://10.1.10.129:2380 clientURLs=https://10.1.10.129:2379 isLeader=false d1272b0b3cb41282: name=etcd-1 peerURLs=https://10.1.10.128:2380 clientURLs=https://10.1.10.128:2379 isLeader=true e4a3a9c93ef84f2d: name=etcd-3 peerURLs=https://10.1.10.130:2380 clientURLs=https://10.1.10.130:2379 isLeader=false （2）、根据member信息移除相应的实例\n# /opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/etcd-ca.pem --cert-file=/opt/etcd/ssl/etcd-server.pem --key-file=/opt/etcd/ssl/etcd-server-key.pem member remove e4a3a9c93ef84f2d Removed member e4a3a9c93ef84f2d from cluster （3）、停止被移除节点的etcd\nsystemctl stop etcd （4）、修改现有etcd集群的配置文件，移除被踢掉的etcd集群\n... ETCD_INITIAL_CLUSTER=\u0026quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380\u0026quot; ... （5）、重启现有集群的etcd\nsystemctl restart etcd （6）、查看集群状态\n# /opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/etcd-ca.pem --cert-file=/opt/etcd/ssl/etcd-server.pem --key-file=/opt/etcd/ssl/etcd-server-key.pem member list a2dba8836695bcf6: name=etcd-2 peerURLs=https://10.1.10.129:2380 clientURLs=https://10.1.10.129:2379 isLeader=false d1272b0b3cb41282: name=etcd-1 peerURLs=https://10.1.10.128:2380 clientURLs=https://10.1.10.128:2379 isLeader=true # /opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/etcd-ca.pem --cert-file=/opt/etcd/ssl/etcd-server.pem --key-file=/opt/etcd/ssl/etcd-server-key.pem --endpoints=\u0026quot;https://10.1.10.128:2379,https://10.1.10.129:2379\u0026quot; cluster-health member a2dba8836695bcf6 is healthy: got healthy result from https://10.1.10.129:2379 member d1272b0b3cb41282 is healthy: got healthy result from https://10.1.10.128:2379 cluster is healthy 新增 如果我们需要新增一个etcd节点，则可以按照以下步骤进行。\n（1）、通过以下命令新增节点\n# /opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/etcd-ca.pem --cert-file=/opt/etcd/ssl/etcd-server.pem --key-file=/opt/etcd/ssl/etcd-server-key.pem member add etcd-3 https://10.1.10.130:2380 Added member named etcd-3 with ID 16ebaad9c9a3a3e3 to cluster ETCD_NAME=\u0026quot;etcd-3\u0026quot; ETCD_INITIAL_CLUSTER=\u0026quot;etcd-3=https://10.1.10.130:2380,etcd-2=https://10.1.10.129:2380,etcd-1=https://10.1.10.128:2380\u0026quot; ETCD_INITIAL_CLUSTER_STATE=\u0026quot;existing\u0026quot;  注意：\n etcd_name: etcd.conf配置文件中ETCD_NAME内容 etdc_node_address: etcd.conf配置文件中的ETCD_LISTEN_PEER_URLS内容   （2）、删除新增成员旧数据目录，并且启动新增成员etcd服务，加入集群时要改下配置文件，把初始化集群状态由new改成existing，如下\n#[Member] ETCD_NAME=\u0026quot;etcd-3\u0026quot; ETCD_DATA_DIR=\u0026quot;/var/lib/etcd/default.etcd\u0026quot; ETCD_LISTEN_PEER_URLS=\u0026quot;https://10.1.10.130:2380\u0026quot; ETCD_LISTEN_CLIENT_URLS=\u0026quot;https://10.1.10.130:2379\u0026quot; #[Clustering] ETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026quot;https://10.1.10.130:2380\u0026quot; ETCD_ADVERTISE_CLIENT_URLS=\u0026quot;https://10.1.10.130:2379\u0026quot; ETCD_INITIAL_CLUSTER=\u0026quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380,etcd-3=https://10.1.10.130:2380\u0026quot; ETCD_INITIAL_CLUSTER_TOKEN=\u0026quot;etcd-cluster\u0026quot; ETCD_INITIAL_CLUSTER_STATE=\u0026quot;existing\u0026quot; （3）、还需要修改systemd unit文件中的参数，如下：\n...... --initial-cluster-state=existing \\ ...... （4）、在现存etcd的配置文件中修改\nETCD_INITIAL_CLUSTER=\u0026quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380,etcd-3=https://10.1.10.130:2380\u0026quot; （5）、重启etcd\nsystemctl restart etcd （6）、查看状态\n1 2 3 4 5 6 7 8 9 10 11 12  # /opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/etcd-ca.pem --cert-file=/opt/etcd/ssl/etcd-server.pem --key-file=/opt/etcd/ssl/etcd-server-key.pem member list 7d38a6cc82b63e33: name=etcd-3 peerURLs=https://10.1.10.130:2380 clientURLs=https://10.1.10.130:2379 isLeader=false a2dba8836695bcf6: name=etcd-2 peerURLs=https://10.1.10.129:2380 clientURLs=https://10.1.10.129:2379 isLeader=true d1272b0b3cb41282: name=etcd-1 peerURLs=https://10.1.10.128:2380 clientURLs=https://10.1.10.128:2379 isLeader=false # /opt/etcd/bin/etcdctl \\ \u0026gt; --ca-file=/opt/etcd/ssl/etcd-ca.pem --cert-file=/opt/etcd/ssl/etcd-server.pem --key-file=/opt/etcd/ssl/etcd-server-key.pem \\ \u0026gt; --endpoints=\u0026#34;https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379\u0026#34; \\ \u0026gt; cluster-health member 7d38a6cc82b63e33 is healthy: got healthy result from https://10.1.10.130:2379 member a2dba8836695bcf6 is healthy: got healthy result from https://10.1.10.129:2379 member d1272b0b3cb41282 is healthy: got healthy result from https://10.1.10.128:2379   ","description":"etcd集群的备份、删除、新增等操作","id":36,"section":"posts","tags":["etcd"],"title":"Etcd集群常用操作","uri":"https://www.coolops.cn/posts/handle-etcd-cluster/"},{"content":"基础规划 1、IP规划\n   主机名 IP 配置 软件     master-k8s 10.1.10.128 2C4G etcd,apiserver,controller-manager,scheduler   node01-k8s 10.1.10.129 2C4G etcd,docker,kubelet,kube-proxy   node02-k8s 10.1.10.130 2C4G etcd,docker,kubelet,kube-proxy    2、软件规划\n   软件名 版本     etcd 3.3.18   docker-ce 19.03.5-3   cfssl 1.2.0   kubernetes 1.16.4   flannel 0.11.0   cni 0.8.3    3、目录规划\n   目录名 用途     /var/log/kubernetes/ 存储日志   /root/kubernetes/install 安装软件目录   /opt/kubernetes K8S项目部署目录，其中ssl是证书目录，bin是二进制目录，config是配置文件目录   /opt/etcd Etcd项目部署目录，子目录功能如上   /opt/cni cni二进制文件保存目录   /opt/kubernetes/ssl 证书生成目录   /opt/kubernetes/kubeconfig kubeconfig统一生成目录   /opt/kubernetes/system 系统组件YAML文件存储目录    mkdir /var/log/kubernetes /root/kubernetes/{ssl,install,kubeconfig} /root/kubernetes/ssl /opt/etcd/{bin,config,ssl} /opt/kubernetes/{bin,config,ssl} /opt/cni/bin -p 主机初始化配置 2、设置hostname\n# 10.1.10.128 hostnamectl set-hostname master-k8s # 10.1.10.129 hostnamectl set-hostname node01-k8s # 10.1.10.130 hostnamectl set-hostname node02-k8s 3、配置Hosts（/etc/hosts）\ncat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt;EOF 10.1.10.128 master-k8s 10.1.10.129 node01-k8s 10.1.10.130 node02-k8s EOF 4、初始化\n关闭防火墙\nsystemctl stop firewalld systemctl disable firewalld 关闭SELINUX\nsetenforce 0 sed -i \u0026quot;s/SELINUX=enforcing/SELINUX=disabled/g\u0026quot; /etc/sysconfig/selinux 刷新yum缓存\nyum clean all yum makecache 修改内核参数\ncat \u0026gt; /etc/sysctl.d/k8s.conf \u0026lt;\u0026lt;EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 vm.swappiness=0 EOF modprobe br_netfilter sysctl -p /etc/sysctl.d/k8s.conf 安装IPVS\ncat \u0026gt; /etc/sysconfig/modules/ipvs.modules \u0026lt;\u0026lt;EOF #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOF chmod 755 /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; bash /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4 yum install ipset ipvsadm -y 同步服务器时间\n master\n #安装chrony： yum -y install chrony #注释默认ntp服务器 sed -i 's/^server/#\u0026amp;/' /etc/chrony.conf #指定上游公共 ntp 服务器，并允许其他节点同步时间 cat \u0026gt;\u0026gt; /etc/chrony.conf \u0026lt;\u0026lt; EOF server 0.asia.pool.ntp.org iburst server 1.asia.pool.ntp.org iburst server 2.asia.pool.ntp.org iburst server 3.asia.pool.ntp.org iburst allow all EOF #重启chronyd服务并设为开机启动： systemctl enable chronyd \u0026amp;\u0026amp; systemctl restart chronyd #开启网络时间同步功能 timedatectl set-ntp true  slave\n #安装chrony： yum -y install chrony #注释默认服务器 sed -i 's/^server/#\u0026amp;/' /etc/chrony.conf #指定内网 master节点为上游NTP服务器 echo 'server 10.1.10.128 iburst' \u0026gt;\u0026gt; /etc/chrony.conf #重启服务并设为开机启动： systemctl enable chronyd \u0026amp;\u0026amp; systemctl restart chronyd 关闭SWAP分区\nswapoff -a sed -i \u0026quot;s/\\/dev\\/mapper\\/centos-swap/#\\/dev\\/mapper\\/centos-swap/g\u0026quot; /etc/fstab 安装docker yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum makecache fast yum install docker-ce -y systemctl start docker systemctl enable docker 配置镜像加速（）\ncurl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io systemctl restart docker 安装其他软件：\nyum install unzip wget lrzsz -y 优化：\nvi daemon.json { \u0026quot;max-concurrent-downloads\u0026quot;: 20, \u0026quot;log-driver\u0026quot;: \u0026quot;json-file\u0026quot;, \u0026quot;bridge\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;oom-score-adjust\u0026quot;: -1000, \u0026quot;debug\u0026quot;: false, \u0026quot;log-opts\u0026quot;: { \u0026quot;max-size\u0026quot;: \u0026quot;100M\u0026quot;, \u0026quot;max-file\u0026quot;: \u0026quot;10\u0026quot; }, \u0026quot;default-ulimits\u0026quot;: { \u0026quot;nofile\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;nofile\u0026quot;, \u0026quot;Hard\u0026quot;: 65535, \u0026quot;Soft\u0026quot;: 65535 }, \u0026quot;nproc\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;nproc\u0026quot;, \u0026quot;Hard\u0026quot;: 65535, \u0026quot;Soft\u0026quot;: 65535 }, \u0026quot;core\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;core\u0026quot;, \u0026quot;Hard\u0026quot;: -1, \u0026quot;Soft\u0026quot;: -1 } } } 安装cfssl证书生成工具 curl -L https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o /usr/local/bin/cfssl curl -L https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o /usr/local/bin/cfssljson curl -L https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -o /usr/local/bin/cfssl-certinfo chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson /usr/local/bin/cfssl-certinfo 搭建ETCD集群 下载地址：https://github.com/etcd-io/etcd/releases/download/v3.3.18/etcd-v3.3.18-linux-amd64.tar.gz\nwget https://github.com/etcd-io/etcd/releases/download/v3.3.18/etcd-v3.3.18-linux-amd64.tar.gz 生成ETCD证书 证书生成的目录统一下/root/kubernetes/ssl/下\nmkdir /root/kubernetes/ssl/etcd -p \u0026amp;\u0026amp; cd /root/kubernetes/ssl/etcd （1）、创建CA的请求文件（etcd-ca-csr.json）\ncat \u0026gt; etcd-ca-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;etcd\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot; } ] } EOF （2）、创建CA的配置文件（etcd-ca-config.json）\ncat \u0026gt; etcd-ca-config.json \u0026lt;\u0026lt;EOF { \u0026quot;signing\u0026quot;: { \u0026quot;default\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;87600h\u0026quot; }, \u0026quot;profiles\u0026quot;: { \u0026quot;www\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;87600h\u0026quot;, \u0026quot;usages\u0026quot;: [ \u0026quot;signing\u0026quot;, \u0026quot;key encipherment\u0026quot;, \u0026quot;server auth\u0026quot;, \u0026quot;client auth\u0026quot; ] } } } } EOF （3）、创建CA证书\ncfssl gencert -initca etcd-ca-csr.json | cfssljson -bare etcd-ca - （4）、创建etcd证书请求文件（etcd-server-csr.json）：\ncat \u0026gt; etcd-server-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;etcd\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;10.1.10.128\u0026quot;, \u0026quot;10.1.10.129\u0026quot;, \u0026quot;10.1.10.130\u0026quot; ], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot; } ] } EOF （5）、生成etcd证书并用 CA签名\ncfssl gencert -ca=etcd-ca.pem -ca-key=etcd-ca-key.pem -config=etcd-ca-config.json -profile=www etcd-server-csr.json | cfssljson -bare etcd-server # ls *.pem ca-key.pem ca.pem etcd-key.pem etcd.pem # cp *.pem /opt/etcd/ssl/ 安装ETCD 解压安装包：\ntar xf etcd-v3.3.18-linux-amd64.tar.gz cp etcd etcdctl /opt/etcd/bin/ 创建配置文件（etcd.conf）\ncat \u0026gt; /opt/etcd/config/etcd.conf \u0026lt;\u0026lt;EOF #[Member] ETCD_NAME=\u0026quot;etcd-1\u0026quot; ETCD_DATA_DIR=\u0026quot;/var/lib/etcd/default.etcd\u0026quot; ETCD_LISTEN_PEER_URLS=\u0026quot;https://10.1.10.128:2380\u0026quot; ETCD_LISTEN_CLIENT_URLS=\u0026quot;https://10.1.10.128:2379\u0026quot; #[Clustering] ETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026quot;https://10.1.10.128:2380\u0026quot; ETCD_ADVERTISE_CLIENT_URLS=\u0026quot;https://10.1.10.128:2379\u0026quot; ETCD_INITIAL_CLUSTER=\u0026quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380,etcd-3=https://10.1.10.130:2380\u0026quot; ETCD_INITIAL_CLUSTER_TOKEN=\u0026quot;etcd-cluster\u0026quot; ETCD_INITIAL_CLUSTER_STATE=\u0026quot;new\u0026quot; EOF  注意：相应的地址按需更改\nETCD_NAME：三台不能相同\nip地址不能相同\n 创建etcd的启动文件etcd.service\ncat \u0026gt; /usr/lib/systemd/system/etcd.service \u0026lt;\u0026lt;EOF [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify EnvironmentFile=/opt/etcd/config/etcd.conf ExecStart=/opt/etcd/bin/etcd \\\\ --name=\\${ETCD_NAME} \\\\ --data-dir=\\${ETCD_DATA_DIR} \\\\ --listen-peer-urls=\\${ETCD_LISTEN_PEER_URLS} \\\\ --listen-client-urls=\\${ETCD_LISTEN_CLIENT_URLS},http://127.0.0.1:2379 \\\\ --advertise-client-urls=\\${ETCD_ADVERTISE_CLIENT_URLS} \\\\ --initial-advertise-peer-urls=\\${ETCD_INITIAL_ADVERTISE_PEER_URLS} \\\\ --initial-cluster=\\${ETCD_INITIAL_CLUSTER} \\\\ --initial-cluster-token=\\${ETCD_INITIAL_CLUSTER_TOKEN} \\\\ --initial-cluster-state=new \\\\ --cert-file=/opt/etcd/ssl/etcd-server.pem \\\\ --key-file=/opt/etcd/ssl/etcd-server-key.pem \\\\ --peer-cert-file=/opt/etcd/ssl/etcd-server.pem \\\\ --peer-key-file=/opt/etcd/ssl/etcd-server-key.pem \\\\ --trusted-ca-file=/opt/etcd/ssl/etcd-ca.pem \\\\ --peer-trusted-ca-file=/opt/etcd/ssl/etcd-ca.pem Restart=on-failure LimitNOFILE=65536 [Install] WantedBy=multi-user.target EOF 另外两天部署一样，只有配置文件需要更改一下，将文件拷贝到另外两台：\nscp -r /opt/etcd 10.1.10.129:/opt/ scp -r /opt/etcd 10.1.10.130:/opt/ scp /usr/lib/systemd/system/etcd.service 10.1.10.129:/usr/lib/systemd/system/ scp /usr/lib/systemd/system/etcd.service 10.1.10.130:/usr/lib/systemd/system/ 然后分别修改配置文件：\n10.1.10.129\n#[Member] ETCD_NAME=\u0026quot;etcd-2\u0026quot; ETCD_DATA_DIR=\u0026quot;/var/lib/etcd/default.etcd\u0026quot; ETCD_LISTEN_PEER_URLS=\u0026quot;https://10.1.10.129:2380\u0026quot; ETCD_LISTEN_CLIENT_URLS=\u0026quot;https://10.1.10.129:2379\u0026quot; #[Clustering] ETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026quot;https://10.1.10.129:2380\u0026quot; ETCD_ADVERTISE_CLIENT_URLS=\u0026quot;https://10.1.10.129:2379\u0026quot; ETCD_INITIAL_CLUSTER=\u0026quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380,etcd-3=https://10.1.10.130:2380\u0026quot; ETCD_INITIAL_CLUSTER_TOKEN=\u0026quot;etcd-cluster\u0026quot; ETCD_INITIAL_CLUSTER_STATE=\u0026quot;new\u0026quot; 10.1.10.130\n#[Member] ETCD_NAME=\u0026quot;etcd-3\u0026quot; ETCD_DATA_DIR=\u0026quot;/var/lib/etcd/default.etcd\u0026quot; ETCD_LISTEN_PEER_URLS=\u0026quot;https://10.1.10.130:2380\u0026quot; ETCD_LISTEN_CLIENT_URLS=\u0026quot;https://10.1.10.130:2379\u0026quot; #[Clustering] ETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026quot;https://10.1.10.130:2380\u0026quot; ETCD_ADVERTISE_CLIENT_URLS=\u0026quot;https://10.1.10.130:2379\u0026quot; ETCD_INITIAL_CLUSTER=\u0026quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380,etcd-3=https://10.1.10.130:2380\u0026quot; ETCD_INITIAL_CLUSTER_TOKEN=\u0026quot;etcd-cluster\u0026quot; ETCD_INITIAL_CLUSTER_STATE=\u0026quot;new\u0026quot; 然后启动三台的etcd服务\nsystemctl daemon-reload \u0026amp;\u0026amp; systemctl start etcd \u0026amp;\u0026amp; systemctl enable etcd 查看集群状态：\n/opt/etcd/bin/etcdctl \\ --ca-file=/opt/etcd/ssl/etcd-ca.pem --cert-file=/opt/etcd/ssl/etcd-server.pem --key-file=/opt/etcd/ssl/etcd-server-key.pem \\ --endpoints=\u0026quot;https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379\u0026quot; \\ cluster-health member a2dba8836695bcf6 is healthy: got healthy result from https://10.1.10.129:2379 member d1272b0b3cb41282 is healthy: got healthy result from https://10.1.10.128:2379 member e4a3a9c93ef84f2d is healthy: got healthy result from https://10.1.10.130:2379 cluster is healthy 安装Flannel  我是在所有节点都部署了，你也可以只部署Node。\n 下载地址：https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz\nFalnnel要用etcd存储自身一个子网信息，所以要保证能成功连接Etcd，写入预定义子网段：\n/opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/ca.pem --cert-file=/opt/etcd/ssl/etcd.pem --key-file=/opt/etcd/ssl/etcd-key.pem --endpoints=\u0026quot;https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379\u0026quot; set /coreos.com/network/config '{ \u0026quot;Network\u0026quot;: \u0026quot;172.17.0.0/16\u0026quot;, \u0026quot;Backend\u0026quot;: {\u0026quot;Type\u0026quot;: \u0026quot;vxlan\u0026quot;}}' 然后可以查看一下：\n# /opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/ca.pem --cert-file=/opt/etcd/ssl/etcd.pem --key-file=/opt/etcd/ssl/etcd-key.pem --endpoints=\u0026quot;https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379\u0026quot; get /coreos.com/network/config { \u0026quot;Network\u0026quot;: \u0026quot;172.17.0.0/16\u0026quot;, \u0026quot;Backend\u0026quot;: {\u0026quot;Type\u0026quot;: \u0026quot;vxlan\u0026quot;}} 解压压缩包\ntar xf flannel-v0.11.0-linux-amd64.tar.gz 将两个重要的二进制文件flanneld和mk-docker-opts.sh拷贝到/opt/kubernetes/bin下\ncp flanneld mk-docker-opts.sh /opt/kubernetes/bin/ 配置Flannel的配置文件：\ncat \u0026gt; /opt/kubernetes/config/flanneld.conf \u0026lt;\u0026lt;EOF FLANNEL_OPTIONS=\u0026quot;\\ --etcd-endpoints=https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379 \\ -etcd-cafile=/opt/etcd/ssl/ca.pem \\ -etcd-certfile=/opt/etcd/ssl/etcd.pem \\ -etcd-keyfile=/opt/etcd/ssl/etcd-key.pem\u0026quot; EOF 配置系统systemd启动文件\ncat \u0026gt; flanneld.service \u0026lt;\u0026lt;EOF [Unit] Description=Flanneld overlay address etcd agent After=network-online.target network.target Before=docker.service [Service] Type=notify EnvironmentFile=/opt/kubernetes/config/flanneld.conf ExecStart=/opt/kubernetes/bin/flanneld --ip-masq $FLANNEL_OPTIONS ExecStartPost=/opt/kubernetes/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/subnet.env Restart=on-failure [Install] WantedBy=multi-user.target EOF 配置Docker的系统文件，指定子网（/usr/lib/systemd/system/docker.service）\n[Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com BindsTo=containerd.service After=network-online.target firewalld.service containerd.service Wants=network-online.target Requires=docker.socket [Service] Type=notify EnvironmentFile=/run/flannel/subnet.env ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS ExecReload=/bin/kill -s HUP $MAINPID TimeoutSec=0 RestartSec=2 Restart=always StartLimitBurst=3 StartLimitInterval=60s LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity TasksMax=infinity Delegate=yes KillMode=process [Install] WantedBy=multi-user.target 将配置文件拷贝到另外主机\ncp /opt/kubernetes/flanneld.service /usr/lib/systemd/system/ scp -r /opt/kubernetes/ 10.1.10.129:/opt/ scp -r /opt/kubernetes/ 10.1.10.130:/opt/ scp /usr/lib/systemd/system/{docker,flanneld}.service 10.1.10.129:/usr/lib/systemd/system/ scp /usr/lib/systemd/system/{docker,flanneld}.service 10.1.10.130:/usr/lib/systemd/system/ 启动flannel和重启docker\nsystemctl daemon-reload \u0026amp;\u0026amp; systemctl enable flanneld \u0026amp;\u0026amp; systemctl start flanneld systemctl restart docker 检查docker是否使用了flannel网络：\n# ps -ef | grep docker root 10201 1 0 11:08 ? 00:00:00 /usr/bin/dockerd --bip=172.17.69.1/24 --ip-masq=false --mtu=1450 起一个容器测试网络连通性是否正确\n# docker run -it --name node02 --rm busybox /bin/sh / # ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 7: eth0@if8: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1450 qdisc noqueue link/ether 02:42:ac:11:50:02 brd ff:ff:ff:ff:ff:ff inet 172.17.80.2/24 brd 172.17.80.255 scope global eth0 valid_lft forever preferred_lft forever / # ping 10.1.10.128 -c 1 PING 10.1.10.128 (10.1.10.128): 56 data bytes 64 bytes from 10.1.10.128: seq=0 ttl=63 time=0.802 ms --- 10.1.10.128 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.802/0.802/0.802 ms / # ping 10.1.10.129 -c 1 PING 10.1.10.129 (10.1.10.129): 56 data bytes 64 bytes from 10.1.10.129: seq=0 ttl=63 time=0.515 ms --- 10.1.10.129 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.515/0.515/0.515 ms / # ping 10.1.10.130 -c 1 PING 10.1.10.130 (10.1.10.130): 56 data bytes 64 bytes from 10.1.10.130: seq=0 ttl=64 time=0.075 ms --- 10.1.10.130 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.075/0.075/0.075 ms / # ping 172.17.7.2 -c 1 PING 172.17.7.2 (172.17.7.2): 56 data bytes 64 bytes from 172.17.7.2: seq=0 ttl=62 time=0.884 ms --- 172.17.7.2 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.884/0.884/0.884 ms 安装mater组件 下载地址： https://dl.k8s.io/v1.16.4/kubernetes-server-linux-amd64.tar.gz\nmkdir /root/kubernetes/ssl/kubernetes -p （1）、解压安装压缩文件\ntar xf kubernetes-server-linux-amd64.tar.gz （2）、将我们需要的二进制文件拷贝到我们部署目录中\ncp kubernetes/server/bin/{kube-apiserver,kubectlkube-scheduler,kube-controller-manager} /opt/kubernetes/bin/ scp kubernetes/server/bin/{kubelet,kube-proxy} 10.1.10.129:/opt/kubernetes/bin/ scp kubernetes/server/bin/{kubelet,kube-proxy} 10.1.10.130:/opt/kubernetes/bin/ （3）、将其加入环境变量\necho \u0026quot;PATH=/opt/kubernetes/bin/:$PATH\u0026quot; \u0026gt;\u0026gt; /etc/profile source /etc/profile （4）、将我们所需的证书和密钥拷贝到部署目录中\n 由于我们master也准备当Node使用，所以我们将所有证书都拷贝到部署证书目录\n cp /root/kubernetes/ssl/kubernetes/*.pem /opt/kubernetes/ssl/ 生成证书 创建CA证书 （1）、新建CA配置文件（ca-csr.json）\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/ca-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF CN CommonName,kube-apiserver从证书中提取该字段作为请求的用户名(User Name)，浏览器使用该字段验证网站是否合法 O Organization,kube-apiserver 从证书中提取该字段作为请求用户和所属组(Group) kube-apiserver将提取的User、Group作为RBAC授权的用户和标识 （2）、新建CA配置文件（ca-config.json）\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/ca-config.json \u0026lt;\u0026lt;EOF { \u0026quot;signing\u0026quot;: { \u0026quot;default\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;87600h\u0026quot; }, \u0026quot;profiles\u0026quot;: { \u0026quot;kubernetes\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;87600h\u0026quot;, \u0026quot;usages\u0026quot;: [ \u0026quot;signing\u0026quot;, \u0026quot;key encipherment\u0026quot;, \u0026quot;server auth\u0026quot;, \u0026quot;client auth\u0026quot; ] } } } } EOF signing 表示该证书可用于签名其它证书，生成的ca.pem证书找中CA=TRUE server auth 表示client可以用该证书对server提供的证书进行验证 client auth 表示server可以用该证书对client提供的证书进行验证 （3）、生成CA证书\n# cfssl gencert -initca ca-csr.json | cfssljson -bare ca - 创建apiserver证书 （1）、新建apiserver证书文件\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/apiserver-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;10.254.0.1\u0026quot;, \u0026quot;127.0.0.1\u0026quot;, \u0026quot;10.1.10.128\u0026quot;, \u0026quot;10.1.10.129\u0026quot;, \u0026quot;10.1.10.130\u0026quot;, \u0026quot;kubernetes\u0026quot;, \u0026quot;kubernetes.default\u0026quot;, \u0026quot;kubernetes.default.svc\u0026quot;, \u0026quot;kubernetes.default.svc.cluster\u0026quot;, \u0026quot;kubernetes.default.svc.cluster.local\u0026quot; ], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF （2）、生成证书\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes apiserver-csr.json | cfssljson -bare apiserver 创建 Kubernetes webhook 证书配置文件 （1）、创建证书文件\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/aggregator-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;aggregator\u0026quot;, \u0026quot;hosts\u0026quot;: [\u0026quot;\u0026quot;], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF （2）、生成证书文件\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes aggregator-csr.json | cfssljson -bare aggregator 创建 Kubernetes admin 证书配置文件 （1）、创建证书文件\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/admin-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;hosts\u0026quot;: [\u0026quot;\u0026quot;], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;system:masters\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF （2）、生成证书和私钥\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin 创建kube-scheduler 证书配置文件 （1）、创建证书文件\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/kube-scheduler-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;system:kube-scheduler\u0026quot;, \u0026quot;hosts\u0026quot;: [\u0026quot;\u0026quot;], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;system:kube-scheduler\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF （2）、生成证书文件和私钥\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler 生成kube-controller-manager证书配置文件 （1）、创建证书文件\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/kube-controller-manager-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;system:kube-controller-manager\u0026quot;, \u0026quot;hosts\u0026quot;: [\u0026quot;\u0026quot;], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;system:kube-controller-manager\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF （2）、生成证书和私钥\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager 创建flannel 证书配置文件 （1）、创建证书文件\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/flannel-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;flannel\u0026quot;, \u0026quot;hosts\u0026quot;: [\u0026quot;\u0026quot;], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;system:masters\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF （2）、生成证书和私钥\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes flannel-csr.json | cfssljson -bare flannel 创建kube-proxy证书 （1）、创建证书文件\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/kube-proxy-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;system:kube-proxy\u0026quot;, \u0026quot;hosts\u0026quot;: [], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;system:masters\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF （2）、生成证书文件\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy 创建 kubernetes-dashboard证书配置文件 （1）、创建证书文件\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/dashboard-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;dashboard\u0026quot;, \u0026quot;hosts\u0026quot;: [\u0026quot;\u0026quot;], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF （2）、生成证书文件和私钥\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes dashboard-csr.json | cfssljson -bare dashboard 创建metrics-server 证书配置文件 （1）、创建证书文件\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/metrics-server-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;metrics-server\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF （2）、生成证书和私钥\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes metrics-server-csr.json | cfssljson -bare metrics-server 创建kubeconfig配置文件  在/root/kubernetes/kubeconfig目录下创建这些文件\n （1）、设置kube-apiserver环境变量\nexport KUBE_APISERVER=\u0026quot;https://10.1.10.128:6443\u0026quot; 创建admin kubeconfig # 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=../ssl/kubernetes/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=admin.kubeconfig # 设置客户端认证参数 kubectl config set-credentials admin \\ --client-certificate=../ssl/kubernetes/admin.pem \\ --client-key=../ssl/kubernetes/admin-key.pem \\ --embed-certs=true \\ --kubeconfig=admin.kubeconfig # 设置上下文参数 kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin \\ --namespace=kube-system \\ --kubeconfig=admin.kubeconfig # 设置默认上下文 kubectl config use-context kubernetes --kubeconfig=admin.kubeconfig 创建kube-scheduler kubeconfig # 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=../ssl/kubernetes/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=kube-scheduler.kubeconfig # 设置客户端认证参数 kubectl config set-credentials system:kube-scheduler \\ --client-certificate=../ssl/kubernetes/kube-scheduler.pem \\ --embed-certs=true \\ --client-key=../ssl/kubernetes/kube-scheduler-key.pem \\ --kubeconfig=kube-scheduler.kubeconfig # 设置上下文参数 kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=kube-scheduler.kubeconfig # 设置默认上下文 kubectl config use-context kubernetes --kubeconfig=kube-scheduler.kubeconfig 创建kube-controller-manager kubeconfig # 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=../ssl/kubernetes/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=kube-controller-manager.kubeconfig # 设置客户端认证参数 kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=../ssl/kubernetes/kube-controller-manager.pem \\ --embed-certs=true \\ --client-key=../ssl/kubernetes/kube-controller-manager-key.pem \\ --kubeconfig=kube-controller-manager.kubeconfig # 设置上下文参数 kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=kube-controller-manager.kubeconfig # 设置默认上下文 kubectl config use-context kubernetes --kubeconfig=kube-controller-manager.kubeconfig 创建bootstrap kubeconfig # 生成TOKEN export TOKEN_ID=$(head -c 6 /dev/urandom | md5sum | head -c 6) export TOKEN_SECRET=$(head -c 16 /dev/urandom | md5sum | head -c 16) export BOOTSTRAP_TOKEN=${TOKEN_ID}.${TOKEN_SECRET} # 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=../ssl/kubernetes/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=bootstrap.kubeconfig # 设置客户端认证参数 kubectl config set-credentials system:bootstrap:${TOKEN_ID} \\ --token=${BOOTSTRAP_TOKEN} \\ --kubeconfig=bootstrap.kubeconfig # 设置上下文参数 kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:bootstrap:${TOKEN_ID} \\ --kubeconfig=bootstrap.kubeconfig # 设置默认上下文 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig  BOOTSTRAP_TOKEN=0a22e7.4b91472175b8aaab\n 创建flannel kubeconfig # 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=../ssl/kubernetes/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=kubeconfig.conf # 设置客户端认证参数 kubectl config set-credentials flannel \\ --client-certificate=../ssl/kubernetes/flannel.pem \\ --client-key=../ssl/kubernetes/flannel-key.pem \\ --embed-certs=true \\ --kubeconfig=kubeconfig.conf # 设置上下文参数 kubectl config set-context default \\ --cluster=kubernetes \\ --user=flannel \\ --kubeconfig=kubeconfig.conf # 设置默认上下文 kubectl config use-context default --kubeconfig=kubeconfig.conf 创建kube-proxy kubeconfig # 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=../ssl/kubernetes/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=kube-proxy.kubeconfig # 设置客户端认证参数 kubectl config set-credentials system:kube-proxy \\ --client-certificate=../ssl/kubernetes/kube-proxy.pem \\ --client-key=../ssl/kubernetes/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 设置上下文参数 kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 设置默认上下文 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 创建组件配置文件 创建kube-apiserver配置文件 （1）、创建主配置文件\ncat \u0026gt; /opt/kubernetes/config/kube-apiserver.conf \u0026lt;\u0026lt;EOF KUBE_APISERVER_OPTS=\u0026quot;--logtostderr=false \\\\ --bind-address=10.1.10.128 \\\\ --advertise-address=10.1.10.128 \\\\ --secure-port=6443 \\\\ --insecure-port=0 \\\\ --service-cluster-ip-range=10.254.0.0/16 \\\\ --service-node-port-range=20000-40000 \\\\ --etcd-cafile=/opt/etcd/ssl/etcd-ca.pem \\\\ --etcd-certfile=/opt/etcd/ssl/etcd-server.pem \\\\ --etcd-keyfile=/opt/etcd/ssl/etcd-server-key.pem \\\\ --etcd-prefix=/registry \\\\ --etcd-servers=https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379 \\\\ --client-ca-file=/opt/kubernetes/ssl/ca.pem \\\\ --tls-cert-file=/opt/kubernetes/ssl/apiserver.pem\\\\ --tls-private-key-file=/opt/kubernetes/ssl/apiserver-key.pem \\\\ --kubelet-client-certificate=/opt/kubernetes/ssl/apiserver.pem \\\\ --kubelet-client-key=/opt/kubernetes/ssl/apiserver-key.pem \\\\ --service-account-key-file=/opt/kubernetes/ssl/ca.pem \\\\ --requestheader-client-ca-file=/opt/kubernetes/ssl/ca.pem \\\\ --proxy-client-cert-file=/opt/kubernetes/ssl/aggregator.pem \\\\ --proxy-client-key-file=/opt/kubernetes/ssl/aggregator-key.pem \\\\ --requestheader-allowed-names=aggregator \\\\ --requestheader-group-headers=X-Remote-Group \\\\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\\\ --requestheader-username-headers=X-Remote-User \\\\ --enable-aggregator-routing=true \\\\ --anonymous-auth=false \\\\ --allow-privileged=true \\\\ --experimental-encryption-provider-config=/opt/kubernetes/config/encryption-config.yaml \\\\ --enable-admission-plugins=DefaultStorageClass,DefaultTolerationSeconds,LimitRanger,NamespaceExists,NamespaceLifecycle,NodeRestriction,OwnerReferencesPermissionEnforcement,PodNodeSelector,PersistentVolumeClaimResize,PodPreset,PodTolerationRestriction,ResourceQuota,ServiceAccount,StorageObjectInUseProtection MutatingAdmissionWebhook ValidatingAdmissionWebhook \\\\ --disable-admission-plugins=DenyEscalatingExec,ExtendedResourceToleration,ImagePolicyWebhook,LimitPodHardAntiAffinityTopology,NamespaceAutoProvision,Priority,EventRateLimit,PodSecurityPolicy \\\\ --cors-allowed-origins=.* \\\\ --enable-swagger-ui \\\\ --runtime-config=api/all=true \\\\ --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\\\ --authorization-mode=Node,RBAC \\\\ --apiserver-count=1 \\\\ --audit-log-maxage=30 \\\\ --audit-log-maxbackup=3 \\\\ --audit-log-maxsize=100 \\\\ --kubelet-https \\\\ --event-ttl=1h \\\\ --feature-gates=RotateKubeletServerCertificate=true,RotateKubeletClientCertificate=true \\\\ --enable-bootstrap-token-auth=true \\\\ --audit-log-path=/var/log/kubernetes/api-server-audit.log \\\\ --alsologtostderr=true \\\\ --log-dir=/var/log/kubernetes \\\\ --v=2 \\\\ --endpoint-reconciler-type=lease \\\\ --max-mutating-requests-inflight=100 \\\\ --max-requests-inflight=500 \\\\ --target-ram-mb=6000\u0026quot; EOF （2）、创建encryption-config.yaml\nexport ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64) cat \u0026gt; /opt/kubernetes/config/encryption-config.yaml \u0026lt;\u0026lt;EOF kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {} EOF 创建kube-controller-manager配置文件 cat \u0026gt; /opt/kubernetes/config/kube-controller-manager.conf \u0026lt;\u0026lt;EOF KUBE_CONTROLLER_MANAGER_OPTS=\u0026quot;--logtostderr=false \\\\ --leader-elect=true \\\\ --address=0.0.0.0 \\\\ --service-cluster-ip-range=10.254.0.0/16 \\\\ --cluster-cidr=172.20.0.0/16 \\\\ --node-cidr-mask-size=24 \\\\ --cluster-name=kubernetes \\\\ --allocate-node-cidrs=true \\\\ --kubeconfig=/opt/kubernetes/config/kube-controller-manager.kubeconfig \\\\ --authentication-kubeconfig=/opt/kubernetes/config/kube-controller-manager.kubeconfig \\\\ --authorization-kubeconfig=/opt/kubernetes/config/kube-controller-manager.kubeconfig \\\\ --use-service-account-credentials=true \\\\ --client-ca-file=/opt/kubernetes/ssl/ca.pem \\\\ --requestheader-client-ca-file=/opt/kubernetes/ssl/ca.pem \\\\ --node-monitor-grace-period=40s \\\\ --node-monitor-period=5s \\\\ --pod-eviction-timeout=5m0s \\\\ --terminated-pod-gc-threshold=50 \\\\ --alsologtostderr=true \\\\ --cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\\\ --cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\ --deployment-controller-sync-period=10s \\\\ --experimental-cluster-signing-duration=86700h0m0s \\\\ --enable-garbage-collector=true \\\\ --root-ca-file=/opt/kubernetes/ssl/ca.pem \\\\ --service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\ --feature-gates=RotateKubeletServerCertificate=true,RotateKubeletClientCertificate=true \\\\ --controllers=*,bootstrapsigner,tokencleaner \\\\ --horizontal-pod-autoscaler-use-rest-clients=true \\\\ --horizontal-pod-autoscaler-sync-period=10s \\\\ --tls-cert-file=/opt/kubernetes/ssl/kube-controller-manager.pem \\\\ --tls-private-key-file=/opt/kubernetes/ssl/kube-controller-manager-key.pem \\\\ --kube-api-qps=100 \\\\ --kube-api-burst=100 \\\\ --log-dir=/var/log/kubernetes \\\\ --v=2\u0026quot; EOF 创建kube-scheduler配置文件 cat \u0026gt; /opt/kubernetes/config/kube-scheduler.conf \u0026lt;\u0026lt;EOF KUBE_SCHEDULER_OPTS=\u0026quot; \\\\ --logtostderr=false \\\\ --address=0.0.0.0 \\\\ --leader-elect=true \\\\ --kubeconfig=/opt/kubernetes/config/kube-scheduler.kubeconfig \\\\ --authentication-kubeconfig=/opt/kubernetes/config/kube-scheduler.kubeconfig \\\\ --authorization-kubeconfig=/opt/kubernetes/config/kube-scheduler.kubeconfig \\\\ --alsologtostderr=true \\\\ --kube-api-qps=100 \\\\ --kube-api-burst=100 \\\\ --log-dir=/var/log/kubernetes \\\\ --v=2\u0026quot; EOF 创建kubelet配置文件  在node节点上创建\n cat \u0026gt; /opt/kubernetes/config/kubelet.conf \u0026lt;\u0026lt;EOF KUBELET_OPTS=\u0026quot;--logtostderr=true \\\\ --v=4 \\\\ --network-plugin=cni \\\\ --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin \\\\ --hostname-override=10.1.10.129 \\\\ --kubeconfig=/opt/kubernetes/config/kubelet.kubeconfig \\\\ --bootstrap-kubeconfig=/opt/kubernetes/config/bootstrap.kubeconfig \\\\ --config=/opt/kubernetes/config/kubelet.yaml \\\\ --cert-dir=/opt/kubernetes/ssl \\\\ --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/rookieops/pause-amd64:3.0\u0026quot; EOF  address: 节点IP，不同节点需要更改\nnode-ip：节点IP，不同节点需要更改\nhostname-override：节点hostname，也可以配置节点IP，不同节点需要更改\nhealthz-bind-address：节点IP，不同节点需要更改\n\u0026ndash;hostname-override 在集群中显示的主机名，其他节点需要更改\n\u0026ndash;kubeconfig 指定kubeconfig文件位置，会自动生成\n\u0026ndash;bootstrap-kubeconfig 指定刚才生成的bootstrap.kubeconfig文件\n\u0026ndash;cert-dir 颁发证书存放位置\n\u0026ndash;pod-infra-container-image 管理Pod网络的镜像\n 创建kubelet.yaml配置文件\ncat \u0026gt; /opt/kubernetes/config/kubelet.yaml \u0026lt;\u0026lt;EOF kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 address: 10.1.10.129 port: 10250 readOnlyPort: 10255 cgroupDriver: cgroupfs clusterDNS: [\u0026quot;10.254.0.2\u0026quot;] clusterDomain: cluster.local. failSwapOn: false authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /opt/kubernetes/ssl/ca.pem authorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30s evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5% maxOpenFiles: 1000000 maxPods: 110 EOF  不同节点需要修改的地方为IP\n 创建kube-proxy配置文件 cat \u0026gt; /opt/kubernetes/config/kube-proxy.conf \u0026lt;\u0026lt;EOF KUBE_PROXY_OPTS=\u0026quot;--logtostderr=false \\\\ --v=2 \\\\ --feature-gates=SupportIPVSProxyMode=true \\\\ --masquerade-all=true \\\\ --proxy-mode=ipvs \\\\ --ipvs-min-sync-period=5s \\\\ --ipvs-sync-period=5s \\\\ --ipvs-scheduler=rr \\\\ --cluster-cidr=172.20.0.0/16 \\\\ --log-dir=/var/log/kubernetes \\\\ --kubeconfig=/opt/kubernetes/config/kube-proxy.kubeconfig\u0026quot; EOF 创建组件systemd启动文件 创建kube-apiserver启动文件 cat \u0026gt; /usr/lib/systemd/system/kube-apiserver.service \u0026lt;\u0026lt;EOF [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes [Service] EnvironmentFile=-/opt/kubernetes/config/kube-apiserver.conf ExecStart=/opt/kubernetes/bin/kube-apiserver \\$KUBE_APISERVER_OPTS Restart=on-failure RestartSec=10 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target EOF 创建kube-controller-manager启动文件 cat \u0026gt; /usr/lib/systemd/system/kube-controller-manager.service \u0026lt;\u0026lt;EOF [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/kubernetes/kubernetes [Service] EnvironmentFile=-/opt/kubernetes/config/kube-controller-manager.conf ExecStart=/opt/kubernetes/bin/kube-controller-manager \\$KUBE_CONTROLLER_MANAGER_OPTS Restart=on-failure [Install] WantedBy=multi-user.target EOF 创建kube-scheduler启动文件 cat \u0026gt; /usr/lib/systemd/system/kube-scheduler.service \u0026lt;\u0026lt;EOF [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/kubernetes/kubernetes [Service] EnvironmentFile=-/opt/kubernetes/config/kube-scheduler.conf ExecStart=/opt/kubernetes/bin/kube-scheduler \\$KUBE_SCHEDULER_OPTS Restart=on-failure [Install] WantedBy=multi-user.target EOF 创建kubelet启动文件  在需要部署的Node上创建\n cat \u0026gt; /usr/lib/systemd/system/kubelet.service \u0026lt;\u0026lt;EOF [Unit] Description=Kubernetes Kubelet After=docker.service Requires=docker.service [Service] EnvironmentFile=/opt/kubernetes/config/kubelet.conf ExecStart=/opt/kubernetes/bin/kubelet \\$KUBELET_OPTS Restart=on-failure KillMode=process [Install] WantedBy=multi-user.target EOF 创建kube-proxy启动文件  在需要部署的Node上创建\n cat \u0026gt; /usr/lib/systemd/system/kube-proxy.service \u0026lt;\u0026lt;EOF [Unit] Description=Kubernetes Proxy After=network.target [Service] EnvironmentFile=-/opt/kubernetes/config/kube-proxy.conf ExecStart=/opt/kubernetes/bin/kube-proxy \\$KUBE_PROXY_OPTS Restart=on-failure [Install] WantedBy=multi-user.target EOF 启动组件 master组件  由于我们master也准备当Node使用，所以我们将所有证书都拷贝到部署证书目录\n cp /root/kubernetes/ssl/kubernetes/*.pem /opt/kubernetes/ssl/ （1）、将我们创建的kubeconfig配置文件也拷贝到部署目录\ncp /root/kubernetes/kubeconfig/* /opt/kubernetes/config/ （2）、创建日志目录，并启动kube-apiserver\nmkdir /var/log/kubernetes systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable kube-apiserver \u0026amp;\u0026amp; systemctl start kube-apiserver （3）、复制kubeconfig文件到~/.kube/\nmv ~/.kube/config{,.old} cp /opt/kubernetes/config/admin.kubeconfig ~/.kube/config （4）、查看状态\nsystemctl status kube-apiserver # kubectl cluster-info Kubernetes master is running at https://10.1.10.128:6443 （5）、启动kube-controller-manager\nsystemctl daemon-reload \u0026amp;\u0026amp; systemctl enable kube-controller-manager \u0026amp;\u0026amp; systemctl start kube-controller-manager （6）、启动kube-scheduler\nsystemctl daemon-reload \u0026amp;\u0026amp; systemctl enable kube-scheduler \u0026amp;\u0026amp; systemctl start kube-scheduler （7）、查看集群状态\n# kubectl get cs -o=go-template='{{printf \u0026quot;|NAME|STATUS|MESSAGE|\\n\u0026quot;}}{{range .items}}{{$name := .metadata.name}}{{range .conditions}}{{printf \u0026quot;|%s|%s|%s|\\n\u0026quot; $name .status .message}}{{end}}{{end}}' |NAME|STATUS|MESSAGE| |scheduler|True|ok| |controller-manager|True|ok| |etcd-2|True|{\u0026quot;health\u0026quot;:\u0026quot;true\u0026quot;}| |etcd-0|True|{\u0026quot;health\u0026quot;:\u0026quot;true\u0026quot;}| |etcd-1|True|{\u0026quot;health\u0026quot;:\u0026quot;true\u0026quot;}| # kubectl get all --all-namespaces NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.254.0.1 \u0026lt;none\u0026gt; 443/TCP 8m26s （8）、授权访问kube-apiserver\n授予 kubernetes API 的权限 kubectl create clusterrolebinding controller-node-clusterrolebing --clusterrole=system:kube-controller-manager --user=system:kube-controller-manager kubectl create clusterrolebinding scheduler-node-clusterrolebing --clusterrole=system:kube-scheduler --user=system:kube-scheduler kubectl create clusterrolebinding controller-manager:system:auth-delegator --user system:kube-controller-manager --clusterrole system:auth-delegator 授予 kubernetes 证书访问 kubelet API 的权限 kubectl create clusterrolebinding --user system:serviceaccount:kube-system:default kube-system-cluster-admin --clusterrole cluster-admin kubectl create clusterrolebinding kubelet-node-clusterbinding --clusterrole=system:node --group=system:nodes kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes （9）、配置kubectl自动补全\nyum install -y bash-completion source /usr/share/bash-completion/bash_completion source \u0026lt;(kubectl completion bash) echo \u0026quot;source \u0026lt;(kubectl completion bash)\u0026quot; \u0026gt;\u0026gt; ~/.bashrc  如果要更改默认得namespace，可以使用如下命令\n kubectl config set-context --current --namespace={{namespace}} node组件  在master上部署bootstrap secret，脚本可以放置任意位置，我习惯放于/root/manifests下。另外TOKEN_ID和TOKEN_SECRET是我们在创建bootstrap kubeconfig生成的，在做那一步的时候以防万一应该记录下来。\n cat \u0026lt;\u0026lt; EOF | tee bootstrap.secret.yaml apiVersion: v1 kind: Secret metadata: # Name MUST be of form \u0026quot;bootstrap-token-\u0026lt;token id\u0026gt;\u0026quot; name: bootstrap-token-${TOKEN_ID} namespace: kube-system # Type MUST be 'bootstrap.kubernetes.io/token' type: bootstrap.kubernetes.io/token stringData: # Human readable description. Optional. description: \u0026quot;The default bootstrap token generated by 'kubelet '.\u0026quot; # Token ID and secret. Required. token-id: ${TOKEN_ID} token-secret: ${TOKEN_SECRET} # Allowed usages. usage-bootstrap-authentication: \u0026quot;true\u0026quot; usage-bootstrap-signing: \u0026quot;true\u0026quot; # Extra groups to authenticate the token as. Must start with \u0026quot;system:bootstrappers:\u0026quot; auth-extra-groups: system:bootstrappers:worker,system:bootstrappers:ingress --- # A ClusterRole which instructs the CSR approver to approve a node requesting a # serving cert matching its client cert. kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver rules: - apiGroups: [\u0026quot;certificates.k8s.io\u0026quot;] resources: [\u0026quot;certificatesigningrequests/selfnodeserver\u0026quot;] verbs: [\u0026quot;create\u0026quot;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026quot;true\u0026quot; labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kubernetes-to-kubelet rules: - apiGroups: - \u0026quot;\u0026quot; resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \u0026quot;*\u0026quot; --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:kubernetes namespace: \u0026quot;\u0026quot; roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kubernetes-to-kubelet subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kubernetes EOF  然后创建资源\n # 创建资源 kubectl create -f bootstrap.secret.yaml ### 查看创建的token kubeadm token list # 允许 system:bootstrappers 组用户创建 CSR 请求 kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers # 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求 kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers # 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求 kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes # 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求 kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes （1）、在Node节点创建我们需要的目录\nmkdir /opt/kubernetes/{bin,config,ssl} -p （2）、将node节点需要的二进制文件拷贝过去\ncd /root/kubernetes/install/kubernetes/server/bin scp kubelet kube-proxy 10.1.10.129:/opt/kubernetes/bin/ scp kubelet kube-proxy 10.1.10.130:/opt/kubernetes/bin/ （3）、将kubeconfig文件拷贝到Node节点上\ncd /root/kubernetes/kubeconfig scp * 10.1.10.129:/opt/kubernetes/config/ scp * 10.1.10.130:/opt/kubernetes/config/ （4）、将证书拷贝到Node节点上\n 只拷贝需要的，我这里仅仅是为了方便~~\n cd /root/kubernetes/ssl/kubernetes scp *.pem 10.1.10.129:/opt/kubernetes/ssl/ scp *.pem 10.1.10.130:/opt/kubernetes/ssl/ （5）、启动kubelet\nsystemctl daemon-reload \u0026amp;\u0026amp; systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet （6）、启动kube-proxy\nsystemctl daemon-reload \u0026amp;\u0026amp; systemctl enable kube-proxy \u0026amp;\u0026amp; systemctl start kube-proxy （7）、在master上查看\nkubectl get node NAME STATUS ROLES AGE VERSION node01-k8s NotReady \u0026lt;none\u0026gt; 72m v1.16.4 node02-k8s NotReady \u0026lt;none\u0026gt; 5m12s v1.16.4  之所以是NotReady，是因为我们还没有部署网络\n 安装组件 部署Flannel kubernetes提供一个CNI接口，它可以和任何支持CNI的网络插件对接，所以我们这里不直接部署Flannel，改成部署cni，然后将flannel部署在集群中。\n 使用CNI插件时，需要做三个配置：\n kubelet启动参数中networkPlugin设置为cni 在/etc/cni/net.d中增加cni的配置文件，配置文件中可以指定需要使用的cni组件及参数 将需要用到的cni组件（二进制可执行文件）放到/opt/cni/bin目录下   （1）、确保配置中开启了cni，如下\nKUBELET_OPTS=\u0026quot;--logtostderr=true \\ --v=4 \\ --network-plugin=cni \\ --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin \\ --hostname-override=10.1.10.128 \\ --kubeconfig=/opt/kubernetes/config/kubelet.kubeconfig \\ --bootstrap-kubeconfig=/opt/kubernetes/config/bootstrap.kubeconfig \\ --config=/opt/kubernetes/config/kubelet.config \\ --cert-dir=/opt/kubernetes/ssl \\ --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/rookieops/pause-amd64:3.0\u0026quot; （2）、下载cni文件\n下载地址：https://github.com/containernetworking/plugins/releases/download/v0.8.3/cni-plugins-linux-amd64-v0.8.3.tgz\n（3）、创建需要的目录\nmkdir /opt/cni/bin /etc/cni/net.d -p （4）、解压压缩包到安装目录/opt/cni/bin\ntar xf cni-plugins-linux-amd64-v0.8.3.tgz -C /opt/cni/bin/ （5）、将其拷贝到另外的节点\nscp -r /opt/cni/bin/* 10.1.10.129:/opt/cni/bin/ scp -r /opt/cni/bin/* 10.1.10.130:/opt/cni/bin/ （6）、配置kube-flannel YAML清单文件（kube-flannel.yaml）\n 下载地址：https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml\n --- apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: psp.flannel.unprivileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default spec: privileged: false volumes: - configMap - secret - emptyDir - hostPath allowedHostPaths: - pathPrefix: \u0026quot;/etc/cni/net.d\u0026quot; - pathPrefix: \u0026quot;/etc/kube-flannel\u0026quot; - pathPrefix: \u0026quot;/run/flannel\u0026quot; readOnlyRootFilesystem: false # Users and groups runAsUser: rule: RunAsAny supplementalGroups: rule: RunAsAny fsGroup: rule: RunAsAny # Privilege Escalation allowPrivilegeEscalation: false defaultAllowPrivilegeEscalation: false # Capabilities allowedCapabilities: ['NET_ADMIN'] defaultAddCapabilities: [] requiredDropCapabilities: [] # Host namespaces hostPID: false hostIPC: false hostNetwork: true hostPorts: - min: 0 max: 65535 # SELinux seLinux: # SELinux is unsed in CaaSP rule: 'RunAsAny' --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel rules: - apiGroups: ['extensions'] resources: ['podsecuritypolicies'] verbs: ['use'] resourceNames: ['psp.flannel.unprivileged'] - apiGroups: - \u0026quot;\u0026quot; resources: - pods verbs: - get - apiGroups: - \u0026quot;\u0026quot; resources: - nodes verbs: - list - watch - apiGroups: - \u0026quot;\u0026quot; resources: - nodes/status verbs: - patch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: flannel namespace: kube-system --- kind: ConfigMap apiVersion: v1 metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flannel data: cni-conf.json: | { \u0026quot;cniVersion\u0026quot;: \u0026quot;0.2.0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;cbr0\u0026quot;, \u0026quot;plugins\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;flannel\u0026quot;, \u0026quot;delegate\u0026quot;: { \u0026quot;hairpinMode\u0026quot;: true, \u0026quot;isDefaultGateway\u0026quot;: true } }, { \u0026quot;type\u0026quot;: \u0026quot;portmap\u0026quot;, \u0026quot;capabilities\u0026quot;: { \u0026quot;portMappings\u0026quot;: true } } ] } net-conf.json: | { \u0026quot;Network\u0026quot;: \u0026quot;172.20.0.0/16\u0026quot;, \u0026quot;Backend\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;vxlan\u0026quot; } } --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-flannel-ds-amd64 namespace: kube-system labels: tier: node app: flannel spec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/os operator: In values: - linux - key: beta.kubernetes.io/arch operator: In values: - amd64 hostNetwork: true tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: registry.cn-hangzhou.aliyuncs.com/rookieops/flannel:v0.11.0-amd64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: registry.cn-hangzhou.aliyuncs.com/rookieops/flannel:v0.11.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \u0026quot;100m\u0026quot; memory: \u0026quot;50Mi\u0026quot; limits: cpu: \u0026quot;100m\u0026quot; memory: \u0026quot;50Mi\u0026quot; securityContext: privileged: false capabilities: add: [\u0026quot;NET_ADMIN\u0026quot;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg （7）、生成资源清单\nkubectl apply -f kube-flannel.yaml （8）、查看集群状态\n# kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE kube-flannel-ds-amd64-2qkcb 1/1 Running 0 85s kube-flannel-ds-amd64-7nzj5 1/1 Running 0 85s # kubectl get node NAME STATUS ROLES AGE VERSION node01-k8s Ready \u0026lt;none\u0026gt; 104m v1.16.4 node02-k8s Ready \u0026lt;none\u0026gt; 37m v1.16.4  可以看到集群状态已经变为ready\n （9）、用一个demo文件测试一下\napiVersion: v1 kind: Pod metadata: name: pod-demo spec: containers: - name: test-ng image: nginx 查看是否能成功分配IP\n# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-flannel-ds-amd64-2qkcb 1/1 Running 0 5m36s 10.1.10.129 node01-k8s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-flannel-ds-amd64-7nzj5 1/1 Running 0 5m36s 10.1.10.130 node02-k8s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod-demo 1/1 Running 0 55s 172.20.1.2 node02-k8s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  测试正常\n 部署core dns YAML清单如下\napiVersion: v1 kind: ServiceAccount metadata: name: coredns namespace: kube-system labels: kubernetes.io/cluster-service: \u0026quot;true\u0026quot; addonmanager.kubernetes.io/mode: Reconcile --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: kubernetes.io/bootstrapping: rbac-defaults addonmanager.kubernetes.io/mode: Reconcile name: system:coredns rules: - apiGroups: - \u0026quot;\u0026quot; resources: - endpoints - services - pods - namespaces verbs: - list - watch - apiGroups: - \u0026quot;\u0026quot; resources: - nodes verbs: - get --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026quot;true\u0026quot; labels: kubernetes.io/bootstrapping: rbac-defaults addonmanager.kubernetes.io/mode: EnsureExists name: system:coredns roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:coredns subjects: - kind: ServiceAccount name: coredns namespace: kube-system --- apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system labels: addonmanager.kubernetes.io/mode: EnsureExists data: Corefile: | .:53 { errors health kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure upstream /etc/resolv.conf fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . /etc/resolv.conf cache 30 reload loadbalance } --- apiVersion: apps/v1 kind: Deployment metadata: name: coredns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: \u0026quot;true\u0026quot; addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: \u0026quot;CoreDNS\u0026quot; spec: # replicas: not specified here: # 1. In order to make Addon Manager do not reconcile this replicas parameter. # 2. Default is 1. # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on. strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 selector: matchLabels: k8s-app: kube-dns template: metadata: labels: k8s-app: kube-dns annotations: seccomp.security.alpha.kubernetes.io/pod: 'docker/default' spec: priorityClassName: system-cluster-critical serviceAccountName: coredns tolerations: - key: \u0026quot;CriticalAddonsOnly\u0026quot; operator: \u0026quot;Exists\u0026quot; nodeSelector: beta.kubernetes.io/os: linux containers: - name: coredns image: coredns/coredns imagePullPolicy: Always resources: limits: memory: 170Mi requests: cpu: 100m memory: 70Mi args: [ \u0026quot;-conf\u0026quot;, \u0026quot;/etc/coredns/Corefile\u0026quot; ] volumeMounts: - name: config-volume mountPath: /etc/coredns readOnly: true ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP livenessProbe: httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: httpGet: path: /health port: 8080 scheme: HTTP securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - all readOnlyRootFilesystem: true dnsPolicy: Default volumes: - name: config-volume configMap: name: coredns items: - key: Corefile path: Corefile --- apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system annotations: prometheus.io/port: \u0026quot;9153\u0026quot; prometheus.io/scrape: \u0026quot;true\u0026quot; labels: k8s-app: kube-dns kubernetes.io/cluster-service: \u0026quot;true\u0026quot; addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: \u0026quot;CoreDNS\u0026quot; spec: selector: k8s-app: kube-dns clusterIP: 10.254.0.2 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP - name: metrics port: 9153 protocol: TCP 测试：\n# 安装测试攻击 yum install bind-utils-y # 测试百度，要在Node节点测试，因为我们master没有安装网络 # dig @10.254.0.2 www.baidu.com ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.11.4-P2-RedHat-9.11.4-9.P2.el7 \u0026lt;\u0026lt;\u0026gt;\u0026gt; @10.254.0.2 www.baidu.com ; (1 server found) ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 24278 ;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;www.baidu.com. IN A ;; ANSWER SECTION: www.baidu.com. 30 IN CNAME www.a.shifen.com. www.a.shifen.com. 30 IN A 112.80.248.75 www.a.shifen.com. 30 IN A 112.80.248.76 ;; Query time: 54 msec ;; SERVER: 10.254.0.2#53(10.254.0.2) ;; WHEN: Sat Dec 28 23:40:43 CST 2019 ;; MSG SIZE rcvd: 149  返回解析正常\n 部署Traefik Ingress （1）、创建RBAC认证配置清单（traefik-rbac.yaml）\n--- apiVersion: v1 kind: ServiceAccount metadata: name: traefik-ingress-controller namespace: kube-system --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: traefik-ingress-controller rules: - apiGroups: - \u0026quot;\u0026quot; resources: - services - endpoints - secrets verbs: - get - list - watch - apiGroups: - extensions resources: - ingresses verbs: - get - list - watch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: traefik-ingress-controller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik-ingress-controller subjects: - kind: ServiceAccount name: traefik-ingress-controller namespace: kube-system （2）、创建traefik配置清单（traefik.yaml）\n--- kind: Deployment apiVersion: apps/v1 metadata: name: traefik-ingress-controller namespace: kube-system labels: k8s-app: traefik-ingress-lb spec: replicas: 1 selector: matchLabels: k8s-app: traefik-ingress-lb template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 60 # tolerations: # - operator: \u0026quot;Exists\u0026quot; # nodeSelector: # kubernetes.io/hostname: master containers: - image: traefik:v1.7.17 name: traefik-ingress-lb ports: - name: http containerPort: 80 - name: admin containerPort: 8080 args: - --api - --kubernetes - --logLevel=INFO --- kind: Service apiVersion: v1 metadata: name: traefik-ingress-service namespace: kube-system spec: selector: k8s-app: traefik-ingress-lb ports: - protocol: TCP port: 80 name: web nodePort: 38000 - protocol: TCP port: 8080 nodePort: 38080 name: admin type: NodePort （3）、创建配置清单\nkubectl apply -f traefik-rbac.yaml kubectl apply -g traefik.yaml （4）、查看结果\nkubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE coredns-9d5b6bdb6-mpwht 1/1 Running 0 22h kube-flannel-ds-amd64-2qkcb 1/1 Running 0 22h kube-flannel-ds-amd64-7nzj5 1/1 Running 0 22h pod-demo 1/1 Running 0 22h traefik-ingress-controller-7758594f89-lwf2t 1/1 Running 0 41s # kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.254.0.2 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 22h traefik-ingress-service NodePort 10.254.33.90 \u0026lt;none\u0026gt; 80:38000/TCP,8080:38080/TCP 3m33s 我们可以通过http://10.1.10.129:38080 来查看Dashboard，如下\n部署Dashboard （1）、部署，直接是官方部署文档\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml （2）、配置Ingress或者将service类型改为NodePort，我这里改为NodePort\nkubectl edit svc -n kubernetes-dashboard kubernetes-dashboard （3）、然后我们在浏览器访问\n# kubectl get svc -n kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE dashboard-metrics-scraper ClusterIP 10.254.224.240 \u0026lt;none\u0026gt; 8000/TCP 2m28s kubernetes-dashboard NodePort 10.254.82.50 \u0026lt;none\u0026gt; 443:28330/TCP 2m28s （4）、创建一个admin token\n# 创建sa kubectl create sa dashboard-admin -n kube-system # 授权 kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin # 获取token ADMIN_SECRET=$(kubectl get secrets -n kube-system | grep dashboard-admin | awk '{print $1}') # 获取dashboard kubeconfig使用token的值 DASHBOARD_LOGIN_TOKEN=$(kubectl describe secret -n kube-system ${ADMIN_SECRET} | grep -E '^token' | awk '{print $2}') echo ${DASHBOARD_LOGIN_TOKEN} （5）、创建dashboard kubeconfig\n 还是在我们统一的Kubeconfig目录下创建/root/kubernetes/kubeconfig\n # 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=../ssl/kubernetes/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=dashboard.kubeconfig # 设置客户端认证参数，使用上面创建的 Token kubectl config set-credentials dashboard_user \\ --token=${DASHBOARD_LOGIN_TOKEN} \\ --kubeconfig=dashboard.kubeconfig # 设置上下文参数 kubectl config set-context default \\ --cluster=kubernetes \\ --user=dashboard_user \\ --kubeconfig=dashboard.kubeconfig # 设置默认上下文 kubectl config use-context default --kubeconfig=dashboard.kubeconfig 然后下载dashboard.kubeconfig，在登录的时候上传即可进入主界面，如下\n部署Metrics Server  github：https://github.com/kubernetes-sigs/metrics-server\n稳定版：https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/metrics-server\n （1）、下载YAML清单\nfor file in auth-delegator.yaml auth-reader.yaml metrics-apiservice.yaml metrics-server-deployment.yaml metrics-server-service.yaml resource-reader.yaml;do wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/metrics-server/${file}; done （2）、修改metrics-server-deployment.yaml配置清单，如下\napiVersion: v1 kind: ServiceAccount metadata: name: metrics-server namespace: kube-system labels: kubernetes.io/cluster-service: \u0026quot;true\u0026quot; addonmanager.kubernetes.io/mode: Reconcile --- apiVersion: v1 kind: ConfigMap metadata: name: metrics-server-config namespace: kube-system labels: kubernetes.io/cluster-service: \u0026quot;true\u0026quot; addonmanager.kubernetes.io/mode: EnsureExists data: NannyConfiguration: |- apiVersion: nannyconfig/v1alpha1 kind: NannyConfiguration --- apiVersion: apps/v1 kind: Deployment metadata: name: metrics-server-v0.3.6 namespace: kube-system labels: k8s-app: metrics-server kubernetes.io/cluster-service: \u0026quot;true\u0026quot; addonmanager.kubernetes.io/mode: Reconcile version: v0.3.6 spec: selector: matchLabels: k8s-app: metrics-server version: v0.3.6 template: metadata: name: metrics-server labels: k8s-app: metrics-server version: v0.3.6 annotations: seccomp.security.alpha.kubernetes.io/pod: 'docker/default' spec: priorityClassName: system-cluster-critical serviceAccountName: metrics-server nodeSelector: kubernetes.io/os: linux containers: - name: metrics-server image: registry.cn-hangzhou.aliyuncs.com/rookieops/metrics-server-amd64:v0.3.6 command: - /metrics-server - --metric-resolution=30s - --kubelet-insecure-tls # These are needed for GKE, which doesn't support secure communication yet. # Remove these lines for non-GKE clusters, and when GKE supports token-based auth. # - --deprecated-kubelet-completely-insecure=true - --kubelet-port=10250 - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP ports: - containerPort: 443 name: https protocol: TCP - name: metrics-server-nanny image: registry.cn-hangzhou.aliyuncs.com/rookieops/addon-resizer:1.8.6 resources: limits: cpu: 100m memory: 300Mi requests: cpu: 100m memory: 300Mi env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: metrics-server-config-volume mountPath: /etc/config command: - /pod_nanny - --config-dir=/etc/config - --cpu=100m - --extra-cpu=0.5m - --memory=100Mi - --extra-memory=50Mi - --threshold=5 - --deployment=metrics-server-v0.3.6 - --container=metrics-server - --poll-period=300000 - --estimator=exponential # Specifies the smallest cluster (defined in number of nodes) # resources will be scaled to. # - --minClusterSize=2 volumes: - name: metrics-server-config-volume configMap: name: metrics-server-config （3）、修改resource-reader.yaml如下\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:metrics-server labels: kubernetes.io/cluster-service: \u0026quot;true\u0026quot; addonmanager.kubernetes.io/mode: Reconcile rules: - apiGroups: - \u0026quot;\u0026quot; resources: - pods - nodes - namespaces - nodes/stats verbs: - get - list - watch - apiGroups: - \u0026quot;apps\u0026quot; resources: - deployments verbs: - get - list - update - watch - apiGroups: - \u0026quot;extensions\u0026quot; resources: - deployments verbs: - get - list - update - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:metrics-server labels: kubernetes.io/cluster-service: \u0026quot;true\u0026quot; addonmanager.kubernetes.io/mode: Reconcile roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:metrics-server subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system （4）、然后创建 配置清单\nfor file in auth-delegator.yaml auth-reader.yaml metrics-apiservice.yaml metrics-server-deployment.yaml metrics-server-service.yaml resource-reader.yaml;do kubectl apply -f ${file};done （5）、查看\n# kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% master-k8s 195m 19% 1147Mi 66% node01-k8s 117m 11% 885Mi 51% node02-k8s 117m 11% 945Mi 54%  如果出现error: metrics not available yet，重启kubelet（至少我是这样）\n ","description":"纯二进制安装Kubernetes集群","id":37,"section":"posts","tags":["kubernetes","docker"],"title":"二进制安装Kubernetes集群","uri":"https://www.coolops.cn/posts/binary-install-kubernetes/"},{"content":"大家好，我的乔克，一线运维人员，擅长各种挖坑和填坑，目前就职于某交易所，是一名运维技术爱好者，专注于运维技术的研究与分享，分享不限于DevOps、Kubernetes、Linux、Prometheus。\n目前和朋友王二、张冬冬、华仔、小姜等维护了一个微信公众号【运维开发故事（mygsdcsf）】，持续分享运维相关技术干货，你可以通过下面二维码进行关注。\n你还可以加我个人微信号【coolops】，加微信请备注【小二，上茶】，欢迎与我进行交流。\n","description":"Everythig For Ops","id":38,"section":"","tags":null,"title":"关于博主","uri":"https://www.coolops.cn/about/"}]