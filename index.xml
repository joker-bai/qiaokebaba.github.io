<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" 
  xmlns:content="http://purl.org/rss/1.0/modules/content/" 
  xmlns:dc="http://purl.org/dc/elements/1.1/" 
  xmlns:atom="http://www.w3.org/2005/Atom" 
  xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" 
  xmlns:media="http://search.yahoo.com/mrss/">
  <channel>
    <title>十点运维-Linux|Kubernetes|Docker|Prometheus|Python|Golang|云原生</title>
    <link>https://www.coolops.cn/</link>
    <description>Recent content on 十点运维-Linux|Kubernetes|Docker|Prometheus|Python|Golang|云原生</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <managingEditor>qiaokebaba@163.com (乔克叔叔)</managingEditor>
    <webMaster>qiaokebaba@163.com (乔克叔叔)</webMaster>
    <copyright>&amp;copy;{year}, All Rights Reserved</copyright>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    
        <atom:link href="https://www.coolops.cn/index.xml" rel="self" type="application/rss+xml" />
    
    
    
      
      
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      

      
      <item>
        <title>友链</title>
        <link>https://www.coolops.cn/friends/</link>
        <pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/friends/</guid>
        <description>爱生活，爱运维分享 DevOps、自动化运维等实践经验</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        
        
        
        
        
        
      </item>
      
      <item>
        <title>使用blackbox_exporter进行黑盒监控</title>
        <link>https://www.coolops.cn/posts/prometheus-blackbox-exporter/</link>
        <pubDate>Thu, 07 Jan 2021 16:38:58 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 07 Jan 2021 16:38:58 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/prometheus-blackbox-exporter/</guid>
        <description>概述 在监控体系里面，通常我们认为监控分为：白盒监控和黑盒监控。
黑盒监控：主要关注的现象，一般都是正在发生的东西，例如出现一个告警，业务接口不正常，那么这种监控就是站在用户的角度能看到的监控，重点在于能对正在发生的故障进行告警。
白盒监控：主要关注的是原因，也就是系统内部暴露的一些指标，例如 redis 的 info 中显示 redis slave down，这个就是 redis info 显示的一个内部的指标，重点在于原因，可能是在黑盒监控中看到 redis down，而查看内部信息的时候，显示 redis port is refused connection。
Blackbox Exporter Blackbox Exporter 是 Prometheus 社区提供的官方黑盒监控解决方案，其允许用户通过：HTTP、HTTPS、DNS、TCP 以及 ICMP 的方式对网络进行探测。
1、HTTP 测试
 定义 Request Header 信息 判断 Http status / Http Respones Header / Http Body 内容  2、TCP 测试
 业务组件端口状态监听 应用层协议定义与监听  3、ICMP 测试
 主机探活机制  4、POST 测试
 接口联通性  5、SSL 证书过期时间
安装Blackbox Exporter （1）创建YAML配置文件（blackbox-deploymeny.yaml）
apiVersion: v1kind: Servicemetadata:name: blackboxnamespace: monitoringlabels:app: blackboxspec:selector:app: blackboxports:- port: 9115targetPort: 9115---apiVersion: v1kind: ConfigMapmetadata:name: blackbox-confignamespace: monitoringdata:blackbox.</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/prometheus.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>prometheus</category>
            
          
            
              <category>blackbox_exporter</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>Kubeadm搭建的K8S集群升级</title>
        <link>https://www.coolops.cn/posts/kubeadm-upgrade-k8s/</link>
        <pubDate>Wed, 06 Jan 2021 15:33:34 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Wed, 06 Jan 2021 15:33:34 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/kubeadm-upgrade-k8s/</guid>
        <description>升级说明  可用的K8S集群，使用kubeadm搭建 可以小版本升级，也可以跨一个大版本升级，不建议跨两个大版本升级 对集群资源做好备份  升级目标 将kubernetes 1.17.9版本升级到1.18.9版本
现有集群版本已经节点如下：
# kubectl get nodes NAME STATUS ROLES AGE VERSIONecs-968f-0005 Ready node 102d v1.17.9k8s-master Ready master 102d v1.17.9备份集群 kubeadm upgrade 不会影响你的工作负载，只会涉及 Kubernetes 内部的组件，但备份终究是好的。这里主要是对集群的所有资源进行备份，我使用的是一个开源的脚本，项目地址是：https://github.com/solomonxu/k8s-backup-restore
（1）下载脚本
$ mkdir -p /datacd /datagit clone https://github.com/solomonxu/k8s-backup-restore.git（2）执行备份
cd /data/k8s-backup-restore./bin/k8s_backup.sh 如果要恢复怎么办？只需要执行如下步骤。
（1）创建恢复目录
$ mkdir -p /data/k8s-backup-restore/data/restore（2）将需要恢复的YAML清单复制到该目录下
$ cp devops_deployments_gitlab.yaml ../../restore/（3）执行恢复命令
cd /data/k8s-backup-restore./bin/k8s_restore.sh会输出如下信息。
2021-01-06 15:09:43.954083 [11623] - INFO Kubernetes Restore start now. All yaml files which located in path [/data/k8s-backup-restore/data/restore] will be applied.2021-01-06 15:09:43.957265 [11623] - INFO If you want to read the log record of restore, please input command &#39; tail -100f &#39;2021-01-06 15:09:43.</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/kubernetes.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
            
              <category>kubeadm</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>Skywalking的几种使用方式</title>
        <link>https://www.coolops.cn/posts/skywalking/</link>
        <pubDate>Mon, 04 Jan 2021 11:18:28 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Mon, 04 Jan 2021 11:18:28 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/skywalking/</guid>
        <description>链路监控的软件很多，比如skywalking、pinpoint、zipkin等。其中
 Zipkin：由Twitter公司开源，开放源代码分布式的跟踪系统，用于收集服务的定时数据，以解决微服务架构中的延迟问题，包括：数据的收集、存储、查找和展现。 Pinpoint：一款对Java编写的大规模分布式系统的APM工具，由韩国人开源的分布式跟踪组件。 Skywalking：国产的优秀APM组件，是一个对JAVA分布式应用程序集群的业务运行情况进行追踪、告警和分析的系统。  这里主要来介绍Skywalking及其安装。
SkyWalking 是观察性分析平台和应用性能管理系统。提供分布式追踪、服务网格遥测分析、度量聚合和可视化一体化解决方案.支持Java, .Net Core, PHP, NodeJS, Golang, LUA语言探针，支持Envoy + Istio构建的Service Mesh。
安装服务端 这里是安装在kubernetes中，使用helm安装。
安装helm 使用helm3，安装如下：
wget https://get.helm.sh/helm-v3.0.0-linux-amd64.tar.gztar zxvf helm-v3.0.0-linux-amd64.tar.gzmv linux-amd64/helm /usr/bin/ 说明：helm3没有tiller这个服务端了，直接用kubeconfig进行验证通信，所以建议部署在master节点
 安装skywalking skywalking的安装比较简单，具体可以参照其官方文档。
这里采用es做后端存储。
（1）、下载项目
git clone https://github.com/apache/skywalking-kubernetes.git（2）、默认安装使用的是es7做后端存储，安装步骤如下：
$ cd chart$ helm repo add elastic https://helm.elastic.co$ helm dep up skywalking$ helm install &amp;lt;release_name&amp;gt; skywalking -n &amp;lt;namespace&amp;gt;如果要使用es6做后端存储，则按照下面的步骤：
$ helm dep up skywalking$ helm install &amp;lt;release_name&amp;gt; skywalking -n &amp;lt;namespace&amp;gt; --values ./skywalking/values-es6.yaml如果本身有ES，如果是ES7，则用下面的命令安装：
$ cd chart$ helm repo add elastic https://helm.elastic.co$ helm dep up skywalking$ helm install skywalking skywalking -n skywalking \--set elasticsearch.</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/skywalking.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>skywalking</category>
            
          
            
              <category>apm</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>在Kubernetes中使用skywalking进行链路监控</title>
        <link>https://www.coolops.cn/posts/skywalking-in-kubernetes/</link>
        <pubDate>Mon, 04 Jan 2021 11:13:14 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Mon, 04 Jan 2021 11:13:14 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/skywalking-in-kubernetes/</guid>
        <description>skywalking是什么？为什么要给你的应用加上skywalking?
在介绍skywalking之前，我们先来了解一个东西，那就是APM（Application Performance Management）系统。
一、什么是APM系统 APM (Application Performance Management) 即应用性能管理系统，是对企业系统即时监控以实现
对应用程序性能管理和故障管理的系统化的解决方案。应用性能管理，主要指对企业的关键业务应用进
行监测、优化，提高企业应用的可靠性和质量，保证用户得到良好的服务，降低IT总拥有成本。
APM系统是可以帮助理解系统行为、用于分析性能问题的工具，以便发生故障的时候，能够快速定位和
解决问题。
说白了就是随着微服务的的兴起，传统的单体应用拆分为不同功能的小应用，用户的一次请求会经过多个系统，不同服务之间的调用非常复杂，其中任何一个系统出错都可能影响整个请求的处理结果。为了解决这个问题，Google 推出了一个分布式链路跟踪系统 Dapper ，之后各个互联网公司都参照Dapper 的思想推出了自己的分布式链路跟踪系统，而这些系统就是分布式系统下的APM系统。
目前市面上的APM系统有很多，比如skywalking、pinpoint、zipkin等。其中
 Zipkin：由Twitter公司开源，开放源代码分布式的跟踪系统，用于收集服务的定时数据，以解决微服务架构中的延迟问题，包括：数据的收集、存储、查找和展现。 Pinpoint：一款对Java编写的大规模分布式系统的APM工具，由韩国人开源的分布式跟踪组件。 Skywalking：国产的优秀APM组件，是一个对JAVA分布式应用程序集群的业务运行情况进行追踪、告警和分析的系统。  二、什么是skywalking SkyWalking是apache基金会下面的一个开源APM项目，为微服务架构和云原生架构系统设计。它通过探针自动收集所需的标，并进行分布式追踪。通过这些调用链路以及指标，Skywalking APM会感知应用间关系和服务间关系，并进行相应的指标统计。Skywalking支持链路追踪和监控应用组件基本涵盖主流框架和容器，如国产RPC Dubbo和motan等，国际化的spring boot，spring cloud。官方网站：http://skywalking.apache.org/
Skywalking的具有以下几个特点：
 多语言自动探针，Java，.NET Core和Node.JS。 多种监控手段，语言探针和service mesh。 轻量高效。不需要额外搭建大数据平台。 模块化架构。UI、存储、集群管理多种机制可选。 支持告警。 优秀的可视化效果。  Skywalking整体架构如下：
整体架构包含如下三个组成部分：
\1. 探针(agent)负责进行数据的收集，包含了Tracing和Metrics的数据，agent会被安装到服务所在的服务器上，以方便数据的获取。
\2. 可观测性分析平台OAP(Observability Analysis Platform)，接收探针发送的数据，并在内存中使用分析引擎（Analysis Core)进行数据的整合运算，然后将数据存储到对应的存储介质上，比如Elasticsearch、MySQL数据库、H2数据库等。同时OAP还使用查询引擎(Query Core)提供HTTP查询接口。
\3. Skywalking提供单独的UI进行数据的查看，此时UI会调用OAP提供的接口，获取对应的数据然后进行展示。
三、搭建并使用 搭建其实很简单，官方有提供搭建案例。
上文提到skywalking的后端数据存储的介质可以是Elasticsearch、MySQL数据库、H2数据库等，我这里使用Elasticsearch作为数据存储，而且为了便与扩展和收集其他应用日志，我将单独搭建Elasticsearch。
3.1、搭建elasticsearch 为了增加es的扩展性，按角色功能分为master节点、data数据节点、client客户端节点。其整体架构如下：
![image](http://image.coolops.cn/img/1601349715218-4bcc2157-c382-4994-9720-ee82f76f8bf5.png#align=left&amp;amp;display=inline&amp;amp;height=329&amp;amp;margin=[object Object]&amp;amp;name=image.png&amp;amp;originHeight=657&amp;amp;originWidth=1082&amp;amp;size=93774&amp;amp;status=done&amp;amp;style=none&amp;amp;width=541)
其中：
 Elasticsearch数据节点Pods被部署为一个有状态集（StatefulSet） Elasticsearch master节点Pods被部署为一个Deployment Elasticsearch客户端节点Pods是以Deployment的形式部署的，其内部服务将允许访问R/W请求的数据节点 Kibana部署为Deployment，其服务可在Kubernetes集群外部访问  （1）先创建estatic的命名空间（es-ns.yaml）：
apiVersion: v1kind: Namespacemetadata:name: elastic执行kubectl apply -f es-ns.yaml
（2）部署es master
配置清单如下（es-master.yaml）：
---apiVersion: v1kind: ConfigMapmetadata:namespace: elasticname: elasticsearch-master-configlabels:app: elasticsearchrole: masterdata:elasticsearch.</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/skywalking.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
            
              <category>skywalking</category>
            
          
            
              <category>apm</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>使用skywalking監控nginx-ingress</title>
        <link>https://www.coolops.cn/posts/skywalking-nginx-ingress/</link>
        <pubDate>Mon, 04 Jan 2021 10:51:46 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Mon, 04 Jan 2021 10:51:46 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/skywalking-nginx-ingress/</guid>
        <description>前提：有可用的kubernetes集群和skywalking监控。
 软件版本信息：
   软件 版本     kubernetes 1.17.2   nginx-ingress-controller 0.34.1   skywalking 8.1.0   skywalking-nginx-lua 0.2.0    下面直接进入正题&amp;hellip;&amp;hellip;
（1）下载skywalking-nginx-lua
git clone https://github.com/apache/skywalking-nginx-lua.git（2）修改util.lua 文件名，因为其和nginx-ingress默认的一个lua脚本名字冲突，这里改一下。
$ skywalking-nginx-lua/lib/skywalking# 修改文件名$ mv util.lua swutil.lua# 修改文件调用## 可以使用grep查看一下哪些文件有调用这个lua$ grep util `find ./ -type f`./correlation_context.lua:local Util = require(&#39;util&#39;)./segment_ref.lua:local Util = require(&#39;util&#39;)./span.lua:local Util = require(&#39;util&#39;)./tracing_context.lua:local Util = require(&#39;util&#39;)./swutil_test.lua:local Util = require(&#39;util&#39;)# 将里面调用的util都改为swuitl（3）修改Nginx-ingress的nginx.tmpl模板文件，增加Skywalking的配置。
 添加Skywalking Lua脚本扫描路径 增加环境变量读取，如：SW_SERVICE_NAME、SW_SERVICE_INSTANCE_NAME、SW_BACKEND_SERVERS 添加Tracing使用的缓存tracing_buffer 设置Skywalking Lua Agent的初始化方法，并将相关配置从环境变量中提取。 设置http节点的追踪配置。  # Skywalking ENVenv SW_SERVICE_NAME;env SW_SERVICE_INSTANCE_NAME;env SW_BACKEND_SERVERS;events {multi_accept {{ if $cfg.</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/skywalking.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>skywalking</category>
            
          
            
              <category>nginx-lua</category>
            
          
            
              <category>nginx-ingress</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>使用loki和grafana展示ingress-nginx的日志</title>
        <link>https://www.coolops.cn/posts/grafana-loki-nginx-ingress/</link>
        <pubDate>Thu, 17 Dec 2020 16:21:37 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Mon, 21 Dec 2020 17:34:20 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/grafana-loki-nginx-ingress/</guid>
        <description>在kubernetes中,对于日志的收集，对于日志的收集，使用最多的是FEK， 不过有时候，FEK在架构上会略显重， ES的查询及全文检索功能其实使用的不是很多。LoKi做为日志架构的新面孔，由grafana开源， 使用了与Prometheus同样的label理念, 同时摒弃了全文检索的能力， 因此比较轻便, 非常具有潜力。
like Prometheus, but for logs Loki是 Grafana Labs 团队最新的开源项目，是一个水平可扩展，高可用性，多租户的日志聚合系统。它的设计非常经济高效且易于操作，因为它不会为日志内容编制索引，而是为每个日志流编制一组标签。项目受 Prometheus 启发，官方的介绍就是：Like Prometheus, but for logs，类似于 Prometheus 的日志系统
与其他日志聚合系统相比，Loki具有下面的一些特性：
 不对日志进行全文索引。通过存储压缩非结构化日志和仅索引元数据，Loki 操作起来会更简单，更省成本。 通过使用与 Prometheus 相同的标签记录流对日志进行索引和分组，这使得日志的扩展和操作效率更高。 特别适合储存 Kubernetes Pod 日志; 诸如 Pod 标签之类的元数据会被自动删除和编入索引。 受 Grafana 原生支持。  Loki 由以下3个部分组成：
 loki是主服务器，负责存储日志和处理查询。 promtail是代理，负责收集日志并将其发送给 loki，当然也支持其它的收集端如fluentd等 Grafana用于 UI 展示  同时Loki也提示了command line工具，通过这个工具可以使用http的方式与loki进行交互。
架构 
安装 官方提供了多种的部署方式, 这里选择使用helm, 如果只是想试用的话则非常简单, 直接参考helm即可run起来。
helm repo add loki https://grafana.github.io/loki/chartshelm repo updatehelm upgrade --install loki loki/loki-stack我这里为了方便配置，就将其下载下来了，使用如下命令
helm pull loki/loki-stacktar xf loki-stack-2.1.2.tgzhelm install loki loki-stack/配置Nginx-Ingress 这里将NG的日志落盘，便与处理。
（1）、修改ConfigMap，如下：
# Source: ingress-nginx/templates/controller-configmap.</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/loki.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>nginx-ingress</category>
            
          
            
              <category>kubernetes</category>
            
          
            
              <category>helm</category>
            
          
            
              <category>grafana loki</category>
            
          
            
              <category>loki</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>基于Jenkins&#43;Argocd&#43;Argo Rollouts的CI/CD实现并用金丝雀发布</title>
        <link>https://www.coolops.cn/posts/jenkins-argocd-argo-rollouts/</link>
        <pubDate>Wed, 16 Dec 2020 15:15:16 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Mon, 21 Dec 2020 17:34:20 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/jenkins-argocd-argo-rollouts/</guid>
        <description>本文主要介绍使用Jenkins配合argocd以及argo rollouts实现CI/CD。其中jenkins配合argocd做CI/CD前面已经介绍过了，这里不再赘述，不懂的地方可以移步《使用Jenkins和Argocd实现CI/CD》。
本篇文章新增了如下几个功能：
 优化argocd的触发CD的速度 使用argo rollouts进行金丝雀发布 给代码仓库打tag  优化Argocd触发CD的速度 Argo CD每三分钟轮询一次Git存储库，以检测清单的变化。为了消除轮询带来的延迟，可以将API服务器配置为接收Webhook事件。Argo CD支持来自GitHub，GitLab，Bitbucket，Bitbucket Server和Gogs的Git Webhook通知，更多点击官网。
我这里使用Gitlab作为仓库地址。
（1）在argocd中配置webhook token
使用kubectl edit secret argocd-secret -n argocd命令进行配置：
apiVersion: v1kind: Secretmetadata:name: argocd-secretnamespace: argocdtype: Opaquedata:...stringData:# gitlab webhook secretwebhook.gitlab.secret: coolops配置完点击保存会自动生成一个secret，如下：
# kubectl describe secret argocd-secret -n argocdName: argocd-secretNamespace: argocdLabels: app.kubernetes.io/name=argocd-secretapp.kubernetes.io/part-of=argocdAnnotations: Type: OpaqueData====admin.passwordMtime: 20 bytesserver.secretkey: 44 bytestls.crt: 1237 bytestls.key: 1679 byteswebhook.gitlab.secret: 7 bytesadmin.password: 60 bytes（2）在gitlab的代码仓库配置webhook，如下：
由于集群内部证书是无效证书，所有要把Enabled SSL去掉，如下：
然后点击保存，点击测试，看是否链接成功。如果有如下提示则表示webhook配置没问题了。
现在可以进行修改gitlab仓库，观察是否一提交，argocd那边就可以响应了。
使用argo rollouts进行金丝雀发布 关于argo rollouts的更多介绍可以查看之前的文章《使用argo-rollouts实现金丝雀发布》。</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/devops.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
            
              <category>devops</category>
            
          
            
              <category>CI/CD</category>
            
          
            
              <category>argocd</category>
            
          
            
              <category>argo-rollouts</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>使用Argo Rollouts实现金丝雀发布</title>
        <link>https://www.coolops.cn/posts/argo-rollouts/</link>
        <pubDate>Fri, 11 Dec 2020 09:29:23 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Mon, 21 Dec 2020 17:34:20 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/argo-rollouts/</guid>
        <description>什么是argo rollouts Argo-Rollout是一个Kubernetes Controller和对应一系列的CRD，提供更强大的Deployment能力。包括灰度发布、蓝绿部署、更新测试(experimentation)、渐进式交付(progressive delivery)等特性。
支持特性如下：
 蓝绿色更新策略 金丝雀更新策略 细粒度，加权流量转移 自动回rollback和promotion 手动判断 可定制的指标查询和业务KPI分析 入口控制器集成：NGINX，ALB 服务网格集成：Istio，Linkerd，SMI Metric provider集成：Prometheus，Wavefront，Kayenta，Web，Kubernetes Jobs  Argo原理和Deployment差不多，只是加强rollout的策略和流量控制。当spec.template发送变化时，Argo-Rollout就会根据spec.strategy进行rollout，通常会产生一个新的ReplicaSet，逐步scale down之前的ReplicaSet的pod数量。
安装 按着官方文档进行安装，官方地址为：https://argoproj.github.io/argo-rollouts/installation/#kubectl-plugin-installation
（1）在Kubernetes集群中安装argo-rollouts
kubectl create namespace argo-rolloutskubectl apply -n argo-rollouts -f https://raw.githubusercontent.com/argoproj/argo-rollouts/stable/manifests/install.yaml（2）安装argo-rollouts的kubectl plugin
curl -LO https://github.com/argoproj/argo-rollouts/releases/latest/download/kubectl-argo-rollouts-linux-amd64chmod +x ./kubectl-argo-rollouts-linux-amd64mv ./kubectl-argo-rollouts-linux-amd64 /usr/local/bin/kubectl-argo-rollouts金丝雀发布 灰度发布包含Replica Shifting和Traffic Shifting两个过程。
 Replica Shifting：版本替换 Traffic Shifting：流量接入  这里使用官方的demo来进行测试。例子：https://argoproj.github.io/argo-rollouts/getting-started/
Replica Shifting 部署应用 使用如下命令部署示例：
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/rollout.yamlkubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/service.yaml我们先看看第一个rollout.yaml的具体内容，如下：
apiVersion: argoproj.io/v1alpha1kind: Rolloutmetadata:name: rollouts-demospec:replicas: 5strategy:canary:steps:- setWeight: 20- pause: {}- setWeight: 40- pause: {duration: 10}- setWeight: 60- pause: {duration: 10}- setWeight: 80- pause: {duration: 10}revisionHistoryLimit: 2selector:matchLabels:app: rollouts-demotemplate:metadata:labels:app: rollouts-demospec:containers:- name: rollouts-demoimage: argoproj/rollouts-demo:blueports:- name: httpcontainerPort: 8080protocol: TCPresources:requests:memory: 32Micpu: 5m可以看到除了apiVersion，kind以及strategy之外，其他和Deployment无异。</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/argocd.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
            
              <category>argo-rollouts</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>在kubernetes中部署Jenkins并配置动态生成slave</title>
        <link>https://www.coolops.cn/posts/kubernetes-jenkins-install/</link>
        <pubDate>Thu, 10 Dec 2020 08:25:12 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/kubernetes-jenkins-install/</guid>
        <description>一、动态生成Slave 1.1、简介 之前我们都是在物理机或者虚拟机上部署jenkins，但是这种部署方式会有一些难点，如下：
 主 Master 发生单点故障时，整个流程都不可用了 每个 Slave 的配置环境不一样，来完成不同语言的编译打包等操作，但是这些差异化的配置导致管理起来非常不方便，维护起来也是比较费劲 资源分配不均衡，有的 Slave 要运行的 job 出现排队等待，而有的 Slave 处于空闲状态 资源有浪费，每台 Slave 可能是物理机或者虚拟机，当 Slave 处于空闲状态时，也不会完全释放掉资源。  正因为上面的这些种种痛点，我们渴望一种更高效更可靠的方式来完成这个 CI/CD 流程，而 Docker 虚拟化容器技术能很好的解决这个痛点，又特别是在 Kubernetes 集群环境下面能够更好来解决上面的问题，下图是基于 Kubernetes 搭建 Jenkins 集群的简单示意图：
从图上可以看到 Jenkins Master 和 Jenkins Slave 以 Pod 形式运行在 Kubernetes 集群的 Node 上，Master 运行在其中一个节点，并且将其配置数据存储到一个 Volume 上去，Slave 运行在各个节点上，并且它不是一直处于运行状态，它会按照需求动态的创建并自动删除。
这种方式的工作流程大致为：当 Jenkins Master 接受到 Build 请求时，会根据配置的 Label 动态创建一个运行在 Pod 中的 Jenkins Slave 并注册到 Master 上，当运行完 Job 后，这个 Slave 会被注销并且这个 Pod 也会自动删除，恢复到最初状态。
这种方式部署给我们带来如下好处：
 服务高可用，当 Jenkins Master 出现故障时，Kubernetes 会自动创建一个新的 Jenkins Master 容器，并且将 Volume 分配给新创建的容器，保证数据不丢失，从而达到集群服务高可用。 动态伸缩，合理使用资源，每次运行 Job 时，会自动创建一个 Jenkins Slave，Job 完成后，Slave 自动注销并删除容器，资源自动释放，而且 Kubernetes 会根据每个资源的使用情况，动态分配 Slave 到空闲的节点上创建，降低出现因某节点资源利用率高，还排队等待在该节点的情况。 扩展性好，当 Kubernetes 集群的资源严重不足而导致 Job 排队等待时，可以很容易的添加一个 Kubernetes Node 到集群中，从而实现扩展。  1.</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/jenkins.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
            
              <category>jenkins</category>
            
          
            
              <category>devops</category>
            
          
            
              <category>CI/CD</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>Argocd是什么？</title>
        <link>https://www.coolops.cn/posts/argocd/</link>
        <pubDate>Tue, 08 Dec 2020 16:24:51 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/argocd/</guid>
        <description>什么是ArgoCD  Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes.
 Argo CD是一个基于Kubernetes的声明式的GitOps工具。
在说Argo CD之前，我们先来了解一下什么是GitOps。
什么是GitOps GitOps是以Git为基础，使用CI/CD来更新运行在云原生环境的应用，它秉承了DevOps的核心理念&amp;ndash;“构建它并交付它(you built it you ship it)”。
概念说起来有点虚，我画了张图，看了你就明白了。
 当开发人员将开发完成的代码推送到git仓库会触发CI制作镜像并推送到镜像仓库 CI处理完成后，可以手动或者自动修改应用配置，再将其推送到git仓库 GitOps会同时对比目标状态和当前状态，如果两者不一致会触发CD将新的配置部署到集群中  其中，目标状态是Git中的状态，现有状态是集群的里的应用状态。
不用GitOps可以么？
当然可以，我们可以使用kubectl、helm等工具直接发布配置，但这会存在一个很严重的安全问题，那就是密钥共享。
为了让CI系统能够自动的部署应用，我们需要将集群的访问密钥共享给它，这会带来潜在的安全问题。
ArgoCD Argo CD遵循GitOps模式，使用Git存储库存储所需应用程序的配置。
Kubernetes清单可以通过以下几种方式指定:
 kustomize应用程序 helm图表 ksonnet应用程序 jsonnet文件 基于YAML/json配置 配置管理插件配置的任何自定义配置管理工具  Argo CD实现为kubernetes控制器，它持续监视运行中的应用程序，并将当前的活动状态与期望的目标状态进行比较(如Git repo中指定的那样)。如果已部署的应用程序的活动状态偏离了目标状态，则认为是OutOfSync。Argo CD报告和可视化这些差异，同时提供了方法，可以自动或手动将活动状态同步回所需的目标状态。在Git repo中对所需目标状态所做的任何修改都可以自动应用并反映到指定的目标环境中。
Argo CD就处在如下位置：
它的优势总结如下：
 应用定义、配置和环境信息是声明式的，并且可以进行版本控制； 应用部署和生命周期管理是全自动化的，是可审计的，清晰易懂； Argo CD是一个独立的部署工具，支持对多个环境、多个Kubernetes集群上的应用进行统一部署和管理  实践  前提：有一个可用的Kubernetes集群。
 实验环境：
 kubernetes：1.17.2 argo cd：latest  安装Argo CD 安装很简单，不过在实际使用中需要对数据进行持久化。
我这里直接使用官方文档的安装命令：
kubectl create namespace argocdkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml执行成功后会在argocd的namespace下创建如下资源。
# kubectl get all -n argocd NAME READY STATUS RESTARTS AGEpod/argocd-application-controller-0 1/1 Running 0 16hpod/argocd-dex-server-74d9998fdb-mvpmh 1/1 Running 0 16hpod/argocd-redis-59dbdbb8f9-msxrp 1/1 Running 0 16hpod/argocd-repo-server-599bdc7cf5-ccv8l 1/1 Running 0 16hpod/argocd-server-576b4c7ff4-cnp9d 1/1 Running 0 16hNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/argocd-dex-server ClusterIP 10.</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/argocd.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
            
              <category>argocd</category>
            
          
            
              <category>gitops</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>Kubernetes中Pod的一些常见错误</title>
        <link>https://www.coolops.cn/posts/kubernetes-pod-common-error/</link>
        <pubDate>Mon, 07 Dec 2020 14:22:42 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/kubernetes-pod-common-error/</guid>
        <description>本章介绍 Pod 运行异常的排错方法。
一般来说，无论 Pod 处于什么异常状态，都可以执行以下命令来查看 Pod 的状态
 kubectl get pod &amp;lt;pod-name&amp;gt; -o yaml 查看 Pod 的配置是否正确 kubectl describe pod &amp;lt;pod-name&amp;gt; 查看 Pod 的事件 kubectl logs &amp;lt;pod-name&amp;gt; [-c &amp;lt;container-name&amp;gt;] 查看容器日志  这些事件和日志通常都会有助于排查 Pod 发生的问题。
Pod 一直处于 Pending 状态 Pending 说明 Pod 还没有调度到某个 Node 上面。可以通过 kubectl describe pod &amp;lt;pod-name&amp;gt; 命令查看到当前 Pod 的事件，进而判断为什么没有调度。可能的原因包括
 资源不足，集群内所有的 Node 都不满足该 Pod 请求的 CPU、内存、GPU 等资源 HostPort 已被占用，通常推荐使用 Service 对外开放服务端口  Pod 一直处于 Waiting 或 ContainerCreating 状态 首先还是通过 kubectl describe pod &amp;lt;pod-name&amp;gt; 命令查看到当前 Pod 的事件。可能的原因包括
  镜像拉取失败，比如
   配置了错误的镜像 Kubelet 无法访问镜像（国内环境访问 gcr.io 需要特殊处理） 私有镜像的密钥配置错误 镜像太大，拉取超时（可以适当调整 kubelet 的 --image-pull-progress-deadline 和 --runtime-request-timeout 选项）    CNI 网络错误，一般需要检查 CNI 网络插件的配置，比如</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/kubernetes-single.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
            
              <category>pod</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>Kubernetes自定义管理集群的用户</title>
        <link>https://www.coolops.cn/posts/kubernetes-costom-user/</link>
        <pubDate>Mon, 07 Dec 2020 14:20:42 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/kubernetes-costom-user/</guid>
        <description>我们通过如下命令可以看到当前Kubernetes集群的管理用户是什么：
[root@master ssl]# kubectl config view apiVersion: v1clusters:- cluster:certificate-authority-data: DATA+OMITTEDserver: https://172.16.1.128:6443name: cluster1contexts:- context:cluster: cluster1user: adminname: context-cluster1-admin- context:cluster: kubernetesnamespace: kube-systemuser: jokername: joker-contextcurrent-context: context-cluster1-adminkind: Configpreferences: {}users:- name: adminuser:client-certificate-data: REDACTEDclient-key-data: REDACTED- name: jokeruser:client-certificate: /root/k8s/rbac/joker.crtclient-key: /root/k8s/rbac/joker.key下面我们自定义证书文件，然后自定义用户来作为集群的管理用户：
1、我们用的ca证书还是集群的原有证书，我的证书位置在如下位置：
[root@master ssl]# pwd/etc/kubernetes/sslYou have new mail in /var/spool/mail/root[root@master ssl]# lltotal 72-rw-r--r-- 1 root root 1679 Sep 4 18:17 admin-key.pem-rw-r--r-- 1 root root 1391 Sep 4 18:17 admin.</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/kubernetes-single.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>如何使用Kubernetes</title>
        <link>https://www.coolops.cn/posts/kubernetes-why-we-used/</link>
        <pubDate>Mon, 07 Dec 2020 14:05:53 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/kubernetes-why-we-used/</guid>
        <description>你是不是和我有同样的感觉，那就是有些知识你明明知道，但是却没办法将其表述出来，确切的说是不怎么从何说起，如何去说。可能在你的脑袋里对每一个组件，每一个模块都有概念，但是却没办法将其完完整整的说出来，用大佬的话说是知识太片面了，或者是太零碎了，对其没有一个整体的把握，也就是全局观。
我们在运维的时候，从发现问题到定位问题，其实是需要我们对自己的业务逻辑熟悉，对架构熟悉，而且处理问题的思路要清晰，这样才能更快的解决问题。如果没有一个全局的认识，那么处理问题的速度可能会大打折扣。今天我就尝试一下如何说出一套容器云平台。
 以下内容纯个人理解，如果有不对的地方还请指出。
 在说之前先考虑几个问题。
1、为什么要用容器？它能给我们带来什么？和直接用虚拟机有何不同？
2、如何选择容器云？用私有云自己搭建维护还是用公有云现有的容器云平台？
3、怎么完成一个完整的架子？
为什么用容器？ 接触过容器的朋友可能会很清楚的意识到，容器云其实也没那么神秘，它能实现的虚拟机依然可以实现，而且稳定性可能比容器云平台还高。既然这样为什么还有这么多公司用容器呢？
其实，不论是公有云还是私有云，技术已经很成熟了，基本上可以满足我们所有的需求，但是容器也有其得天独厚的优势。
1、启动速度快。容器的启动速度和虚拟机不是一个量级，容器可以达到秒级，而虚拟机基本都是分钟级别。
2、资源利用率高。一个容器镜像最多几百MB，而一个虚拟机镜像基本都是已GB为单位了。再则，使用虚拟机我们需要在底层系统之上再创建一个虚拟机，而容器仅仅是一个进程，可以直接运行在底层系统之上。
3、环境一致性。用虚拟机的时候经常会遇到环境不一致导致应用各种问题，而容器我们只需要将其打包成一个镜像，这个应用运行所需的基础环境都放在这个镜像中了，那么我们就可以在&amp;quot;任何&amp;quot;地方运行。
4、故障迁移、自愈能力高。以kubernetes为例，kubernetes有各种controller来保证实际状态和期望状态高度一致，一个实例在某个节点故障可以快速另起一个实例，基本上无需人为干预，而且故障实例的资源会被回收。
下面简单总结以下容器和虚拟机的区别。
   特性 容器 虚拟机     启动 秒级 分钟级   硬盘使用 一般为 MB 一般为 GB   性能 接近原生 弱于   系统支持量 单机支持上千个容器 一般几十个    如何选择容器云平台？ 这个其实没有一个标准，主要还是看公司现在用的是哪一种。比如公司现在就用的公有云，那么直接用公有云就好，用公有云的好处是底层的网络、存储等等不需要自己去考虑、去维护，基本上只要出钱就可以了。但是如果直接用公有云的服务其实是存在一个依赖问题，对于以后要换云或者下云是有一定的阻碍的。
私有云平台的话定制性高，很多都可以自我控制，但是维护成本大，而且底层的网络、存储这些专业的东西需要专业的人士来处理，稍有不慎就万劫不复~
如何搭架子？  下面以私有云平台中使用kubernetes为例。
 在选的好使用哪种方式过后，一般情况下会评估以下几个问题。
1、集群会有多大的规模
2、集群的高可用怎么保证
3、存储如何选择
4、集群如何监控
5、应用如何部署
6、日志如何收集
7、如何演练故障
1、集群会有多大规模 这个根据自己目前的业务情况来看，我个人认为上下浮动20%，而且集群本身可以快速的扩缩容。
2、集群高可用如何保证 对于kubernetes来说，需要高可用就是master组件和etcd存储。对于etcd来说，本身就支持高可用部署，建议基数节点，可以和master组件部署在相同节点也可以部署在集群外。我个人建议部署在集群外，这样和集群是解耦合的。
对于master组件来说，需要我们自己去做高可用的组件只有apiserver，其他组件可以在配置文件中配置。对于apiserver我们可以使用HAProxy+keepalived，也可以使用NGINX+keepalived等等。其他应用的高可用完全有kubernetes自己控制了。
3、存储如何选择 存储的选择面其实非常多，有商业版的也有开源版的，商业版的好处就太明显了，一切问题找厂家。开源版的维护成本和学习成本比较高，比如ceph，它的维护成本和学习成本就很高。如果公司有现成的存储工程师，那么这都不是问题了，都找他来给你解决。如果用开源产品并且自己维护，就要好好考虑一下了，这个存储软件我能hold住么？如果故障了，我能解决么？它在业界的使用情况和相关的文档丰富么（方便自己处理问题~~）？
4、集群如何监控 对于监控来说，目前容器的监控主流的就是Prometheus，可以说它是为容器而生的。
它主要有以下几个组件。
 Prometheus Server exporters alter pushGateway Prometheus UI  其中Prometheus Server就是中枢神经，exporters就是负责收集监控信息的，alter负责处理报警相关，pushGateway用于接收定制监控信息的，UI就是一个简单的UI界面。整个监控系统看起来并不复杂，复杂的地方在于我们收集到了各种信息，如何提取我们需要的，如果制定相应的规则，说白了就是如何去提高监控的质量，具体的规则其实是需要研发，测试，运维共同制定的。
应用如何部署 我个人觉得这主要是咱们CI/CD部分了，目前做CI/CD的开源软件很多，而且各有所长。目前做CI的开源软件活跃度比较高的就是Jenkins，我们也是使用的Jenkins。Jenkins社区的活跃度很高，而且基本上遇到的问题也都有答案。CD部分，因为咱们用的是容器，我们的制品是镜像，那么只要是可以部署镜像的方法都可以，比如调API，使用helm，或者直接用kubectl命令。
这中间其实主要考虑几个问题：
1、pipeline的规范，共享库的建立、管理等</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/kubernetes-single.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>Kubernetes中强制删除Pod、namespace</title>
        <link>https://www.coolops.cn/posts/kubernetes-delete-namespace/</link>
        <pubDate>Sat, 05 Dec 2020 14:40:53 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/kubernetes-delete-namespace/</guid>
        <description>解决方法  可使用kubectl中的强制删除命令  # 删除PODkubectl delete pod PODNAME --force --grace-period=0# 删除NAMESPACEkubectl delete namespace NAMESPACENAME --force --grace-period=0 若以上方法无法删除，可使用第二种方法，直接从ETCD中删除源数据  # 删除default namespace下的pod名为pod-to-be-deleted-0ETCDCTL_API=3 etcdctl del /registry/pods/default/pod-to-be-deleted-0# 删除需要删除的NAMESPACEetcdctl del /registry/namespaces/NAMESPACENAMEkubernetes 有时候在K8S中删除一个 namespace 会卡住，强制删除也没用，前面我们介绍了可以去 etcd 里面去删除对应的数据，这种方式比较暴力，除此之外我们也可以通过 API 去删除。
首先执行如下命令开启 API 代理：
kubectl proxy然后在另外一个终端中执行如下所示的命令：(将 monitoring 替换成你要删除的 namespace 即可)
kubectl get namespace monitoring -o json | jq &#39;del(.spec.finalizers[] | select(&amp;quot;kubernetes&amp;quot;))&#39; | curl -s -k -H &amp;quot;Content-Type: application/json&amp;quot; -X PUT -o /dev/null --data-binary @- http://localhost:8001/api/v1/namespaces/monitoring/finalize如果这样还是不行，就手动去edit namespace，如下：
 kubectl edit ns &amp;lt;NAMESPACE&amp;gt; -o json然后找到里面的&amp;quot;finalizers&amp;quot;，把它的值设置成一个空数组。</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/kubernetes-single.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>Kubernetes 中优雅停机和零宕机部署</title>
        <link>https://www.coolops.cn/posts/kubernetes-pod-grace-deploy/</link>
        <pubDate>Sat, 05 Dec 2020 14:39:17 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/kubernetes-pod-grace-deploy/</guid>
        <description>设置一个preStop hook，在hook中指定怎么优雅停止容器在K8S中，创建pod、删除pod是最频繁的操作，不论是新增还是升级都会触发。对于新增或者重建我们最关心的是什么时候提供服务，对于删除我们关心的是什么时候不提供服务。那么对于这个临界点在K8S中是如何判定的呢？
在讨论这个临界点之前，我们先看看创建或删除pod的流程。
创建Pod的过程 当API收到创建Pod的请求，然后会将Pod的定义存储到etcd中，然后scheduler会将pod加入到调度队列中（如果没有做调度优先级配置，默认是放在队列最后），然后scheduler会根据预选、优选策略给pod分配一个最有的node节点，然后这个pod会被标记为Scheduled，并将其状态存储到etcd中。
到目前为止pod还并没有被创建，因为创建Pod需要通过kubelet组件来完成。kubelet组件会通过apiserver来获取pod的状态，同样也会上报pod的状态。当某个节点检测到该pod是调度到自己节点的时候，就会在本节点创建这个pod，不过创建pod并不是kubelet自己动手，而是交给下面三个组件来完成。
 容器运行时接口（CRI）：为 Pod 创建容器的组件。 容器网络接口（CNI）：将容器连接到集群网络并分配 IP 地址的组件。 容器存储接口（CSI）：在容器中装载卷的组件。  到现在pod创建完成了，然后会将该pod的状态上报给apiserver并存储在etcd中。
现在pod创建完成了，但是在k8s中，pod并不适合直接提供服务，如果在集群内部是通过service来提供服务，如果集群外部需要访问，是通过ingress来提供访问入口。那如果我ingress以及service的某个pod发生了变化，它们又该如何更新呢？
在这之前先简单介绍一下service和pod的关系。
service和pod是通过label selector（标签选择器）来进行关联的，只要符合service中定义的label selector，就会将其地址和端口维护到Endpoints中，如下：
# kubectl describe svc website Name: websiteNamespace: defaultLabels: &amp;lt;none&amp;gt;Annotations: kubectl.kubernetes.io/last-applied-configuration:{&amp;quot;apiVersion&amp;quot;:&amp;quot;v1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;Service&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;website&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;default&amp;quot;},&amp;quot;spec&amp;quot;:{&amp;quot;ports&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;http&amp;quot;,&amp;quot;...Selector: app=websiteType: ClusterIPIP: 10.101.58.163Port: http 80/TCPTargetPort: 80/TCPEndpoints: 192.168.4.9:80Session Affinity: NoneEvents: &amp;lt;none&amp;gt;Endpoints 对象会从 Pod 中收集所有的 IP 地址和端口，而且不仅一次。在以下情况中，Endpoint 对象将更新一个 endpiont 新列表：
 Pod 创建时。 Pod 删除时。 在 Pod 上修改标签时。  当pod通过Readiness探针后，才标识这个pod真正可用。当pod可用过后，service会通过label selector找到所有匹配的Pod，然后通过k8s更新endpoint，Endpoints也会做相应的更新。
除了service，还有kube-proxy，ingress都会使用到endpoint，它们也会进行相应的更新，kube-proxy会通过endpoint来更新iptables或者ipvs规则，ingress更新endpoint是为了让pod接入外部流量。
所以创建pod的过程以及pod创建完成后的一系列变化可以总结如下：
1、apiserver收到创建pod的请求（可以是直接创建pod的定义，也可以是通过其他控制器来完成的）。
2、Pod 的定义存储在 etcd 中。
3、scheduler参与调度Pod，为其分配最优节点，并把相关信息存储到etcd中。
4、kubelet监听到pod的信息，在节点上创建pod，分配资源以及IP等，将信息存储到etcd中。
5、kubelet等待pod的Readiness探针成功，并对相关的Endpoints对象更改进行通知。
6、Endpoints将新的endpoint添加到列表中。
7、其他组件控制器根据Endpoints做相应的更改配置，比如kube-proxy会重新创建或者更改iptables/ipvs规则等。
**
**
删除Pod的过程 删除pod的主要流程如下：
  用户发送命令删除 Pod，使用的是默认的宽限期（30秒）</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/kubernetes-single.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>kubectl的奇巧淫技</title>
        <link>https://www.coolops.cn/posts/kubernetes-kubectl-other-use/</link>
        <pubDate>Sat, 05 Dec 2020 14:36:24 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/kubernetes-kubectl-other-use/</guid>
        <description>以下文章来源于崔秀龙的博客，文章地址：https://blog.fleeto.us/post/tips-for-kubectl/
原文：Ready-to-use commands and tips for kubectl
作者：Flant staff
Kubectl 是 Kubernetes 最重要的命令行工具。在 Flant，我们会在 Wiki 和 Slack 上相互分享 Kubectl 的妙用（其实我们还有个搜索引擎，不过那就是另外一回事了）。多年以来，我们在 kubectl 方面积累了很多技巧，现在想要将其中的部分分享给社区。
我相信很多读者对这些命令都非常熟悉；然而我还是希望读者能够从本文中有所获益，进而提高生产力。
 下列内容有的是来自我们的工程师，还有的是来自互联网。我们对后者也进行了测试，并且确认其有效性。
 现在开始吧。
获取 Pod 和节点 （1）、我猜你知道如何获取 Kubernetes 集群中所有 Namespace 的 Pod——使用 &amp;ndash;all-namepsaces 就可以。然而不少朋友还不知道，现在这一开关还有了 -A 的缩写。
（2）、如何查找非 running 状态的 Pod 呢？
kubectl get pods -A --field-selector=status.phase!=Running | grep -v Complete顺便一说，&amp;ndash;field-selector 是个值得深入一点的参数。
（3）、如何获取节点列表及其内存容量：
kubectl get no -o json | \jq -r &#39;.items | sort_by(.status.capacity.memory)[]|[.metadata.name,.status.capacity.memory]| @tsv&#39;（4）、获取节点列表，其中包含运行在每个节点上的 Pod 数量：
kubectl get po -o json --all-namespaces | \jq &#39;.items | group_by(.spec.nodeName) | map({&amp;quot;nodeName&amp;quot;: .[0].spec.nodeName, &amp;quot;count&amp;quot;: length}) | sort_by(.</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/kubernetes-single.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
            
              <category>kubectl</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>使用Kustomize管理K8S的yaml清单</title>
        <link>https://www.coolops.cn/posts/kubernetes-kustomize/</link>
        <pubDate>Sat, 05 Dec 2020 14:33:14 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/kubernetes-kustomize/</guid>
        <description>将应用部署到Kubernetes中的方式有很多，目前主流是就是使用kubectl和Helm，不过其先决条件都需要YAML清单文件。
不同由于部署环境的多样化，比如有开发环境、测试环境、预生产环境、生产环境，我们就会针对不同的环境定制各种YAML文件，但是在很多情况下同一个应用在不同的环境可能只做了简单的更改，这样就会导致YAML泛滥。
而**Kustomize 就是用于帮助解决这些问题的开源配置管理工具。**从 Kubernetes v1.14 开始，kubectl 就完全支持 Kustomize 和 kustomization 文件。
kustomize是什么？  kustomize lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is.
 上面是官方对于kustomize的定义。大致是说：kustomize允许您自定义无模板的原始YAML文件来用于多种目的，而原始的YAML则保持不变并可以使用。
kustomize的作用 当我们在K8S中有多套环境的时候，就会面临如下问题：
 多环境多团队多个YAML资源清单 不同环境差异微小，但是不得不copy and change helm稍显复杂，需要额外的学习投入  而kustomize可以很好的解决这些问题：
 kustomize 通过 Base &amp;amp; Overlays 方式方式维护不同环境的应用配置 kustomize 使用 patch 方式复用 Base 配置，并在 Overlay 描述与 Base 应用配置的差异部分来实现资源复用 kustomize 管理的都是 Kubernetes 原生 YAML 文件，不需要学习额外的 DSL 语法  安装 在kubernetes 1.14版本以上，已经集成到kubectl中了，你可以通过kubectl --help来进行查看命令。
如果需要额外安装，直接到https://github.com/kubernetes-sigs/kustomize/releases里进行下载对应的版本。
比如：
wget https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize%2Fv3.8.7/kustomize_v3.8.7_linux_amd64.tar.gztar xf kustomize_v3.8.7_linux_amd64.tar.gzcp kustomize/kustomize /usr/local/bin这样就完成了简单的安装了。
实践测试 背景   版本信息</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/kustomize.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
            
              <category>kustomize</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>kubedog：解决K8S中资源跟踪问题</title>
        <link>https://www.coolops.cn/posts/kubernetes-kubedog/</link>
        <pubDate>Sat, 05 Dec 2020 14:28:59 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/kubernetes-kubedog/</guid>
        <description>Kubedog 是一个开源的 Golang 项目，使用 watch 方式对 Kubernetes 资源进行跟踪，能够方便的用于日常运维和 CI/CD 过程之中，项目中除了一个 CLI 小工具之外，还提供了一组 SDK，用户可以将其中的 Watch 功能集成到自己的系统之中。安装过程非常简单，在项目网页直接下载即可。
源码地址：https://github.com/werf/kubedog.git
kubedog主要使用以下三种方式进行资源跟踪：
 follow rollout multitrack  分别对应三个命令：
 kubedog follow kubedog rollout track kubedog multitrack  rollout track 在 Kubernetes 上运行应用时，通常的做法是使用 kubectl apply 提交 YAML 之后，使用 kubectl get -w 或者 watch kubectl get 之类的命令等待 Pod 启动。如果启动成功，则进行测试等后续动作；如果启动失败，就需要用 kubectl logs、kubectl describe 等命令来查看失败原因。kubedog 能在一定程度上简化这一过程。
例如使用 kubectl run 命令创建一个新的 Deployment 资源，并使用 kubedog 跟进创建进程：
$ kubectl run nginx --image=nginx22...deployment.apps/nginx created$ kubedog rollout track deployment nginx# deploy/nginx added# deploy/nginx rs/nginx-6cc78cbf64 added# deploy/nginx po/nginx-6cc78cbf64-8pnjz added# deploy/nginx po/nginx-6cc78cbf64-8pnjz nginx error: ImagePullBackOff: Back-off pulling image &amp;quot;nginx22&amp;quot;deploy/nginx po/nginx-6cc78cbf64-8pnjz nginx failed: ImagePullBackOff: Back-off pulling image &amp;quot;nginx22&amp;quot;$ echo $?</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/kubedog.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
            
              <category>kubedog</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>使用kubectl-debug诊断kubernetes集群</title>
        <link>https://www.coolops.cn/posts/kubernetes-kubectl-debug/</link>
        <pubDate>Sat, 05 Dec 2020 14:25:50 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/kubernetes-kubectl-debug/</guid>
        <description>kubectl-debug是K8S中Pod的诊断工具，它通过启动一个排错工具容器，并将其加入到目标业务容器的pid, network, user 以及 ipc namespace 中，这时我们就可以在新容器中直接用 netstat, tcpdump 这些熟悉的工具来解决问题了, 而业务容器可以保持最小化, 不需要预装任何额外的排障工具。
其主要有两部分组成：
 kubectl-debug工具，是一个二进制文件 debug-agent，部署在node节点，用于启动关联的排错容器  部署 1、部署kubectl-debug工具 由于其仅仅是一个二进制文件，所以部署很简单，不过由于集群是通过kubeconfig进行授权访问的，所以我们将kubectl-debug工具部署在master上。
项目地址：https://github.com/aylei/kubectl-debug
# install kubectl-debugexport PLUGIN_VERSION=0.1.1#linux x86_64curl -Lo kubectl-debug.tar.gz https://github.com/aylei/kubectl-debug/releases/download/v${PLUGIN_VERSION}/kubectl-debug_${PLUGIN_VERSION}_linux_amd64.tar.gztar -zxvf kubectl-debug.tar.gz kubectl-debugsudo mv kubectl-debug /usr/local/bin/2、部署debug-agent debug-agent以daemonSet的形式部署。直接执行下面命令即可
kubectl apply -f https://raw.githubusercontent.com/aylei/kubectl-debug/master/scripts/agent_daemonset.yml然后查看pod的状态，直到其变为running
kubectl get podNAME READY STATUS RESTARTS AGEdebug-agent-5prws 1/1 Running 0 27ddebug-agent-5qm2w 1/1 Running 1 23ddebug-agent-dd7bt 1/1 Running 1 27ddebug-agent-nfzqp 1/1 Running 0 27ddebug-agent-wx4qp 1/1 Running 0 27d使用 1、pod里只有一个contaienr
kubectl-debug POD_NAME2、pod里有多个container
kubectl-debug POD_NAME -c CONTAINER_NAME3、指定namespace</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/debug.jpg" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
            
              <category>kubectl-debug</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>如何给nginx-ingress进行日志落盘</title>
        <link>https://www.coolops.cn/posts/kubernetes-update-nginx-ingress-log/</link>
        <pubDate>Sat, 05 Dec 2020 14:21:49 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/kubernetes-update-nginx-ingress-log/</guid>
        <description>nginx-ingress-controller的日志
nginx-ingress-controller的日志包括三个部分：
 controller日志： 输出到stdout，通过启动参数中的–log_dir可已配置输出到文件，重定向到文件后会自动轮转，但不会自动清理 accesslog：输出到stdout，通过nginx-configuration中的字段可以配置输出到哪个文件。输出到文件后不会自动轮转或清理 errorlog：输出到stderr，配置方式与accesslog类似。  给controller日志落盘
 给nginx-ingress-controller挂一个hostpath： /data/log/nginx/ 映射到容器里的/var/log/nginx/ ， 给nginx-ingress-controller配置log-dir和logtostderr参数，将日志重定向到/var/log/nginx/中。  controller的日志需要做定时清理。由于controller的日志是通过klog(k8s.io/klog)输出的，会进行日志滚动，所以我们通过脚本定时清理一定时间之前的日志文件即可。
给nginx日志落盘
修改configmap： nginx-configuration。配置accesslog和errorlog的输出路径，替换默认的stdout和stderr。输出路径我们可以与controller一致，便于查找。
accesslog和errorlog都只有一个日志文件，我们可以使用logrotate进行日志轮转，将输出到宿主机上的日志进行轮转和清理。配置如：
$ cat /etc/logrotate.d/nginx.log/data/log/nginx/access.log {su root listrotate 7dailymaxsize 50Mcopytruncatemissingokcreate 0644 www-data root}官方提供的模板中，nginx-ingress-controller默认都是以33这个用户登录启动容器的，因此挂载hostpath路径时存在权限问题。我们需要手动在机器上执行chown -R 33:33 /data/log/nginx.
自动化
nginx日志落盘中，第2、3两点均需要人工运维，有什么解决办法吗？
问题的关键是：有什么办法可以在nginx-ingress-controller容器启动之前加一个hook，将宿主机的指定目录执行chown呢？
可以用initContainer。initcontainer必须在containers中的容器运行前运行完毕并成功退出。再说第二点，我们注意到nginx-ingress-controller的基础镜像中就自带了logrotate，那么问题就简单了，我们将写好的logrotate配置文件以configmap的形式挂载到容器中就可以了。
完整的yaml文件如下：
apiVersion: v1kind: Namespacemetadata:name: ingress-nginxlabels:app.kubernetes.io/name: ingress-nginxapp.kubernetes.io/instance: ingress-nginx---# Source: ingress-nginx/templates/controller-serviceaccount.yamlapiVersion: v1kind: ServiceAccountmetadata:labels:helm.sh/chart: ingress-nginx-2.0.3app.kubernetes.io/name: ingress-nginxapp.kubernetes.io/instance: ingress-nginxapp.kubernetes.io/version: 0.32.0app.kubernetes.io/managed-by: Helmapp.kubernetes.io/component: controllername: ingress-nginxnamespace: ingress-nginx---# Source: ingress-nginx/templates/controller-configmap.</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/ingress.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
            
              <category>ingress-nginx</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>修改kubeadm搭建集群的证书时间</title>
        <link>https://www.coolops.cn/posts/kubernetes-update-tls-time/</link>
        <pubDate>Sat, 05 Dec 2020 14:19:24 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/kubernetes-update-tls-time/</guid>
        <description>（1）、查看当前的证书时间
# kubeadm alpha certs check-expiration[check-expiration] Reading configuration from the cluster...[check-expiration] FYI: You can look at this config file with &#39;kubectl -n kube-system get cm kubeadm-config -oyaml&#39;CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGEDadmin.conf Jun 20, 2021 11:21 UTC 364d no apiserver Jun 20, 2021 11:21 UTC 364d ca no apiserver-etcd-client Jun 20, 2021 11:21 UTC 364d etcd-ca no apiserver-kubelet-client Jun 20, 2021 11:21 UTC 364d ca no controller-manager.conf Jun 20, 2021 11:21 UTC 364d no etcd-healthcheck-client Jun 20, 2021 11:21 UTC 364d etcd-ca no etcd-peer Jun 20, 2021 11:21 UTC 364d etcd-ca no etcd-server Jun 20, 2021 11:21 UTC 364d etcd-ca no front-proxy-client Jun 20, 2021 11:21 UTC 364d front-proxy-ca no scheduler.</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/kubeadm.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
            
              <category>kubeadm</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>修改kubernetes集群中Node的名字</title>
        <link>https://www.coolops.cn/posts/kubernetes-update-node-name/</link>
        <pubDate>Sat, 05 Dec 2020 14:16:14 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/kubernetes-update-node-name/</guid>
        <description>kubeadm搭建的集群 修改之前节点信息如下：
# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-master Ready master 95d v1.17.2k8s-node01 Ready node01 95d v1.17.2k8s-node02 Ready node02 95d v1.17.2（1）、修改主机名
hostnamectl set-hostname k8s-node03（2）、删除node节点
kubectl delete nodes k8s-node02（3）、在删除的Node节点上重置节点
kubeadm reset（4）、在master节点上查看token是否存在（默认24小时过期）
kubeadm token list（5）、如果token不存在则创建token
# kubeadm token create4u4w7j.qv34axysi783i7wg（6）、获取ca证书sha256编码hash值
# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&amp;gt;/dev/null | openssl dgst -sha256 -hex | sed &#39;s/^.* //&#39;d0d8bc0728a15007638f3ff4f047b3ef8b6359fd0dc3cf409efe94147cb32e32（7）、在删除的Node节点执行kubeadm join加入集群
kubeadm join 10.1.10.128:6443 --token cq7ufd.t05znkzrnrinosjn \--discovery-token-ca-cert-hash sha256:b7f7676bf5af4ce251f96390d70e1158f01ddfaa2767f9734b03e238a5b6a798 \--node-name k8s-node03（8）、在master上查看是否已经加入
# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-master Ready master 95d v1.</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/kubernetes-single.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>在Kubernetes中让nginx容器热加载配置文件</title>
        <link>https://www.coolops.cn/posts/kubernetes-nginx-hot-update/</link>
        <pubDate>Sat, 05 Dec 2020 14:13:02 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/kubernetes-nginx-hot-update/</guid>
        <description>Nginx作为WEB服务器被广泛使用。其自身支持热更新，在修改配置文件后，使用nginx -s reload命令可以不停服务重新加载配置。然而对于Dockerize的Nginx来说，如果每次都进到容器里执行对应命令去实现配置重载，这个过程是很痛苦的。本文介绍了一种kubernetes集群下nginx的热更新方案。
首先我们创建正常的一个nginx资源，资源清单如下：
apiVersion: v1kind: ConfigMapmetadata:name: nginx-configdata:default.conf: |-server {server_name localhost;listen 80 default_server;location = /healthz {add_header Content-Type text/plain;return 200 &#39;ok&#39;;}location / {root /usr/share/nginx/html;index index.html index.htm;}error_page 500 502 503 504 /50x.html;location = /50x.html {root /usr/share/nginx/html;}}---apiVersion: apps/v1kind: Deploymentmetadata:name: my-appspec:replicas: 1selector:matchLabels:app: my-apptemplate:metadata:labels:app: my-appspec:containers:- name: my-appimage: nginximagePullPolicy: IfNotPresentvolumeMounts:- name: nginx-configmountPath: /etc/nginx/conf.dvolumes:- name: nginx-configconfigMap:name: nginx-config然后创建资源对象。</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/nginx.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
            
              <category>nginx</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>使用Jenkins和Argocd实现企业级CI/CD</title>
        <link>https://www.coolops.cn/posts/devops-jenkins-argocd/</link>
        <pubDate>Sat, 05 Dec 2020 11:59:51 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/devops-jenkins-argocd/</guid>
        <description>CI/CD并不是陌生的东西，大部分企业都有自己的CI/CD，不过今天我要介绍的是使用Jenkins和GitOps实现CI/CD。
整体架构如下：
涉及的软件以及版本信息如下：
   软件 版本     kubernetes 1.17.9   docker 19.03.13   jenkins 2.249.3   argocd 1.8.0   gitlab 社区版11.8.1   sonarqube 社区版8.5.1   traefik 2.3.3   代码仓库 阿里云仓库    涉及的技术：
 Jenkins shareLibrary Jenkins pipeline Jenkinsfile Argocd sonarqube api操作  软件安装 软件安装我这里不贴具体的安装代码了，所有的代码我都放在了github上，地址：https://github.com/cool-ops/kubernetes-software-yaml.git
所以这里默认你已经安装好所以软件了。
在Jenkins上安装如下插件  kubernetes AnsiColor HTTP Request SonarQube Scanner Utility Steps Email Extension Template Gitlab Hook Gitlab  在Jenkins上配置Kubernetes集群信息 在系统管理&amp;ndash;&amp;gt;系统配置&amp;ndash;&amp;gt;cloud
在Jenkins上配置邮箱地址 系统设置&amp;ndash;&amp;gt;系统配置&amp;ndash;&amp;gt;Email
（1）设置管理员邮箱
配置SMTP服务
在Gitlab上准备一个测试代码 我这里有一个简单的java测试代码，地址如下：https://gitee.com/jokerbai/springboot-helloworld.git
可以将其导入到自己的gitlab仓库。
在Gitlab上创建一个共享库 首先在gitlab上创建一个共享库，我这里取名叫shareLibrary，如下：
然后创建src/org/devops目录，并在该目录下创建一下文件。
它们的内容分别如下：
build.groovy</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/devops/devops.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>DevOps</category>
            
          
            
              <category>Argocd</category>
            
          
            
              <category>Jenkins</category>
            
          
            
              <category>CI/CD</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>Harbor：在云原生中使用，附安装</title>
        <link>https://www.coolops.cn/posts/harbor/</link>
        <pubDate>Sat, 05 Dec 2020 10:51:24 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/harbor/</guid>
        <description>内容主要源自邹佳在云原生社区的分享。
 Harbor 是一个用于存储和分发Docker 镜像的企业级Registry 服务器，由vmware开源，是一个可信的云原生制品仓库，用来存储、签名、管理相关的内容。
Harbor的一切设计都是围绕了云原生展开的，并且会在这个方向一直坚持下去。
在云原生下，镜像就是命脉，一切应用都是围绕着镜像，可以说镜像技术加速了云原生的发展。
那么Harbor在镜像方面做了哪些呢？
让镜像分发更高效 （1）基于策略的内容复制机制 Harbor支持多种过滤器（镜像库、标签等）与多种触发模式（手动、定时等）来实现镜像的推送和拉取。
 初始的时候进行全量拉取 然后再通过增量拉取  在大集群，多机房的情况下，可以使用主从模式（中心-边缘模式）来进行镜像的分发。
（2）提供项目级别的缓存能力 最近开源圈热论的话题就是DockerHub限速问题s，不过“上有政策，下有对策”，Harbor就可以有效解决这个问题。
通过Harbor缓存下来的制品与“本地”制品无异，而且Harbor方面相关的管理策略也可以应用到缓存的镜像上，比如配额、扫描等。目前仅支持上游的DockerHub和其他的Harbor。
在配置缓存时要注意几点：
 要使用缓存功能，则必须在新建项目的时候选择启用，切该项目不可推送 已创建的普通项目无法直接转为缓存项目 Pull镜像的路径有专门的格式。docker pull &amp;lt;harbor-host&amp;gt;/[cache-project-name]/&amp;lt;repository&amp;gt;_path，比如docker pull goharbor.io/my_cache_pro/library/nginx:latest  （3）可以使用P2P进行镜像预热  PS：这里的P2P不是网贷机构，不会暴雷的。
 那什么是P2P技术呢？
在C/S模式中，数据的分发采用专门的服务器，多个客户端都从此服务器获取数据。这种模式的优点是：数据的一致性容易控制，系统也容易管理。但是此种模式的缺点是：因为服务器的个数只有一个(即便有多个也非常有限)，系统容易出现单一失效点；单一服务器面对众多的客户端，由于CPU能力、内存大小、网络带宽的限制，可同时服务的客户端非常有限，可扩展性差。
P2P技术正是为了解决这些问题而提出来的一种对等网络结构。在P2P网络中，每个节点既可以从其他节点得到服务，也可以向其他节点提供服务。这样，庞大的终端资源被利用起来，一举解决了C/S模式中的两个弊端。
Harbor也充分利用了这种技术，将所选镜像提前分发到P2P网络中，以便客户端拉取的时候直接从P2P网络中拉取。
  基于策略实现自动化
   Repository过滤器 Tag过滤器 标签（Label）过滤器 漏洞状态条件 签名状态条件    基于事件触发或定时触发
  Harbor目前仅支持：
 Dragonfly。是阿里自研并捐献给 CNCF 的 P2P 文件分发系统。 Kraken。是 Uber 开源的点对点（P2P）Docker 容器仓库。  让镜像分发更安全 容器实际上是不透明的，被封装成一个个繁琐的镜像。当越来越多的镜像被创建时，没有人能确定镜像里到底封装了什么，所以日常使用的镜像都面临着严重的安全问题。
Harbor在安全方面做了严格的把关。
（1）对镜像进行签名  基于开源的Notary实现镜像的签名   基于GPG实现对Helm Chart的签名支持  （2）对镜像进行漏洞扫描  通过插件化接入扫描器，对镜像进行漏洞扫描   可以生成相应的扫描报告，以便与管理相关漏洞信息和了解安全威胁程度  （3）通过策略限制不安全镜像分发 可以在项目里设置相关的安全策略，以阻止不合安全规范的镜像分发。
 基于内容信任，仅允许通过认证的镜像分发 基于危害级别，可以设置危害级别限制镜像分发  （4）通过规则来限制Tag不被覆盖或删除 默认情况下Harbor里的镜像是可以被覆盖和删除的，不过可以添加一些规则来保护一些Tag不被删除，比如latest的tag。</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/harbor.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>Cloud Native</category>
            
          
            
              <category>Harbor</category>
            
          
            
              <category>kubernetes</category>
            
          
            
              <category>docker-compose</category>
            
          
            
              <category>docker</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>Kubeadm搭建单master节点的Kubernetes集群</title>
        <link>https://www.coolops.cn/posts/kubeadm-install-single-master-kubernetes/</link>
        <pubDate>Sat, 05 Dec 2020 10:38:08 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/kubeadm-install-single-master-kubernetes/</guid>
        <description>环境准备
3个节点，都是 Centos 7.6 系统，内核版本：3.10.0-957.12.2.el7.x86_64，在每个节点上添加 hosts 信息：
$ cat /etc/hosts
172.16.1.128 k8s-master172.16.1.129 k8s-node01172.16.1.130 k8s-node02禁用防火墙：
$ systemctl stop firewalld$ systemctl disable firewalld禁用SELINUX：
$ setenforce 0$ cat /etc/selinux/configSELINUX=disabled创建/etc/sysctl.d/k8s.conf文件，添加如下内容：
net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1执行如下命令使修改生效：
$ modprobe br_netfilter$ sysctl -p /etc/sysctl.d/k8s.conf安装 ipvs
$ cat &amp;gt; /etc/sysconfig/modules/ipvs.modules &amp;lt;&amp;lt;EOF#!/bin/bashmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4EOF$ chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; bash /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4上面脚本创建了的/etc/sysconfig/modules/ipvs.</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/kubeadm.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
            
              <category>kubeadm</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>Kubeadm搭建高可用Kubernetes集群</title>
        <link>https://www.coolops.cn/posts/kubeadm-install-multi-master-kubernetes/</link>
        <pubDate>Fri, 04 Dec 2020 14:13:34 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/kubeadm-install-multi-master-kubernetes/</guid>
        <description>PS: 最近经常有朋友问我有没有用kubeadm搭建高可用集群的文档，说实在的我确实没有，我自己测试的话就用kubeadm单master版，公司用的话就用二进制搭建的。所以就找了个下班时间搭建测试了一番。希望对大家有帮助！如果觉得有用的话就帮忙点个关注或转发吧，哈哈~
节点规划信息    名称 IP     k8s-master01 10.1.10.100   k8s-master02 10.1.10.101   k8s-master03 10.1.10.102   k8s-node01 10.1.10.103   k8s-lb 10.1.10.200    基础环境配置 环境信息    系统 CentOS7.6.1810     内核版本 4.9.220       软件 版本     kubernetes 1.18.2   docker 19.0.3    环境初始化 （1）、配置主机名，以k8s-master01为例
hostnamectl set-hostname k8s-master01（1）、配置主机hosts映射
10.1.10.100 k8s-master0110.1.10.101 k8s-master0210.1.10.102 k8s-master0310.1.10.103 k8s-node0110.1.10.200 k8s-lb配置完后可以通过如下命令测试
for host in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-lb;do ping -c 1 $host;done 这里ping k8s-node01不通，是因为我们还没配置VIP</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/kubeadm.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
        
        
          
            
          
        
        
          
            
          
        
      </item>
      
      <item>
        <title>Etcd集群常用操作</title>
        <link>https://www.coolops.cn/posts/handle-etcd-cluster/</link>
        <pubDate>Fri, 04 Dec 2020 11:10:44 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/handle-etcd-cluster/</guid>
        <description>备份 备份策略：
 每两个小时用命令对etcd进行备份 备份数据保留2天  ETCDCTL_API=3 /opt/etcd/bin/etcdctl snapshot save /root/backup/etcd_$(date &amp;quot;+%Y%m%d%H%M%S&amp;quot;).db 恢复 （1）、停止kube-apiserver，确保不会再写入数据
systemctl stop kube-apiserver （2）、停止所有节点etcd
systemctl stop etcd （3）、将etcd节点上原有的数据目录备份（具体的目录可以在etcd的配置文件中查看）
mv /var/lib/etcd/default.etcd{,.bak} （4）、将备份的数据拷贝到所有etcd节点
scp etcd_20200106152240.db 10.1.10.129:/root/backup/ scp etcd_20200106152240.db 10.1.10.130:/root/backup/ （5）、使用命令进行恢复
# 在etcd-1上 ETCDCTL_API=3 /opt/etcd/bin/etcdctl snapshot --endpoints=&amp;quot;https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379&amp;quot; --cacert=/opt/etcd/ssl/etcd-ca.pem --cert=/opt/etcd/ssl/etcd-server.pem --key=/opt/etcd/ssl/etcd-server-key.pem restore ~/backup/etcd_20200106152240.db --name=etcd-1 --data-dir=/var/lib/etcd/default.etcd --initial-cluster=&amp;quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380,etcd-3=https://10.1.10.130:2380&amp;quot; --initial-cluster-token=&amp;quot;etcd-cluster&amp;quot; --initial-advertise-peer-urls=https://10.1.10.128:2380 # 在etcd-2上 ETCDCTL_API=3 /opt/etcd/bin/etcdctl snapshot --endpoints=&amp;quot;https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379&amp;quot; --cacert=/opt/etcd/ssl/etcd-ca.pem --cert=/opt/etcd/ssl/etcd-server.pem --key=/opt/etcd/ssl/etcd-server-key.pem restore ~/backup/etcd_20200106152240.db --name=etcd-2 --data-dir=/var/lib/etcd/default.etcd --initial-cluster=&amp;quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380,etcd-3=https://10.1.10.130:2380&amp;quot; --initial-cluster-token=&amp;quot;etcd-cluster&amp;quot; --initial-advertise-peer-urls=https://10.1.10.129:2380 # 在etcd-3上 ETCDCTL_API=3 /opt/etcd/bin/etcdctl snapshot --endpoints=&amp;quot;https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379&amp;quot; --cacert=/opt/etcd/ssl/etcd-ca.pem --cert=/opt/etcd/ssl/etcd-server.pem --key=/opt/etcd/ssl/etcd-server-key.pem restore ~/backup/etcd_20200106152240.db --name=etcd-3 --data-dir=/var/lib/etcd/default.etcd --initial-cluster=&amp;quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380,etcd-3=https://10.1.10.130:2380&amp;quot; --initial-cluster-token=&amp;quot;etcd-cluster&amp;quot; --initial-advertise-peer-urls=https://10.1.10.130:2380 （6）、启动etcd服务
systemctl start etcd （7）、启动kube-apiserver
systemctl start kube-apiserver （8）、查看集群状态
# /opt/etcd/bin/etcdctl \ &amp;gt; --ca-file=/opt/etcd/ssl/etcd-ca.</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/etcd.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>etcd</category>
            
          
        
        
        
      </item>
      
      <item>
        <title>二进制安装Kubernetes集群</title>
        <link>https://www.coolops.cn/posts/binary-install-kubernetes/</link>
        <pubDate>Fri, 04 Dec 2020 09:40:07 +0800</pubDate>
        <author>qiaokebaba@163.com (乔克叔叔)</author>
        <atom:modified>Thu, 10 Dec 2020 08:31:22 +0800</atom:modified>
        <guid>https://www.coolops.cn/posts/binary-install-kubernetes/</guid>
        <description>基础规划 1、IP规划
   主机名 IP 配置 软件     master-k8s 10.1.10.128 2C4G etcd,apiserver,controller-manager,scheduler   node01-k8s 10.1.10.129 2C4G etcd,docker,kubelet,kube-proxy   node02-k8s 10.1.10.130 2C4G etcd,docker,kubelet,kube-proxy    2、软件规划
   软件名 版本     etcd 3.3.18   docker-ce 19.03.5-3   cfssl 1.2.0   kubernetes 1.16.4   flannel 0.11.0   cni 0.8.3    3、目录规划
   目录名 用途     /var/log/kubernetes/ 存储日志   /root/kubernetes/install 安装软件目录   /opt/kubernetes K8S项目部署目录，其中ssl是证书目录，bin是二进制目录，config是配置文件目录   /opt/etcd Etcd项目部署目录，子目录功能如上   /opt/cni cni二进制文件保存目录   /opt/kubernetes/ssl 证书生成目录   /opt/kubernetes/kubeconfig kubeconfig统一生成目录   /opt/kubernetes/system 系统组件YAML文件存储目录    mkdir /var/log/kubernetes /root/kubernetes/{ssl,install,kubeconfig} /root/kubernetes/ssl /opt/etcd/{bin,config,ssl} /opt/kubernetes/{bin,config,ssl} /opt/cni/bin -p 主机初始化配置 2、设置hostname</description>
        
        <dc:creator>乔克叔叔</dc:creator>
        <media:content url="https://www.coolops.cnimages/kubernetes/kubernetes.png" medium="image"><media:title type="html">featured image</media:title></media:content>
        
        
        
          
            
              <category>kubernetes</category>
            
          
            
              <category>docker</category>
            
          
        
        
        
      </item>
      

    
  </channel>
</rss>