[{"content":"CI/CD并不是陌生的东西，大部分企业都有自己的CI/CD，不过今天我要介绍的是使用Jenkins和GitOps实现CI/CD。\n整体架构如下：\n涉及的软件以及版本信息如下：\n   软件 版本     kubernetes 1.17.9   docker 19.03.13   jenkins 2.249.3   argocd 1.8.0   gitlab 社区版11.8.1   sonarqube 社区版8.5.1   traefik 2.3.3   代码仓库 阿里云仓库    涉及的技术：\n Jenkins shareLibrary Jenkins pipeline Jenkinsfile Argocd sonarqube api操作  软件安装 软件安装我这里不贴具体的安装代码了，所有的代码我都放在了github上，地址：https://github.com/cool-ops/kubernetes-software-yaml.git\n所以这里默认你已经安装好所以软件了。\n在Jenkins上安装如下插件  kubernetes AnsiColor HTTP Request SonarQube Scanner Utility Steps Email Extension Template Gitlab Hook Gitlab  在Jenkins上配置Kubernetes集群信息 在系统管理\u0026ndash;\u0026gt;系统配置\u0026ndash;\u0026gt;cloud\n在Jenkins上配置邮箱地址 系统设置\u0026ndash;\u0026gt;系统配置\u0026ndash;\u0026gt;Email\n（1）设置管理员邮箱\n配置SMTP服务\n在Gitlab上准备一个测试代码 我这里有一个简单的java测试代码，地址如下：https://gitee.com/jokerbai/springboot-helloworld.git\n可以将其导入到自己的gitlab仓库。\n在Gitlab上创建一个共享库 首先在gitlab上创建一个共享库，我这里取名叫shareLibrary，如下：\n然后创建src/org/devops目录，并在该目录下创建一下文件。\n它们的内容分别如下：\nbuild.groovy\npackage org.devops\r// docker容器直接build\rdef DockerBuild(buildShell){\rsh \u0026quot;\u0026quot;\u0026quot;\r${buildShell}\r\u0026quot;\u0026quot;\u0026quot;\r}\rsendEmail.groovy\npackage org.devops\r//定义邮件内容\rdef SendEmail(status,emailUser){\remailext body: \u0026quot;\u0026quot;\u0026quot;\r\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body leftmargin=\u0026quot;8\u0026quot; marginwidth=\u0026quot;0\u0026quot; topmargin=\u0026quot;8\u0026quot; marginheight=\u0026quot;4\u0026quot; offset=\u0026quot;0\u0026quot;\u0026gt; \u0026lt;table width=\u0026quot;95%\u0026quot; cellpadding=\u0026quot;0\u0026quot; cellspacing=\u0026quot;0\u0026quot; style=\u0026quot;font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sans-serif\u0026quot;\u0026gt; \u0026lt;tr\u0026gt;\r本邮件由系统自动发出，无需回复！\u0026lt;br/\u0026gt;\r各位同事，大家好，以下为${JOB_NAME}项目构建信息\u0026lt;/br\u0026gt;\r\u0026lt;td\u0026gt;\u0026lt;font color=\u0026quot;#CC0000\u0026quot;\u0026gt;构建结果 - ${status}\u0026lt;/font\u0026gt;\u0026lt;/td\u0026gt;\r\u0026lt;/tr\u0026gt;\r\u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;b\u0026gt;\u0026lt;font color=\u0026quot;#0B610B\u0026quot;\u0026gt;构建信息\u0026lt;/font\u0026gt;\u0026lt;/b\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;项目名称：${JOB_NAME}\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;构建编号：${BUILD_ID}\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;构建状态: ${status} \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;项目地址：\u0026lt;a href=\u0026quot;${BUILD_URL}\u0026quot;\u0026gt;${BUILD_URL}\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;构建日志：\u0026lt;a href=\u0026quot;${BUILD_URL}console\u0026quot;\u0026gt;${BUILD_URL}console\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026quot;\u0026quot;\u0026quot;,\rsubject: \u0026quot;Jenkins-${JOB_NAME}项目构建信息 \u0026quot;,\rto: emailUser\r}\rsonarAPI.groovy\npackage ore.devops\r// 封装HTTP请求\rdef HttpReq(requestType,requestUrl,requestBody){\r// 定义sonar api接口\rdef sonarServer = \u0026quot;http://sonar.devops.svc.cluster.local:9000/api\u0026quot;\rresult = httpRequest authentication: 'sonar-admin-user',\rhttpMode: requestType,\rcontentType: \u0026quot;APPLICATION_JSON\u0026quot;,\rconsoleLogResponseBody: true,\rignoreSslErrors: true,\rrequestBody: requestBody,\rurl: \u0026quot;${sonarServer}/${requestUrl}\u0026quot;\rreturn result\r}\r// 获取soanr项目的状态\rdef GetSonarStatus(projectName){\rdef apiUrl = \u0026quot;project_branches/list?project=${projectName}\u0026quot;\r// 发请求\rresponse = HttpReq(\u0026quot;GET\u0026quot;,apiUrl,\u0026quot;\u0026quot;)\r// 对返回的文本做JSON解析\rresponse = readJSON text: \u0026quot;\u0026quot;\u0026quot;${response.content}\u0026quot;\u0026quot;\u0026quot;\r// 获取状态值\rresult = response[\u0026quot;branches\u0026quot;][0][\u0026quot;status\u0026quot;][\u0026quot;qualityGateStatus\u0026quot;]\rreturn result\r}\r// 获取sonar项目，判断项目是否存在\rdef SearchProject(projectName){\rdef apiUrl = \u0026quot;projects/search?projects=${projectName}\u0026quot;\r// 发请求\rresponse = HttpReq(\u0026quot;GET\u0026quot;,apiUrl,\u0026quot;\u0026quot;)\rprintln \u0026quot;搜索的结果：${response}\u0026quot;\r// 对返回的文本做JSON解析\rresponse = readJSON text: \u0026quot;\u0026quot;\u0026quot;${response.content}\u0026quot;\u0026quot;\u0026quot;\r// 获取total字段，该字段如果是0则表示项目不存在,否则表示项目存在\rresult = response[\u0026quot;paging\u0026quot;][\u0026quot;total\u0026quot;]\r// 对result进行判断\rif (result.toString() == \u0026quot;0\u0026quot;){\rreturn \u0026quot;false\u0026quot;\r}else{\rreturn \u0026quot;true\u0026quot;\r}\r}\r// 创建sonar项目\rdef CreateProject(projectName){\rdef apiUrl = \u0026quot;projects/create?name=${projectName}\u0026amp;project=${projectName}\u0026quot;\r// 发请求\rresponse = HttpReq(\u0026quot;POST\u0026quot;,apiUrl,\u0026quot;\u0026quot;)\rprintln(response)\r}\r// 配置项目质量规则\rdef ConfigQualityProfiles(projectName,lang,qpname){\rdef apiUrl = \u0026quot;qualityprofiles/add_project?language=${lang}\u0026amp;project=${projectName}\u0026amp;qualityProfile=${qpname}\u0026quot;\r// 发请求\rresponse = HttpReq(\u0026quot;POST\u0026quot;,apiUrl,\u0026quot;\u0026quot;)\rprintln(response)\r}\r// 获取质量阈ID\rdef GetQualityGateId(gateName){\rdef apiUrl = \u0026quot;qualitygates/show?name=${gateName}\u0026quot;\r// 发请求\rresponse = HttpReq(\u0026quot;GET\u0026quot;,apiUrl,\u0026quot;\u0026quot;)\r// 对返回的文本做JSON解析\rresponse = readJSON text: \u0026quot;\u0026quot;\u0026quot;${response.content}\u0026quot;\u0026quot;\u0026quot;\r// 获取total字段，该字段如果是0则表示项目不存在,否则表示项目存在\rresult = response[\u0026quot;id\u0026quot;]\rreturn result\r}\r// 更新质量阈规则\rdef ConfigQualityGate(projectKey,gateName){\r// 获取质量阈id\rgateId = GetQualityGateId(gateName)\rapiUrl = \u0026quot;qualitygates/select?projectKey=${projectKey}\u0026amp;gateId=${gateId}\u0026quot;\r// 发请求\rresponse = HttpReq(\u0026quot;POST\u0026quot;,apiUrl,\u0026quot;\u0026quot;)\rprintln(response)\r}\r//获取Sonar质量阈状态\rdef GetProjectStatus(projectName){\rapiUrl = \u0026quot;project_branches/list?project=${projectName}\u0026quot;\rresponse = HttpReq(\u0026quot;GET\u0026quot;,apiUrl,'')\rresponse = readJSON text: \u0026quot;\u0026quot;\u0026quot;${response.content}\u0026quot;\u0026quot;\u0026quot;\rresult = response[\u0026quot;branches\u0026quot;][0][\u0026quot;status\u0026quot;][\u0026quot;qualityGateStatus\u0026quot;]\r//println(response)\rreturn result\r}\rsonarqube.groovy\npackage ore.devops\rdef SonarScan(projectName,projectDesc,projectPath){\r// sonarScanner安装地址\rdef sonarHome = \u0026quot;/opt/sonar-scanner\u0026quot;\r// sonarqube服务端地址\rdef sonarServer = \u0026quot;http://sonar.devops.svc.cluster.local:9000/\u0026quot;\r// 以时间戳为版本\rdef scanTime = sh returnStdout: true, script: 'date +%Y%m%d%H%m%S'\rscanTime = scanTime - \u0026quot;\\n\u0026quot;\rsh \u0026quot;\u0026quot;\u0026quot;\r${sonarHome}/bin/sonar-scanner -Dsonar.host.url=${sonarServer} \\\r-Dsonar.projectKey=${projectName} \\\r-Dsonar.projectName=${projectName} \\\r-Dsonar.projectVersion=${scanTime} \\\r-Dsonar.login=admin \\\r-Dsonar.password=admin \\\r-Dsonar.ws.timeout=30 \\\r-Dsonar.projectDescription=\u0026quot;${projectDesc}\u0026quot; \\\r-Dsonar.links.homepage=http://www.baidu.com \\\r-Dsonar.sources=${projectPath} \\\r-Dsonar.sourceEncoding=UTF-8 \\\r-Dsonar.java.binaries=target/classes \\\r-Dsonar.java.test.binaries=target/test-classes \\\r-Dsonar.java.surefire.report=target/surefire-reports -X echo \u0026quot;${projectName} scan success!\u0026quot;\r\u0026quot;\u0026quot;\u0026quot;\r}\rtools.groovy\npackage org.devops\r//格式化输出\rdef PrintMes(value,color){\rcolors = ['red' : \u0026quot;\\033[40;31m \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;${value}\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; \\033[0m\u0026quot;,\r'blue' : \u0026quot;\\033[47;34m ${value} \\033[0m\u0026quot;,\r'green' : \u0026quot;\u001b[1;32m\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;${value}\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u001b[m\u0026quot;,\r'green1' : \u0026quot;\\033[40;32m \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;${value}\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; \\033[0m\u0026quot; ]\ransiColor('xterm') {\rprintln(colors[color])\r}\r}\r// 获取镜像版本\rdef createVersion() {\r// 定义一个版本号作为当次构建的版本，输出结果 20191210175842_69\rreturn new Date().format('yyyyMMddHHmmss') + \u0026quot;_${env.BUILD_ID}\u0026quot;\r}\r// 获取时间\rdef getTime() {\r// 定义一个版本号作为当次构建的版本，输出结果 20191210175842\rreturn new Date().format('yyyyMMddHHmmss')\r}\r在Gitlab上创建一个YAML管理仓库 我这里创建了一个叫devops-cd的共享仓库，如下：\n然后以应用名创建一个目录，并在目录下创建以下几个文件。\n它们的内容分别如下。\nservice.yaml\nkind: Service\rapiVersion: v1\rmetadata:\rname: the-service\rnamespace: default\rspec:\rselector:\rdeployment: hello\rtype: NodePort\rports:\r- protocol: TCP\rport: 8080\rtargetPort: 8080\ringress.yaml\napiVersion: extensions/v1beta1\rkind: Ingress\rmetadata:\rname: the-ingress namespace: default\rspec:\rrules:\r- host: test.coolops.cn http:\rpaths:\r- backend:\rserviceName: the-service servicePort: 8080 path: /\rdeploymeny.yaml\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: the-deployment\rnamespace: default\rspec:\rreplicas: 3\rselector:\rmatchLabels:\rdeployment: hello\rtemplate:\rmetadata:\rlabels:\rdeployment: hello\rspec:\rcontainers:\r- args:\r- -jar\r- /opt/myapp.jar\r- --server.port=8080\rcommand:\r- java\renv:\r- name: HOST_IP\rvalueFrom:\rfieldRef:\rapiVersion: v1\rfieldPath: status.hostIP\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/myapp:latest\rimagePullPolicy: IfNotPresent\rlifecycle:\rpreStop:\rexec:\rcommand:\r- /bin/sh\r- -c\r- /bin/sleep 30\rlivenessProbe:\rfailureThreshold: 3\rhttpGet:\rpath: /hello\rport: 8080\rscheme: HTTP\rinitialDelaySeconds: 60\rperiodSeconds: 15\rsuccessThreshold: 1\rtimeoutSeconds: 1\rname: myapp\rports:\r- containerPort: 8080\rname: http\rprotocol: TCP\rreadinessProbe:\rfailureThreshold: 3\rhttpGet:\rpath: /hello\rport: 8080\rscheme: HTTP\rperiodSeconds: 15\rsuccessThreshold: 1\rtimeoutSeconds: 1\rresources:\rlimits:\rcpu: \u0026quot;1\u0026quot;\rmemory: 2Gi\rrequests:\rcpu: 100m\rmemory: 1Gi\rterminationMessagePath: /dev/termination-log\rterminationMessagePolicy: File\rdnsPolicy: ClusterFirstWithHostNet\rimagePullSecrets:\r- name: gitlab-registry\rkustomization.yaml\n# Example configuration for the webserver\r# at https://github.com/monopole/hello\rcommonLabels:\rapp: hello\rresources:\r- deployment.yaml\r- service.yaml\r- ingress.yaml\rapiVersion: kustomize.config.k8s.io/v1beta1\rkind: Kustomization\rimages:\r- name: registry.cn-hangzhou.aliyuncs.com/rookieops/myapp\rnewTag: \u0026quot;20201127150733_70\u0026quot;\rnamespace: dev\r在Jenkins上配置共享库 （1）需要在Jenkins上添加凭证\n（2）在Jenkins的系统配置里面配置共享库（系统管理\u0026ndash;\u0026gt;系统配置）\n然后点击应用并保存\n然后我们可以用一个简单的Jenkinsfile测试一下共享库，看配置是否正确。\n在Jenkins上创建一个项目，如下：\n然后在最地下的pipeline处贴入以下代码：\ndef labels = \u0026quot;slave-${UUID.randomUUID().toString()}\u0026quot;\r// 引用共享库\r@Library(\u0026quot;jenkins_shareLibrary\u0026quot;)\r// 应用共享库中的方法\rdef tools = new org.devops.tools()\rpipeline {\ragent {\rkubernetes {\rlabel labels\ryaml \u0026quot;\u0026quot;\u0026quot;\rapiVersion: v1\rkind: Pod\rmetadata:\rlabels:\rsome-label: some-label-value\rspec:\rvolumes:\r- name: docker-sock\rhostPath:\rpath: /var/run/docker.sock\rtype: ''\rcontainers:\r- name: jnlp\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/inbound-agent:4.3-4\r- name: maven\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/maven:3.5.0-alpine\rcommand:\r- cat\rtty: true\r- name: docker\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/docker:19.03.11\rcommand:\r- cat\rtty: true\rvolumeMounts:\r- name: docker-sock\rmountPath: /var/run/docker.sock\r\u0026quot;\u0026quot;\u0026quot;\r}\r}\rstages {\rstage('Checkout') {\rsteps {\rscript{\rtools.PrintMes(\u0026quot;拉代码\u0026quot;,\u0026quot;green\u0026quot;)\r}\r}\r}\rstage('Build') {\rsteps {\rcontainer('maven') {\rscript{\rtools.PrintMes(\u0026quot;编译打包\u0026quot;,\u0026quot;green\u0026quot;)\r}\r}\r}\r}\rstage('Make Image') {\rsteps {\rcontainer('docker') {\rscript{\rtools.PrintMes(\u0026quot;构建镜像\u0026quot;,\u0026quot;green\u0026quot;)\r}\r}\r}\r}\r}\r}\r然后点击保存并运行，如果看到输出有颜色，就代表共享库配置成功，如下：\n到此共享库配置完成。\n编写Jenkinsfile 整个java的Jenkinsfile如下：\ndef labels = \u0026quot;slave-${UUID.randomUUID().toString()}\u0026quot;\r// 引用共享库\r@Library(\u0026quot;jenkins_shareLibrary\u0026quot;)\r// 应用共享库中的方法\rdef tools = new org.devops.tools()\rdef sonarapi = new org.devops.sonarAPI()\rdef sendEmail = new org.devops.sendEmail()\rdef build = new org.devops.build()\rdef sonar = new org.devops.sonarqube()\r// 前端传来的变量\rdef gitBranch = env.branch\rdef gitUrl = env.git_url\rdef buildShell = env.build_shell\rdef image = env.image\rdef dockerRegistryUrl = env.dockerRegistryUrl\rdef devops_cd_git = env.devops_cd_git\rpipeline {\ragent {\rkubernetes {\rlabel labels\ryaml \u0026quot;\u0026quot;\u0026quot;\rapiVersion: v1\rkind: Pod\rmetadata:\rlabels:\rsome-label: some-label-value\rspec:\rvolumes:\r- name: docker-sock\rhostPath:\rpath: /var/run/docker.sock\rtype: ''\r- name: maven-cache\rpersistentVolumeClaim:\rclaimName: maven-cache-pvc\rcontainers:\r- name: jnlp\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/inbound-agent:4.3-4\r- name: maven\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/maven:3.5.0-alpine\rcommand:\r- cat\rtty: true\rvolumeMounts:\r- name: maven-cache\rmountPath: /root/.m2\r- name: docker\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/docker:19.03.11\rcommand:\r- cat\rtty: true\rvolumeMounts:\r- name: docker-sock\rmountPath: /var/run/docker.sock\r- name: sonar-scanner\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/sonar-scanner:latest\rcommand:\r- cat\rtty: true\r- name: kustomize\rimage: registry.cn-hangzhou.aliyuncs.com/rookieops/kustomize:v3.8.1\rcommand:\r- cat\rtty: true\r\u0026quot;\u0026quot;\u0026quot;\r}\r}\renvironment{\rauth = 'joker'\r}\roptions {\rtimestamps() // 日志会有时间\rskipDefaultCheckout() // 删除隐式checkout scm语句\rdisableConcurrentBuilds() //禁止并行\rtimeout(time:1,unit:'HOURS') //设置流水线超时时间\r}\rstages {\r// 拉取代码\rstage('GetCode') {\rsteps {\rcheckout([$class: 'GitSCM', branches: [[name: \u0026quot;${gitBranch}\u0026quot;]],\rdoGenerateSubmoduleConfigurations: false,\rextensions: [],\rsubmoduleCfg: [],\ruserRemoteConfigs: [[credentialsId: '83d2e934-75c9-48fe-9703-b48e2feff4d8', url: \u0026quot;${gitUrl}\u0026quot;]]])\r}\r}\r// 单元测试和编译打包\rstage('Build\u0026amp;Test') {\rsteps {\rcontainer('maven') {\rscript{\rtools.PrintMes(\u0026quot;编译打包\u0026quot;,\u0026quot;blue\u0026quot;)\rbuild.DockerBuild(\u0026quot;${buildShell}\u0026quot;)\r}\r}\r}\r}\r// 代码扫描\rstage('CodeScanner') {\rsteps {\rcontainer('sonar-scanner') {\rscript {\rtools.PrintMes(\u0026quot;代码扫描\u0026quot;,\u0026quot;green\u0026quot;)\rtools.PrintMes(\u0026quot;搜索项目\u0026quot;,\u0026quot;green\u0026quot;)\rresult = sonarapi.SearchProject(\u0026quot;${JOB_NAME}\u0026quot;)\rprintln(result)\rif (result == \u0026quot;false\u0026quot;){\rprintln(\u0026quot;${JOB_NAME}---项目不存在,准备创建项目---\u0026gt; ${JOB_NAME}！\u0026quot;)\rsonarapi.CreateProject(\u0026quot;${JOB_NAME}\u0026quot;)\r} else {\rprintln(\u0026quot;${JOB_NAME}---项目已存在！\u0026quot;)\r}\rtools.PrintMes(\u0026quot;代码扫描\u0026quot;,\u0026quot;green\u0026quot;)\rsonar.SonarScan(\u0026quot;${JOB_NAME}\u0026quot;,\u0026quot;${JOB_NAME}\u0026quot;,\u0026quot;src\u0026quot;)\rsleep 10\rtools.PrintMes(\u0026quot;获取扫描结果\u0026quot;,\u0026quot;green\u0026quot;)\rresult = sonarapi.GetProjectStatus(\u0026quot;${JOB_NAME}\u0026quot;)\rprintln(result)\rif (result.toString() == \u0026quot;ERROR\u0026quot;){\rtoemail.Email(\u0026quot;代码质量阈错误！请及时修复！\u0026quot;,userEmail)\rerror \u0026quot; 代码质量阈错误！请及时修复！\u0026quot;\r} else {\rprintln(result)\r}\r}\r}\r}\r}\r// 构建镜像\rstage('BuildImage') {\rsteps {\rwithCredentials([[$class: 'UsernamePasswordMultiBinding',\rcredentialsId: 'dockerhub',\rusernameVariable: 'DOCKER_HUB_USER',\rpasswordVariable: 'DOCKER_HUB_PASSWORD']]) {\rcontainer('docker') {\rscript{\rtools.PrintMes(\u0026quot;构建镜像\u0026quot;,\u0026quot;green\u0026quot;)\rimageTag = tools.createVersion()\rsh \u0026quot;\u0026quot;\u0026quot;\rdocker login ${dockerRegistryUrl} -u ${DOCKER_HUB_USER} -p ${DOCKER_HUB_PASSWORD}\rdocker build -t ${image}:${imageTag} .\rdocker push ${image}:${imageTag}\rdocker rmi ${image}:${imageTag}\r\u0026quot;\u0026quot;\u0026quot;\r}\r}\r}\r}\r}\r// 部署\rstage('Deploy') {\rsteps {\rwithCredentials([[$class: 'UsernamePasswordMultiBinding',\rcredentialsId: 'ci-devops',\rusernameVariable: 'DEVOPS_USER',\rpasswordVariable: 'DEVOPS_PASSWORD']]){\rcontainer('kustomize') {\rscript{\rAPP_DIR=\u0026quot;${JOB_NAME}\u0026quot;.split(\u0026quot;_\u0026quot;)[0]\rsh \u0026quot;\u0026quot;\u0026quot;\rgit remote set-url origin http://${DEVOPS_USER}:${DEVOPS_PASSWORD}@${devops_cd_git}\rgit config --global user.name \u0026quot;Administrator\u0026quot;\rgit config --global user.email \u0026quot;coolops@163.com\u0026quot;\rgit clone http://${DEVOPS_USER}:${DEVOPS_PASSWORD}@${devops_cd_git} /opt/devops-cd\rcd /opt/devops-cd\rgit pull\rcd /opt/devops-cd/${APP_DIR}\rkustomize edit set image ${image}:${imageTag}\rgit commit -am 'image update'\rgit push origin master\r\u0026quot;\u0026quot;\u0026quot;\r}\r}\r}\r}\r}\r// 接口测试\rstage('InterfaceTest') {\rsteps{\rsh 'echo \u0026quot;接口测试\u0026quot;'\r}\r}\r}\r// 构建后的操作\rpost {\rsuccess {\rscript{\rprintln(\u0026quot;success:只有构建成功才会执行\u0026quot;)\rcurrentBuild.description += \u0026quot;\\n构建成功！\u0026quot;\r// deploy.AnsibleDeploy(\u0026quot;${deployHosts}\u0026quot;,\u0026quot;-m ping\u0026quot;)\rsendEmail.SendEmail(\u0026quot;构建成功\u0026quot;,toEmailUser)\r// dingmes.SendDingTalk(\u0026quot;构建成功 ✅\u0026quot;)\r}\r}\rfailure {\rscript{\rprintln(\u0026quot;failure:只有构建失败才会执行\u0026quot;)\rcurrentBuild.description += \u0026quot;\\n构建失败!\u0026quot;\rsendEmail.SendEmail(\u0026quot;构建失败\u0026quot;,toEmailUser)\r// dingmes.SendDingTalk(\u0026quot;构建失败 ❌\u0026quot;)\r}\r}\raborted {\rscript{\rprintln(\u0026quot;aborted:只有取消构建才会执行\u0026quot;)\rcurrentBuild.description += \u0026quot;\\n构建取消!\u0026quot;\rsendEmail.SendEmail(\u0026quot;取消构建\u0026quot;,toEmailUser)\r// dingmes.SendDingTalk(\u0026quot;构建失败 ❌\u0026quot;,\u0026quot;暂停或中断\u0026quot;)\r}\r}\r}\r}\r需要在Jenkins上创建两个凭证，一个id叫dockerhub，一个叫ci-devops，还有一个叫sonar-admin-user。\ndockerhub是登录镜像仓库的用户名和密码。\nci-devops是管理YAML仓库的用户名和密码。\nsonar-admin-user是管理sonarqube的用户名和密码。\n然后将这个Jenkinsfile保存到shareLibrary的根目录下，命名为java.Jenkinsfile。\n在Jenkins上配置项目 在Jenkins上新建一个项目，如下：\n然后添加以下参数化构建。\n然后在流水线处配置Pipeline from SCM\n此处需要注意脚本名。\n然后点击应用保存，并运行。\n也可以在sonarqube上看到代码扫描的结果。\n在Argocd上配置CD流程 在argocd上添加代码仓库，如下：\n然后创建应用，如下：\n点击创建后，如下：\n点进去可以看到更多的详细信息。\n argocd有一个小bug，它ingress的健康检查必须要loadBalance有值，不然就不通过，但是并不影响使用。\n 然后可以正常访问应用了。\nnode项目的Jenkinsfile大同小异，由于我没有测试用例，所以并没有测试。\n集成Gitlab，通过Webhook触发Jenkins 在Jenkins中选择项目，在项目中配置gitlab触发，如下：\n生成token，如下\n在gitlab上配置集成。进入项目\u0026ndash;\u0026gt;项目设置\u0026ndash;\u0026gt;集成\n配置Jenkins上生成的回调URL和TOKEN\n到此配置完成，然后点击下方test，可以观察是否触发流水线。\n也可以通过修改仓库代码进行测试。\n写在最后 本片文章是纯操作步骤，大家在测试的时候可能会对Jenkinsfile做细微的调整，不过整体没什么问题。\n","description":"","id":0,"section":"posts","tags":["DevOps","Jenkins","Argocd","CI/CD"],"title":"实现Jenkins和Argocd实现CI/CD","uri":"http://qiaokebaba.github.io/posts/devops-jenkins-argocd/"},{"content":"PS: 最近经常有朋友问我有没有用kubeadm搭建高可用集群的文档，说实在的我确实没有，我自己测试的话就用kubeadm单master版，公司用的话就用二进制搭建的。所以就找了个下班时间搭建测试了一番。希望对大家有帮助！如果觉得有用的话就帮忙点个关注或转发吧，哈哈~\n节点规划信息    名称 IP     k8s-master01 10.1.10.100   k8s-master02 10.1.10.101   k8s-master03 10.1.10.102   k8s-node01 10.1.10.103   k8s-lb 10.1.10.200    基础环境配置 环境信息    系统 CentOS7.6.1810     内核版本 4.9.220       软件 版本     kubernetes 1.18.2   docker 19.0.3    环境初始化 （1）、配置主机名，以k8s-master01为例\nhostnamectl set-hostname k8s-master01\r（1）、配置主机hosts映射\n10.1.10.100 k8s-master01\r10.1.10.101 k8s-master02\r10.1.10.102 k8s-master03\r10.1.10.103 k8s-node01\r10.1.10.200 k8s-lb\r配置完后可以通过如下命令测试\nfor host in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-lb;do ping -c 1 $host;done\r 这里ping k8s-node01不通，是因为我们还没配置VIP\n （2）、禁用防火墙\nsystemctl stop firewalld\rsystemctl disable firewalld\r（3）、关闭selinux\nsetenforce 0\rsed -i \u0026quot;s/^SELINUX=.*/SELINUX=disabled/g\u0026quot; /etc/sysconfig/selinux\rsed -i \u0026quot;s/^SELINUX=.*/SELINUX=disabled/g\u0026quot; /etc/selinux/config\r（4）、关闭swap分区\nswapoff -a \u0026amp;\u0026amp; sysctl -w vm.swappiness=0\r（5）、时间同步\nyum install chrony -y\rsystemctl enable chronyd\rsystemctl start chronyd\rchronyc sources\r（6）、配置ulimt\nulimit -SHn 65535\r（7）、配置内核参数\ncat \u0026gt;\u0026gt; /etc/sysctl.d/k8s.conf \u0026lt;\u0026lt; EOF\rnet.bridge.bridge-nf-call-ip6tables = 1\rnet.bridge.bridge-nf-call-iptables = 1\rnet.ipv4.ip_forward = 1\rvm.swappiness=0\rEOF\r使之生效\nsysctl -p\r（8）、master之间添加互信（按需）\nssh-keygen\rssh-copy-id 10.1.10.101\rssh-copy-id 10.1.10.102\r内核升级 由于centos7.6的系统默认内核版本是3.10，3.10的内核有很多BUG，最常见的一个就是group memory leak。\n（1）、下载所需要的内核版本，我这里采用rpm安装，所以直接下载的rpm包\nwget https://cbs.centos.org/kojifiles/packages/kernel/4.9.220/37.el7/x86_64/kernel-4.9.220-37.el7.x86_64.rpm\r（2）、执行rpm升级即可\nrpm -ivh kernel-4.9.220-37.el7.x86_64.rpm\r（3）、升级完reboot，然后查看内核是否成功升级\nreboot\runame -r\r组件安装 安装ipvs （1）、安装ipvs需要的软件\n由于我准备使用ipvs作为kube-proxy的代理模式，所以需要安装相应的软件包。\nyum install ipvsadm ipset sysstat conntrack libseccomp -y\r（2）、加载模块\ncat \u0026gt; /etc/sysconfig/modules/ipvs.modules \u0026lt;\u0026lt;EOF\r#!/bin/bash\rmodprobe -- ip_vs\rmodprobe -- ip_vs_rr\rmodprobe -- ip_vs_wrr\rmodprobe -- ip_vs_sh\rmodprobe -- nf_conntrack\rmodprobe -- ip_tables\rmodprobe -- ip_set\rmodprobe -- xt_set\rmodprobe -- ipt_set\rmodprobe -- ipt_rpfilter\rmodprobe -- ipt_REJECT\rmodprobe -- ipip\rEOF\r 注意：在内核4.19版本nf_conntrack_ipv4已经改为nf_conntrack\n 配置重启自动加载\nchmod 755 /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; bash /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; lsmod | grep -e ip_vs -e nf_conntrack\r安装docker-ce # 安装需要的软件\ryum install -y yum-utils device-mapper-persistent-data lvm2\r# 添加yum源\ryum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\r查看是否有docker-ce包\n# yum list | grep docker-ce\rcontainerd.io.x86_64 1.2.13-3.1.el7 docker-ce-stable\rdocker-ce.x86_64 3:19.03.8-3.el7 docker-ce-stable\rdocker-ce-cli.x86_64 1:19.03.8-3.el7 docker-ce-stable\rdocker-ce-selinux.noarch 17.03.3.ce-1.el7 docker-ce-stable\r安装docker-ce\nyum install docker-ce-19.03.8-3.el7 -y\rsystemctl start docker\rsystemctl enable docker\r配置镜像加速\ncurl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io\rsystemctl restart docker\r安装kubernetes组件 添加yum源\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo\r[kubernetes]\rname=Kubernetes\rbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\renabled=1\rgpgcheck=0\rrepo_gpgcheck=0\rgpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\rhttp://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\rEOF\r安装软件\nyum install -y kubelet-1.18.2-0 kubeadm-1.18.2-0 kubectl-1.18.2-0 --disableexcludes=kubernetes\r将kubelet设置为开机自启动\nsystemctl enable kubelet.service\r 以上操作在所有节点执行\n 集群初始化 配置VIP 高可用采用的是HAProxy+Keepalived，HAProxy和KeepAlived以守护进程的方式在所有Master节点部署。\n安装软件 yum install keepalived haproxy -y\r配置haproxy 所有master节点的配置相同，如下：\n#---------------------------------------------------------------------\r# Global settings\r#---------------------------------------------------------------------\rglobal\r# to have these messages end up in /var/log/haproxy.log you will\r# need to:\r#\r# 1) configure syslog to accept network log events. This is done\r# by adding the '-r' option to the SYSLOGD_OPTIONS in\r# /etc/sysconfig/syslog\r#\r# 2) configure local2 events to go to the /var/log/haproxy.log\r# file. A line like the following can be added to\r# /etc/sysconfig/syslog\r#\r# local2.* /var/log/haproxy.log\r#\rlog 127.0.0.1 local2\rchroot /var/lib/haproxy\rpidfile /var/run/haproxy.pid\rmaxconn 4000\ruser haproxy\rgroup haproxy\rdaemon\r# turn on stats unix socket\rstats socket /var/lib/haproxy/stats\r#---------------------------------------------------------------------\r# common defaults that all the 'listen' and 'backend' sections will\r# use if not designated in their block\r#---------------------------------------------------------------------\rdefaults\rmode http\rlog global\roption httplog\roption dontlognull\roption http-server-close\roption redispatch\rretries 3\rtimeout http-request 10s\rtimeout queue 1m\rtimeout connect 10s\rtimeout client 1m\rtimeout server 1m\rtimeout http-keep-alive 10s\rtimeout check 10s\rmaxconn 3000\r#---------------------------------------------------------------------\r# kubernetes apiserver frontend which proxys to the backends\r#---------------------------------------------------------------------\rfrontend kubernetes\rmode tcp\rbind *:16443\roption tcplog\rdefault_backend kubernetes-apiserver\r#---------------------------------------------------------------------\r# round robin balancing between the various backends\r#---------------------------------------------------------------------\rbackend kubernetes-apiserver\rmode tcp\rbalance roundrobin\rserver k8s-master01 10.1.10.100:6443 check\rserver k8s-master02 10.1.10.101:6443 check\rserver k8s-master03 10.1.10.102:6443 check\r#---------------------------------------------------------------------\r# collection haproxy statistics message\r#---------------------------------------------------------------------\rlisten stats\rbind *:9999\rstats auth admin:P@ssW0rd\rstats refresh 5s\rstats realm HAProxy\\ Statistics\rstats uri /admin?stats\r配置keepalived k8s-master01\n! Configuration File for keepalived\rglobal_defs {\rnotification_email {\racassen@firewall.loc\rfailover@firewall.loc\rsysadmin@firewall.loc\r}\rnotification_email_from Alexandre.Cassen@firewall.loc\rsmtp_server 192.168.200.1\rsmtp_connect_timeout 30\rrouter_id LVS_DEVEL\rvrrp_skip_check_adv_addr\rvrrp_garp_interval 0\rvrrp_gna_interval 0\r}\r# 定义脚本\rvrrp_script check_apiserver {\rscript \u0026quot;/etc/keepalived/check_apiserver.sh\u0026quot; interval 2 weight -5 fall 3 rise 2 }\rvrrp_instance VI_1 {\rstate MASTER\rinterface eth33\rvirtual_router_id 51\rpriority 100\radvert_int 1\rauthentication {\rauth_type PASS\rauth_pass 1111\r}\rvirtual_ipaddress {\r10.1.10.200\r}\r# 调用脚本\rtrack_script {\rcheck_apiserver\r}\r}\rk8s-master02\n! Configuration File for keepalived\rglobal_defs {\rnotification_email {\racassen@firewall.loc\rfailover@firewall.loc\rsysadmin@firewall.loc\r}\rnotification_email_from Alexandre.Cassen@firewall.loc\rsmtp_server 192.168.200.1\rsmtp_connect_timeout 30\rrouter_id LVS_DEVEL\rvrrp_skip_check_adv_addr\rvrrp_garp_interval 0\rvrrp_gna_interval 0\r}\r# 定义脚本\rvrrp_script check_apiserver {\rscript \u0026quot;/etc/keepalived/check_apiserver.sh\u0026quot; interval 2 weight -5 fall 3 rise 2 }\rvrrp_instance VI_1 {\rstate MASTER\rinterface eth33\rvirtual_router_id 51\rpriority 99\radvert_int 1\rauthentication {\rauth_type PASS\rauth_pass 1111\r}\rvirtual_ipaddress {\r10.1.10.200\r}\r# 调用脚本\rtrack_script {\rcheck_apiserver\r}\r}\rk8s-master03\n! Configuration File for keepalived\rglobal_defs {\rnotification_email {\racassen@firewall.loc\rfailover@firewall.loc\rsysadmin@firewall.loc\r}\rnotification_email_from Alexandre.Cassen@firewall.loc\rsmtp_server 192.168.200.1\rsmtp_connect_timeout 30\rrouter_id LVS_DEVEL\rvrrp_skip_check_adv_addr\rvrrp_garp_interval 0\rvrrp_gna_interval 0\r}\r# 定义脚本\rvrrp_script check_apiserver {\rscript \u0026quot;/etc/keepalived/check_apiserver.sh\u0026quot; interval 2 weight -5 fall 3 rise 2 }\rvrrp_instance VI_1 {\rstate MASTER\rinterface ens33\rvirtual_router_id 51\rpriority 98 advert_int 1\rauthentication {\rauth_type PASS\rauth_pass 1111\r}\rvirtual_ipaddress {\r10.1.10.200\r}\r# 调用脚本\r#track_script {\r# check_apiserver\r#}\r}\r 先把健康检查关闭，等部署好了过后再打开\n 编写健康检测脚本check-apiserver.sh\n#!/bin/bash\rfunction check_apiserver(){\rfor ((i=0;i\u0026lt;5;i++))\rdo\rapiserver_job_id=${pgrep kube-apiserver}\rif [[ ! -z ${apiserver_job_id} ]];then\rreturn\relse\rsleep 2\rfi\rdone\rapiserver_job_id=0\r}\r# 1-\u0026gt;running 0-\u0026gt;stopped\rcheck_apiserver\rif [[ $apiserver_job_id -eq 0 ]];then\r/usr/bin/systemctl stop keepalived\rexit 1\relse\rexit 0\rfi\r启动haproxy和keepalived\nsystemctl enable --now keepalived\rsystemctl enable --now haproxy\r部署master （1）、在k8s-master01上，编写kubeadm.yaml配置文件，如下：\ncat \u0026gt;\u0026gt; kubeadm.yaml \u0026lt;\u0026lt;EOF\rapiVersion: kubeadm.k8s.io/v1beta2\rkind: ClusterConfiguration\rkubernetesVersion: v1.18.2\rimageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers\rcontrolPlaneEndpoint: \u0026quot;k8s-lb:16443\u0026quot;\rnetworking:\rdnsDomain: cluster.local\rpodSubnet: 192.168.0.0/16\rserviceSubnet: 10.96.0.0/12\r---\rapiVersion: kubeproxy.config.k8s.io/v1alpha1\rkind: KubeProxyConfiguration\rfeatureGates:\rSupportIPVSProxyMode: true\rmode: ipvs\rEOF\r提前下载镜像\nkubeadm config images pull --config kubeadm.yaml\r进行初始化\nkubeadm init --config kubeadm.yaml --upload-certs\rW0509 22:37:40.702752 65728 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]\r[init] Using Kubernetes version: v1.18.2\r[preflight] Running pre-flight checks\r[WARNING IsDockerSystemdCheck]: detected \u0026quot;cgroupfs\u0026quot; as the Docker cgroup driver. The recommended driver is \u0026quot;systemd\u0026quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/\r[preflight] Pulling images required for setting up a Kubernetes cluster\r[preflight] This might take a minute or two, depending on the speed of your internet connection\r[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\r[kubelet-start] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot;\r[kubelet-start] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot;\r[kubelet-start] Starting the kubelet\r[certs] Using certificateDir folder \u0026quot;/etc/kubernetes/pki\u0026quot;\r[certs] Generating \u0026quot;ca\u0026quot; certificate and key\r[certs] Generating \u0026quot;apiserver\u0026quot; certificate and key\r[certs] apiserver serving cert is signed for DNS names [k8s-master01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local k8s-lb] and IPs [10.96.0.1 10.1.10.100]\r[certs] Generating \u0026quot;apiserver-kubelet-client\u0026quot; certificate and key\r[certs] Generating \u0026quot;front-proxy-ca\u0026quot; certificate and key\r[certs] Generating \u0026quot;front-proxy-client\u0026quot; certificate and key\r[certs] Generating \u0026quot;etcd/ca\u0026quot; certificate and key\r[certs] Generating \u0026quot;etcd/server\u0026quot; certificate and key\r[certs] etcd/server serving cert is signed for DNS names [k8s-master01 localhost] and IPs [10.1.10.100 127.0.0.1 ::1]\r[certs] Generating \u0026quot;etcd/peer\u0026quot; certificate and key\r[certs] etcd/peer serving cert is signed for DNS names [k8s-master01 localhost] and IPs [10.1.10.100 127.0.0.1 ::1]\r[certs] Generating \u0026quot;etcd/healthcheck-client\u0026quot; certificate and key\r[certs] Generating \u0026quot;apiserver-etcd-client\u0026quot; certificate and key\r[certs] Generating \u0026quot;sa\u0026quot; key and public key\r[kubeconfig] Using kubeconfig folder \u0026quot;/etc/kubernetes\u0026quot;\r[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address\r[kubeconfig] Writing \u0026quot;admin.conf\u0026quot; kubeconfig file\r[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address\r[kubeconfig] Writing \u0026quot;kubelet.conf\u0026quot; kubeconfig file\r[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address\r[kubeconfig] Writing \u0026quot;controller-manager.conf\u0026quot; kubeconfig file\r[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address\r[kubeconfig] Writing \u0026quot;scheduler.conf\u0026quot; kubeconfig file\r[control-plane] Using manifest folder \u0026quot;/etc/kubernetes/manifests\u0026quot;\r[control-plane] Creating static Pod manifest for \u0026quot;kube-apiserver\u0026quot;\r[control-plane] Creating static Pod manifest for \u0026quot;kube-controller-manager\u0026quot;\rW0509 22:37:47.750722 65728 manifests.go:225] the default kube-apiserver authorization-mode is \u0026quot;Node,RBAC\u0026quot;; using \u0026quot;Node,RBAC\u0026quot;\r[control-plane] Creating static Pod manifest for \u0026quot;kube-scheduler\u0026quot;\rW0509 22:37:47.764989 65728 manifests.go:225] the default kube-apiserver authorization-mode is \u0026quot;Node,RBAC\u0026quot;; using \u0026quot;Node,RBAC\u0026quot;\r[etcd] Creating static Pod manifest for local etcd in \u0026quot;/etc/kubernetes/manifests\u0026quot;\r[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026quot;/etc/kubernetes/manifests\u0026quot;. This can take up to 4m0s\r[apiclient] All control plane components are healthy after 20.024575 seconds\r[upload-config] Storing the configuration used in ConfigMap \u0026quot;kubeadm-config\u0026quot; in the \u0026quot;kube-system\u0026quot; Namespace\r[kubelet] Creating a ConfigMap \u0026quot;kubelet-config-1.18\u0026quot; in namespace kube-system with the configuration for the kubelets in the cluster\r[upload-certs] Storing the certificates in Secret \u0026quot;kubeadm-certs\u0026quot; in the \u0026quot;kube-system\u0026quot; Namespace\r[upload-certs] Using certificate key:\rf25e738324e4f027703f24b55d47d28f692b4edc21c2876171ff87877dc8f2ef\r[mark-control-plane] Marking the node k8s-master01 as control-plane by adding the label \u0026quot;node-role.kubernetes.io/master=''\u0026quot;\r[mark-control-plane] Marking the node k8s-master01 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]\r[bootstrap-token] Using token: 3k4vr0.x3y2nc3ksfnei4y1\r[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\r[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes\r[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\r[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\r[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\r[bootstrap-token] Creating the \u0026quot;cluster-info\u0026quot; ConfigMap in the \u0026quot;kube-public\u0026quot; namespace\r[kubelet-finalize] Updating \u0026quot;/etc/kubernetes/kubelet.conf\u0026quot; to point to a rotatable kubelet client certificate and key\r[addons] Applied essential addon: CoreDNS\r[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address\r[addons] Applied essential addon: kube-proxy\rYour Kubernetes control-plane has initialized successfully!\rTo start using your cluster, you need to run the following as a regular user:\rmkdir -p $HOME/.kube\rsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\rsudo chown $(id -u):$(id -g) $HOME/.kube/config\rYou should now deploy a pod network to the cluster.\rRun \u0026quot;kubectl apply -f [podnetwork].yaml\u0026quot; with one of the options listed at:\rhttps://kubernetes.io/docs/concepts/cluster-administration/addons/\rYou can now join any number of the control-plane node running the following command on each as root:\rkubeadm join k8s-lb:16443 --token 3k4vr0.x3y2nc3ksfnei4y1 \\\r--discovery-token-ca-cert-hash sha256:a5f761f332bd45a199d0676875e7f58c323226df6fb9b4f0b977b6f63b252791 \\\r--control-plane --certificate-key f25e738324e4f027703f24b55d47d28f692b4edc21c2876171ff87877dc8f2ef\rPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!\rAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use\r\u0026quot;kubeadm init phase upload-certs --upload-certs\u0026quot; to reload certs afterward.\rThen you can join any number of worker nodes by running the following on each as root:\rkubeadm join k8s-lb:16443 --token 3k4vr0.x3y2nc3ksfnei4y1 \\\r--discovery-token-ca-cert-hash sha256:a5f761f332bd45a199d0676875e7f58c323226df6fb9b4f0b977b6f63b252791 配置环境变量\ncat \u0026gt;\u0026gt; /root/.bashrc \u0026lt;\u0026lt;EOF\rexport KUBECONFIG=/etc/kubernetes/admin.conf\rEOF\rsource /root/.bashrc\r查看节点状态\n# kubectl get nodes\rNAME STATUS ROLES AGE VERSION\rk8s-master01 NotReady master 3m1s v1.18.2\r安装网络插件\nwget https://docs.projectcalico.org/v3.8/manifests/calico.yaml\r如果有节点是多网卡，所以需要在资源清单文件中指定内网网卡\nvi calico.yaml\n......\rspec:\rcontainers:\r- env:\r- name: DATASTORE_TYPE\rvalue: kubernetes\r- name: IP_AUTODETECTION_METHOD # DaemonSet中添加该环境变量\rvalue: interface=ens33 # 指定内网网卡\r- name: WAIT_FOR_DATASTORE\rvalue: \u0026quot;true\u0026quot;\r......\rkubectl apply -f calico.yaml # 安装calico网络插件\n当网络插件安装完成后，查看node节点信息如下：\n# kubectl get nodes\rNAME STATUS ROLES AGE VERSION\rk8s-master01 Ready master 10m v1.18.2\r可以看到状态已经从NotReady变为ready了。\n（2）、将master02加入集群\n提前下载镜像\nkubeadm config images pull --config kubeadm.yaml\r加入集群\n kubeadm join k8s-lb:16443 --token 3k4vr0.x3y2nc3ksfnei4y1 \\\r--discovery-token-ca-cert-hash sha256:a5f761f332bd45a199d0676875e7f58c323226df6fb9b4f0b977b6f63b252791 \\\r--control-plane --certificate-key f25e738324e4f027703f24b55d47d28f692b4edc21c2876171ff87877dc8f2ef\r输出如下：\n...\rThis node has joined the cluster and a new control plane instance was created:\r* Certificate signing request was sent to apiserver and approval was received.\r* The Kubelet was informed of the new secure connection details.\r* Control plane (master) label and taint were applied to the new node.\r* The Kubernetes control plane instances scaled up.\r* A new etcd member was added to the local/stacked etcd cluster.\rTo start administering your cluster from this node, you need to run the following as a regular user:\rmkdir -p $HOME/.kube\rsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\rsudo chown $(id -u):$(id -g) $HOME/.kube/config\rRun 'kubectl get nodes' to see this node join the cluster.\r...\r配置环境变量\ncat \u0026gt;\u0026gt; /root/.bashrc \u0026lt;\u0026lt;EOF\rexport KUBECONFIG=/etc/kubernetes/admin.conf\rEOF\rsource /root/.bashrc\r另一台的操作一样。\n查看集群状态\n# kubectl get nodes NAME STATUS ROLES AGE VERSION\rk8s-master01 Ready master 41m v1.18.2\rk8s-master02 Ready master 29m v1.18.2\rk8s-master03 Ready master 27m v1.18.2\r查看集群组件状态\n# kubectl get pod -n kube-system -o wide\rNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES\rcalico-kube-controllers-77c5fc8d7f-stl57 1/1 Running 0 26m 192.168.32.130 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rcalico-node-ppsph 1/1 Running 0 26m 10.1.10.100 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rcalico-node-tl6sq 0/1 Init:2/3 0 26m 10.1.10.101 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rcalico-node-w92qh 1/1 Running 0 26m 10.1.10.102 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rcoredns-546565776c-vtlhr 1/1 Running 0 42m 192.168.32.129 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rcoredns-546565776c-wz9bk 1/1 Running 0 42m 192.168.32.131 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\retcd-k8s-master01 1/1 Running 0 42m 10.1.10.100 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\retcd-k8s-master02 1/1 Running 0 30m 10.1.10.101 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\retcd-k8s-master03 1/1 Running 0 28m 10.1.10.102 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-apiserver-k8s-master01 1/1 Running 0 42m 10.1.10.100 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-apiserver-k8s-master02 1/1 Running 0 30m 10.1.10.101 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-apiserver-k8s-master03 1/1 Running 0 28m 10.1.10.102 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-controller-manager-k8s-master01 1/1 Running 1 42m 10.1.10.100 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-controller-manager-k8s-master02 1/1 Running 1 30m 10.1.10.101 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-controller-manager-k8s-master03 1/1 Running 0 28m 10.1.10.102 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-proxy-6sbpp 1/1 Running 0 28m 10.1.10.102 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-proxy-dpppr 1/1 Running 0 42m 10.1.10.100 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-proxy-ln7l7 1/1 Running 0 30m 10.1.10.101 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-scheduler-k8s-master01 1/1 Running 1 42m 10.1.10.100 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-scheduler-k8s-master02 1/1 Running 1 30m 10.1.10.101 k8s-master02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rkube-scheduler-k8s-master03 1/1 Running 0 28m 10.1.10.102 k8s-master03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\r查看CSR\n kubectl get csr\rNAME AGE SIGNERNAME REQUESTOR CONDITION\rcsr-cfl2w 42m kubernetes.io/kube-apiserver-client-kubelet system:node:k8s-master01 Approved,Issued\rcsr-mm7g7 28m kubernetes.io/kube-apiserver-client-kubelet system:bootstrap:3k4vr0 Approved,Issued\rcsr-qzn6r 30m kubernetes.io/kube-apiserver-client-kubelet system:bootstrap:3k4vr0 Approved,Issued\r部署node node节点只需加入集群即可\nkubeadm join k8s-lb:16443 --token 3k4vr0.x3y2nc3ksfnei4y1 \\\r--discovery-token-ca-cert-hash sha256:a5f761f332bd45a199d0676875e7f58c323226df6fb9b4f0b977b6f63b252791 输出日志如下：\nW0509 23:24:12.159733 10635 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.\r[preflight] Running pre-flight checks\r[WARNING IsDockerSystemdCheck]: detected \u0026quot;cgroupfs\u0026quot; as the Docker cgroup driver. The recommended driver is \u0026quot;systemd\u0026quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/\r[preflight] Reading configuration from the cluster...\r[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'\r[kubelet-start] Downloading configuration for the kubelet from the \u0026quot;kubelet-config-1.18\u0026quot; ConfigMap in the kube-system namespace\r[kubelet-start] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot;\r[kubelet-start] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot;\r[kubelet-start] Starting the kubelet\r[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...\rThis node has joined the cluster:\r* Certificate signing request was sent to apiserver and a response was received.\r* The Kubelet was informed of the new secure connection details.\rRun 'kubectl get nodes' on the control-plane to see this node join the cluster.\r然后查看集群节点信息\n# kubectl get nodes NAME STATUS ROLES AGE VERSION\rk8s-master01 Ready master 47m v1.18.2\rk8s-master02 Ready master 35m v1.18.2\rk8s-master03 Ready master 32m v1.18.2\rk8s-node01 Ready node01 55s v1.18.2\r测试切换 关闭一台master主机，看集群是否可用。\n关闭master01主机，然后查看整个集群。\n# 模拟关掉keepalived\rsystemctl stop keepalived\r# 然后查看集群是否可用\r[root@k8s-master03 ~]# kubectl get nodes\rNAME STATUS ROLES AGE VERSION\rk8s-master01 Ready master 64m v1.18.2\rk8s-master02 Ready master 52m v1.18.2\rk8s-master03 Ready master 50m v1.18.2\rk8s-node01 Ready \u0026lt;none\u0026gt; 18m v1.18.2\r[root@k8s-master03 ~]# kubectl get pod -n kube-system\rNAME READY STATUS RESTARTS AGE\rcalico-kube-controllers-77c5fc8d7f-stl57 1/1 Running 0 49m\rcalico-node-8t5ft 1/1 Running 0 19m\rcalico-node-ppsph 1/1 Running 0 49m\rcalico-node-tl6sq 1/1 Running 0 49m\rcalico-node-w92qh 1/1 Running 0 49m\rcoredns-546565776c-vtlhr 1/1 Running 0 65m\rcoredns-546565776c-wz9bk 1/1 Running 0 65m\retcd-k8s-master01 1/1 Running 0 65m\retcd-k8s-master02 1/1 Running 0 53m\retcd-k8s-master03 1/1 Running 0 51m\rkube-apiserver-k8s-master01 1/1 Running 0 65m\rkube-apiserver-k8s-master02 1/1 Running 0 53m\rkube-apiserver-k8s-master03 1/1 Running 0 51m\rkube-controller-manager-k8s-master01 1/1 Running 2 65m\rkube-controller-manager-k8s-master02 1/1 Running 1 53m\rkube-controller-manager-k8s-master03 1/1 Running 0 51m\rkube-proxy-6sbpp 1/1 Running 0 51m\rkube-proxy-dpppr 1/1 Running 0 65m\rkube-proxy-ln7l7 1/1 Running 0 53m\rkube-proxy-r5ltk 1/1 Running 0 19m\rkube-scheduler-k8s-master01 1/1 Running 2 65m\rkube-scheduler-k8s-master02 1/1 Running 1 53m\rkube-scheduler-k8s-master03 1/1 Running 0 51m\r 到此集群搭建完了，然后可以开启keepalived的检查脚本了。另外一些组件就自己自行安装。\n 安装自动补全命令 yum install -y bash-completion\rsource /usr/share/bash-completion/bash_completion\rsource \u0026lt;(kubectl completion bash)\recho \u0026quot;source \u0026lt;(kubectl completion bash)\u0026quot; \u0026gt;\u0026gt; ~/.bashrc\r","description":"使用kubeadm搭建高可用kubernetes集群","id":1,"section":"posts","tags":["kubernetes"],"title":"Kubeadm搭建高可用Kubernetes集群","uri":"http://qiaokebaba.github.io/posts/kubeadm-install-kubernetes/"},{"content":"备份 备份策略：\n 每两个小时用命令对etcd进行备份 备份数据保留2天  ETCDCTL_API=3 /opt/etcd/bin/etcdctl snapshot save /root/backup/etcd_$(date \u0026quot;+%Y%m%d%H%M%S\u0026quot;).db 恢复 （1）、停止kube-apiserver，确保不会再写入数据\nsystemctl stop kube-apiserver （2）、停止所有节点etcd\nsystemctl stop etcd （3）、将etcd节点上原有的数据目录备份（具体的目录可以在etcd的配置文件中查看）\nmv /var/lib/etcd/default.etcd{,.bak} （4）、将备份的数据拷贝到所有etcd节点\nscp etcd_20200106152240.db 10.1.10.129:/root/backup/ scp etcd_20200106152240.db 10.1.10.130:/root/backup/ （5）、使用命令进行恢复\n# 在etcd-1上 ETCDCTL_API=3 /opt/etcd/bin/etcdctl snapshot --endpoints=\u0026quot;https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379\u0026quot; --cacert=/opt/etcd/ssl/etcd-ca.pem --cert=/opt/etcd/ssl/etcd-server.pem --key=/opt/etcd/ssl/etcd-server-key.pem restore ~/backup/etcd_20200106152240.db --name=etcd-1 --data-dir=/var/lib/etcd/default.etcd --initial-cluster=\u0026quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380,etcd-3=https://10.1.10.130:2380\u0026quot; --initial-cluster-token=\u0026quot;etcd-cluster\u0026quot; --initial-advertise-peer-urls=https://10.1.10.128:2380 # 在etcd-2上 ETCDCTL_API=3 /opt/etcd/bin/etcdctl snapshot --endpoints=\u0026quot;https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379\u0026quot; --cacert=/opt/etcd/ssl/etcd-ca.pem --cert=/opt/etcd/ssl/etcd-server.pem --key=/opt/etcd/ssl/etcd-server-key.pem restore ~/backup/etcd_20200106152240.db --name=etcd-2 --data-dir=/var/lib/etcd/default.etcd --initial-cluster=\u0026quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380,etcd-3=https://10.1.10.130:2380\u0026quot; --initial-cluster-token=\u0026quot;etcd-cluster\u0026quot; --initial-advertise-peer-urls=https://10.1.10.129:2380 # 在etcd-3上 ETCDCTL_API=3 /opt/etcd/bin/etcdctl snapshot --endpoints=\u0026quot;https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379\u0026quot; --cacert=/opt/etcd/ssl/etcd-ca.pem --cert=/opt/etcd/ssl/etcd-server.pem --key=/opt/etcd/ssl/etcd-server-key.pem restore ~/backup/etcd_20200106152240.db --name=etcd-3 --data-dir=/var/lib/etcd/default.etcd --initial-cluster=\u0026quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380,etcd-3=https://10.1.10.130:2380\u0026quot; --initial-cluster-token=\u0026quot;etcd-cluster\u0026quot; --initial-advertise-peer-urls=https://10.1.10.130:2380 （6）、启动etcd服务\nsystemctl start etcd （7）、启动kube-apiserver\nsystemctl start kube-apiserver （8）、查看集群状态\n# /opt/etcd/bin/etcdctl \\ \u0026gt; --ca-file=/opt/etcd/ssl/etcd-ca.pem --cert-file=/opt/etcd/ssl/etcd-server.pem --key-file=/opt/etcd/ssl/etcd-server-key.pem \\ \u0026gt; --endpoints=\u0026quot;https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379\u0026quot; \\ \u0026gt; cluster-health member a2dba8836695bcf6 is healthy: got healthy result from https://10.1.10.129:2379 member d1272b0b3cb41282 is healthy: got healthy result from https://10.1.10.128:2379 member e4a3a9c93ef84f2d is healthy: got healthy result from https://10.1.10.130:2379 cluster is healthy # kubectl get node NAME STATUS ROLES AGE VERSION master-k8s Ready \u0026lt;none\u0026gt; 4h9m v1.16.4 node01-k8s Ready \u0026lt;none\u0026gt; 40h v1.16.4 node02-k8s Ready \u0026lt;none\u0026gt; 39h v1.16.4 # kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE coredns-9d5b6bdb6-mpwht 1/1 Running 0 38h kube-flannel-ds-amd64-2qkcb 1/1 Running 0 38h kube-flannel-ds-amd64-7nzj5 1/1 Running 0 38h kube-flannel-ds-amd64-hlfdf 1/1 Running 0 4h9m metrics-server-v0.3.6-6c57d48cb4-tzjc7 2/2 Running 0 3h53m traefik-ingress-controller-7758594f89-lwf2t 1/1 Running 0 16h 剔除 如果我们要剔除某个节点，我们可以通过下面步骤进行。\n（1）、查看现有节点的member信息\n /opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/etcd-ca.pem --cert-file=/opt/etcd/ssl/etcd-server.pem --key-file=/opt/etcd/ssl/etcd-server-key.pem member list a2dba8836695bcf6: name=etcd-2 peerURLs=https://10.1.10.129:2380 clientURLs=https://10.1.10.129:2379 isLeader=false d1272b0b3cb41282: name=etcd-1 peerURLs=https://10.1.10.128:2380 clientURLs=https://10.1.10.128:2379 isLeader=true e4a3a9c93ef84f2d: name=etcd-3 peerURLs=https://10.1.10.130:2380 clientURLs=https://10.1.10.130:2379 isLeader=false （2）、根据member信息移除相应的实例\n# /opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/etcd-ca.pem --cert-file=/opt/etcd/ssl/etcd-server.pem --key-file=/opt/etcd/ssl/etcd-server-key.pem member remove e4a3a9c93ef84f2d Removed member e4a3a9c93ef84f2d from cluster （3）、停止被移除节点的etcd\nsystemctl stop etcd （4）、修改现有etcd集群的配置文件，移除被踢掉的etcd集群\n... ETCD_INITIAL_CLUSTER=\u0026quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380\u0026quot; ... （5）、重启现有集群的etcd\nsystemctl restart etcd （6）、查看集群状态\n# /opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/etcd-ca.pem --cert-file=/opt/etcd/ssl/etcd-server.pem --key-file=/opt/etcd/ssl/etcd-server-key.pem member list a2dba8836695bcf6: name=etcd-2 peerURLs=https://10.1.10.129:2380 clientURLs=https://10.1.10.129:2379 isLeader=false d1272b0b3cb41282: name=etcd-1 peerURLs=https://10.1.10.128:2380 clientURLs=https://10.1.10.128:2379 isLeader=true # /opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/etcd-ca.pem --cert-file=/opt/etcd/ssl/etcd-server.pem --key-file=/opt/etcd/ssl/etcd-server-key.pem --endpoints=\u0026quot;https://10.1.10.128:2379,https://10.1.10.129:2379\u0026quot; cluster-health member a2dba8836695bcf6 is healthy: got healthy result from https://10.1.10.129:2379 member d1272b0b3cb41282 is healthy: got healthy result from https://10.1.10.128:2379 cluster is healthy 新增 如果我们需要新增一个etcd节点，则可以按照以下步骤进行。\n（1）、通过以下命令新增节点\n# /opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/etcd-ca.pem --cert-file=/opt/etcd/ssl/etcd-server.pem --key-file=/opt/etcd/ssl/etcd-server-key.pem member add etcd-3 https://10.1.10.130:2380 Added member named etcd-3 with ID 16ebaad9c9a3a3e3 to cluster ETCD_NAME=\u0026quot;etcd-3\u0026quot; ETCD_INITIAL_CLUSTER=\u0026quot;etcd-3=https://10.1.10.130:2380,etcd-2=https://10.1.10.129:2380,etcd-1=https://10.1.10.128:2380\u0026quot; ETCD_INITIAL_CLUSTER_STATE=\u0026quot;existing\u0026quot;  注意：\n etcd_name: etcd.conf配置文件中ETCD_NAME内容 etdc_node_address: etcd.conf配置文件中的ETCD_LISTEN_PEER_URLS内容   （2）、删除新增成员旧数据目录，并且启动新增成员etcd服务，加入集群时要改下配置文件，把初始化集群状态由new改成existing，如下\n#[Member] ETCD_NAME=\u0026quot;etcd-3\u0026quot; ETCD_DATA_DIR=\u0026quot;/var/lib/etcd/default.etcd\u0026quot; ETCD_LISTEN_PEER_URLS=\u0026quot;https://10.1.10.130:2380\u0026quot; ETCD_LISTEN_CLIENT_URLS=\u0026quot;https://10.1.10.130:2379\u0026quot; #[Clustering] ETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026quot;https://10.1.10.130:2380\u0026quot; ETCD_ADVERTISE_CLIENT_URLS=\u0026quot;https://10.1.10.130:2379\u0026quot; ETCD_INITIAL_CLUSTER=\u0026quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380,etcd-3=https://10.1.10.130:2380\u0026quot; ETCD_INITIAL_CLUSTER_TOKEN=\u0026quot;etcd-cluster\u0026quot; ETCD_INITIAL_CLUSTER_STATE=\u0026quot;existing\u0026quot; （3）、还需要修改systemd unit文件中的参数，如下：\n...... --initial-cluster-state=existing \\ ...... （4）、在现存etcd的配置文件中修改\nETCD_INITIAL_CLUSTER=\u0026quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380,etcd-3=https://10.1.10.130:2380\u0026quot; （5）、重启etcd\nsystemctl restart etcd （6）、查看状态\n1 2 3 4 5 6 7 8 9 10 11 12  # /opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/etcd-ca.pem --cert-file=/opt/etcd/ssl/etcd-server.pem --key-file=/opt/etcd/ssl/etcd-server-key.pem member list 7d38a6cc82b63e33: name=etcd-3 peerURLs=https://10.1.10.130:2380 clientURLs=https://10.1.10.130:2379 isLeader=false a2dba8836695bcf6: name=etcd-2 peerURLs=https://10.1.10.129:2380 clientURLs=https://10.1.10.129:2379 isLeader=true d1272b0b3cb41282: name=etcd-1 peerURLs=https://10.1.10.128:2380 clientURLs=https://10.1.10.128:2379 isLeader=false # /opt/etcd/bin/etcdctl \\ \u0026gt; --ca-file=/opt/etcd/ssl/etcd-ca.pem --cert-file=/opt/etcd/ssl/etcd-server.pem --key-file=/opt/etcd/ssl/etcd-server-key.pem \\ \u0026gt; --endpoints=\u0026#34;https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379\u0026#34; \\ \u0026gt; cluster-health member 7d38a6cc82b63e33 is healthy: got healthy result from https://10.1.10.130:2379 member a2dba8836695bcf6 is healthy: got healthy result from https://10.1.10.129:2379 member d1272b0b3cb41282 is healthy: got healthy result from https://10.1.10.128:2379   ","description":"etcd集群的备份、删除、新增等操作","id":2,"section":"posts","tags":["etcd"],"title":"Etcd集群常用操作","uri":"http://qiaokebaba.github.io/posts/handle-etcd-cluster/"},{"content":"基础规划 1、IP规划\n   主机名 IP 配置 软件     master-k8s 10.1.10.128 2C4G etcd,apiserver,controller-manager,scheduler   node01-k8s 10.1.10.129 2C4G etcd,docker,kubelet,kube-proxy   node02-k8s 10.1.10.130 2C4G etcd,docker,kubelet,kube-proxy    2、软件规划\n   软件名 版本     etcd 3.3.18   docker-ce 19.03.5-3   cfssl 1.2.0   kubernetes 1.16.4   flannel 0.11.0   cni 0.8.3    3、目录规划\n   目录名 用途     /var/log/kubernetes/ 存储日志   /root/kubernetes/install 安装软件目录   /opt/kubernetes K8S项目部署目录，其中ssl是证书目录，bin是二进制目录，config是配置文件目录   /opt/etcd Etcd项目部署目录，子目录功能如上   /opt/cni cni二进制文件保存目录   /opt/kubernetes/ssl 证书生成目录   /opt/kubernetes/kubeconfig kubeconfig统一生成目录   /opt/kubernetes/system 系统组件YAML文件存储目录    mkdir /var/log/kubernetes /root/kubernetes/{ssl,install,kubeconfig} /root/kubernetes/ssl /opt/etcd/{bin,config,ssl} /opt/kubernetes/{bin,config,ssl} /opt/cni/bin -p 主机初始化配置 2、设置hostname\n# 10.1.10.128 hostnamectl set-hostname master-k8s # 10.1.10.129 hostnamectl set-hostname node01-k8s # 10.1.10.130 hostnamectl set-hostname node02-k8s 3、配置Hosts（/etc/hosts）\ncat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt;EOF 10.1.10.128 master-k8s 10.1.10.129 node01-k8s 10.1.10.130 node02-k8s EOF 4、初始化\n关闭防火墙\nsystemctl stop firewalld systemctl disable firewalld 关闭SELINUX\nsetenforce 0 sed -i \u0026quot;s/SELINUX=enforcing/SELINUX=disabled/g\u0026quot; /etc/sysconfig/selinux 刷新yum缓存\nyum clean all yum makecache 修改内核参数\ncat \u0026gt; /etc/sysctl.d/k8s.conf \u0026lt;\u0026lt;EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 vm.swappiness=0 EOF modprobe br_netfilter sysctl -p /etc/sysctl.d/k8s.conf 安装IPVS\ncat \u0026gt; /etc/sysconfig/modules/ipvs.modules \u0026lt;\u0026lt;EOF #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOF chmod 755 /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; bash /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4 yum install ipset ipvsadm -y 同步服务器时间\n master\n #安装chrony： yum -y install chrony #注释默认ntp服务器 sed -i 's/^server/#\u0026amp;/' /etc/chrony.conf #指定上游公共 ntp 服务器，并允许其他节点同步时间 cat \u0026gt;\u0026gt; /etc/chrony.conf \u0026lt;\u0026lt; EOF server 0.asia.pool.ntp.org iburst server 1.asia.pool.ntp.org iburst server 2.asia.pool.ntp.org iburst server 3.asia.pool.ntp.org iburst allow all EOF #重启chronyd服务并设为开机启动： systemctl enable chronyd \u0026amp;\u0026amp; systemctl restart chronyd #开启网络时间同步功能 timedatectl set-ntp true  slave\n #安装chrony： yum -y install chrony #注释默认服务器 sed -i 's/^server/#\u0026amp;/' /etc/chrony.conf #指定内网 master节点为上游NTP服务器 echo 'server 10.1.10.128 iburst' \u0026gt;\u0026gt; /etc/chrony.conf #重启服务并设为开机启动： systemctl enable chronyd \u0026amp;\u0026amp; systemctl restart chronyd 关闭SWAP分区\nswapoff -a sed -i \u0026quot;s/\\/dev\\/mapper\\/centos-swap/#\\/dev\\/mapper\\/centos-swap/g\u0026quot; /etc/fstab 安装docker yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum makecache fast yum install docker-ce -y systemctl start docker systemctl enable docker 配置镜像加速（）\ncurl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io systemctl restart docker 安装其他软件：\nyum install unzip wget lrzsz -y 优化：\nvi daemon.json { \u0026quot;max-concurrent-downloads\u0026quot;: 20, \u0026quot;log-driver\u0026quot;: \u0026quot;json-file\u0026quot;, \u0026quot;bridge\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;oom-score-adjust\u0026quot;: -1000, \u0026quot;debug\u0026quot;: false, \u0026quot;log-opts\u0026quot;: { \u0026quot;max-size\u0026quot;: \u0026quot;100M\u0026quot;, \u0026quot;max-file\u0026quot;: \u0026quot;10\u0026quot; }, \u0026quot;default-ulimits\u0026quot;: { \u0026quot;nofile\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;nofile\u0026quot;, \u0026quot;Hard\u0026quot;: 65535, \u0026quot;Soft\u0026quot;: 65535 }, \u0026quot;nproc\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;nproc\u0026quot;, \u0026quot;Hard\u0026quot;: 65535, \u0026quot;Soft\u0026quot;: 65535 }, \u0026quot;core\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;core\u0026quot;, \u0026quot;Hard\u0026quot;: -1, \u0026quot;Soft\u0026quot;: -1 } } } 安装cfssl证书生成工具 curl -L https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o /usr/local/bin/cfssl curl -L https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o /usr/local/bin/cfssljson curl -L https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -o /usr/local/bin/cfssl-certinfo chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson /usr/local/bin/cfssl-certinfo 搭建ETCD集群 下载地址：https://github.com/etcd-io/etcd/releases/download/v3.3.18/etcd-v3.3.18-linux-amd64.tar.gz\nwget https://github.com/etcd-io/etcd/releases/download/v3.3.18/etcd-v3.3.18-linux-amd64.tar.gz 生成ETCD证书 证书生成的目录统一下/root/kubernetes/ssl/下\nmkdir /root/kubernetes/ssl/etcd -p \u0026amp;\u0026amp; cd /root/kubernetes/ssl/etcd （1）、创建CA的请求文件（etcd-ca-csr.json）\ncat \u0026gt; etcd-ca-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;etcd\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot; } ] } EOF （2）、创建CA的配置文件（etcd-ca-config.json）\ncat \u0026gt; etcd-ca-config.json \u0026lt;\u0026lt;EOF { \u0026quot;signing\u0026quot;: { \u0026quot;default\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;87600h\u0026quot; }, \u0026quot;profiles\u0026quot;: { \u0026quot;www\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;87600h\u0026quot;, \u0026quot;usages\u0026quot;: [ \u0026quot;signing\u0026quot;, \u0026quot;key encipherment\u0026quot;, \u0026quot;server auth\u0026quot;, \u0026quot;client auth\u0026quot; ] } } } } EOF （3）、创建CA证书\ncfssl gencert -initca etcd-ca-csr.json | cfssljson -bare etcd-ca - （4）、创建etcd证书请求文件（etcd-server-csr.json）：\ncat \u0026gt; etcd-server-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;etcd\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;10.1.10.128\u0026quot;, \u0026quot;10.1.10.129\u0026quot;, \u0026quot;10.1.10.130\u0026quot; ], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot; } ] } EOF （5）、生成etcd证书并用 CA签名\ncfssl gencert -ca=etcd-ca.pem -ca-key=etcd-ca-key.pem -config=etcd-ca-config.json -profile=www etcd-server-csr.json | cfssljson -bare etcd-server # ls *.pem ca-key.pem ca.pem etcd-key.pem etcd.pem # cp *.pem /opt/etcd/ssl/ 安装ETCD 解压安装包：\ntar xf etcd-v3.3.18-linux-amd64.tar.gz cp etcd etcdctl /opt/etcd/bin/ 创建配置文件（etcd.conf）\ncat \u0026gt; /opt/etcd/config/etcd.conf \u0026lt;\u0026lt;EOF #[Member] ETCD_NAME=\u0026quot;etcd-1\u0026quot; ETCD_DATA_DIR=\u0026quot;/var/lib/etcd/default.etcd\u0026quot; ETCD_LISTEN_PEER_URLS=\u0026quot;https://10.1.10.128:2380\u0026quot; ETCD_LISTEN_CLIENT_URLS=\u0026quot;https://10.1.10.128:2379\u0026quot; #[Clustering] ETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026quot;https://10.1.10.128:2380\u0026quot; ETCD_ADVERTISE_CLIENT_URLS=\u0026quot;https://10.1.10.128:2379\u0026quot; ETCD_INITIAL_CLUSTER=\u0026quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380,etcd-3=https://10.1.10.130:2380\u0026quot; ETCD_INITIAL_CLUSTER_TOKEN=\u0026quot;etcd-cluster\u0026quot; ETCD_INITIAL_CLUSTER_STATE=\u0026quot;new\u0026quot; EOF  注意：相应的地址按需更改\nETCD_NAME：三台不能相同\nip地址不能相同\n 创建etcd的启动文件etcd.service\ncat \u0026gt; /usr/lib/systemd/system/etcd.service \u0026lt;\u0026lt;EOF [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify EnvironmentFile=/opt/etcd/config/etcd.conf ExecStart=/opt/etcd/bin/etcd \\\\ --name=\\${ETCD_NAME} \\\\ --data-dir=\\${ETCD_DATA_DIR} \\\\ --listen-peer-urls=\\${ETCD_LISTEN_PEER_URLS} \\\\ --listen-client-urls=\\${ETCD_LISTEN_CLIENT_URLS},http://127.0.0.1:2379 \\\\ --advertise-client-urls=\\${ETCD_ADVERTISE_CLIENT_URLS} \\\\ --initial-advertise-peer-urls=\\${ETCD_INITIAL_ADVERTISE_PEER_URLS} \\\\ --initial-cluster=\\${ETCD_INITIAL_CLUSTER} \\\\ --initial-cluster-token=\\${ETCD_INITIAL_CLUSTER_TOKEN} \\\\ --initial-cluster-state=new \\\\ --cert-file=/opt/etcd/ssl/etcd-server.pem \\\\ --key-file=/opt/etcd/ssl/etcd-server-key.pem \\\\ --peer-cert-file=/opt/etcd/ssl/etcd-server.pem \\\\ --peer-key-file=/opt/etcd/ssl/etcd-server-key.pem \\\\ --trusted-ca-file=/opt/etcd/ssl/etcd-ca.pem \\\\ --peer-trusted-ca-file=/opt/etcd/ssl/etcd-ca.pem Restart=on-failure LimitNOFILE=65536 [Install] WantedBy=multi-user.target EOF 另外两天部署一样，只有配置文件需要更改一下，将文件拷贝到另外两台：\nscp -r /opt/etcd 10.1.10.129:/opt/ scp -r /opt/etcd 10.1.10.130:/opt/ scp /usr/lib/systemd/system/etcd.service 10.1.10.129:/usr/lib/systemd/system/ scp /usr/lib/systemd/system/etcd.service 10.1.10.130:/usr/lib/systemd/system/ 然后分别修改配置文件：\n10.1.10.129\n#[Member] ETCD_NAME=\u0026quot;etcd-2\u0026quot; ETCD_DATA_DIR=\u0026quot;/var/lib/etcd/default.etcd\u0026quot; ETCD_LISTEN_PEER_URLS=\u0026quot;https://10.1.10.129:2380\u0026quot; ETCD_LISTEN_CLIENT_URLS=\u0026quot;https://10.1.10.129:2379\u0026quot; #[Clustering] ETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026quot;https://10.1.10.129:2380\u0026quot; ETCD_ADVERTISE_CLIENT_URLS=\u0026quot;https://10.1.10.129:2379\u0026quot; ETCD_INITIAL_CLUSTER=\u0026quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380,etcd-3=https://10.1.10.130:2380\u0026quot; ETCD_INITIAL_CLUSTER_TOKEN=\u0026quot;etcd-cluster\u0026quot; ETCD_INITIAL_CLUSTER_STATE=\u0026quot;new\u0026quot; 10.1.10.130\n#[Member] ETCD_NAME=\u0026quot;etcd-3\u0026quot; ETCD_DATA_DIR=\u0026quot;/var/lib/etcd/default.etcd\u0026quot; ETCD_LISTEN_PEER_URLS=\u0026quot;https://10.1.10.130:2380\u0026quot; ETCD_LISTEN_CLIENT_URLS=\u0026quot;https://10.1.10.130:2379\u0026quot; #[Clustering] ETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026quot;https://10.1.10.130:2380\u0026quot; ETCD_ADVERTISE_CLIENT_URLS=\u0026quot;https://10.1.10.130:2379\u0026quot; ETCD_INITIAL_CLUSTER=\u0026quot;etcd-1=https://10.1.10.128:2380,etcd-2=https://10.1.10.129:2380,etcd-3=https://10.1.10.130:2380\u0026quot; ETCD_INITIAL_CLUSTER_TOKEN=\u0026quot;etcd-cluster\u0026quot; ETCD_INITIAL_CLUSTER_STATE=\u0026quot;new\u0026quot; 然后启动三台的etcd服务\nsystemctl daemon-reload \u0026amp;\u0026amp; systemctl start etcd \u0026amp;\u0026amp; systemctl enable etcd 查看集群状态：\n/opt/etcd/bin/etcdctl \\ --ca-file=/opt/etcd/ssl/etcd-ca.pem --cert-file=/opt/etcd/ssl/etcd-server.pem --key-file=/opt/etcd/ssl/etcd-server-key.pem \\ --endpoints=\u0026quot;https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379\u0026quot; \\ cluster-health member a2dba8836695bcf6 is healthy: got healthy result from https://10.1.10.129:2379 member d1272b0b3cb41282 is healthy: got healthy result from https://10.1.10.128:2379 member e4a3a9c93ef84f2d is healthy: got healthy result from https://10.1.10.130:2379 cluster is healthy 安装Flannel  我是在所有节点都部署了，你也可以只部署Node。\n 下载地址：https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz\nFalnnel要用etcd存储自身一个子网信息，所以要保证能成功连接Etcd，写入预定义子网段：\n/opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/ca.pem --cert-file=/opt/etcd/ssl/etcd.pem --key-file=/opt/etcd/ssl/etcd-key.pem --endpoints=\u0026quot;https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379\u0026quot; set /coreos.com/network/config '{ \u0026quot;Network\u0026quot;: \u0026quot;172.17.0.0/16\u0026quot;, \u0026quot;Backend\u0026quot;: {\u0026quot;Type\u0026quot;: \u0026quot;vxlan\u0026quot;}}' 然后可以查看一下：\n# /opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/ca.pem --cert-file=/opt/etcd/ssl/etcd.pem --key-file=/opt/etcd/ssl/etcd-key.pem --endpoints=\u0026quot;https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379\u0026quot; get /coreos.com/network/config { \u0026quot;Network\u0026quot;: \u0026quot;172.17.0.0/16\u0026quot;, \u0026quot;Backend\u0026quot;: {\u0026quot;Type\u0026quot;: \u0026quot;vxlan\u0026quot;}} 解压压缩包\ntar xf flannel-v0.11.0-linux-amd64.tar.gz 将两个重要的二进制文件flanneld和mk-docker-opts.sh拷贝到/opt/kubernetes/bin下\ncp flanneld mk-docker-opts.sh /opt/kubernetes/bin/ 配置Flannel的配置文件：\ncat \u0026gt; /opt/kubernetes/config/flanneld.conf \u0026lt;\u0026lt;EOF FLANNEL_OPTIONS=\u0026quot;\\ --etcd-endpoints=https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379 \\ -etcd-cafile=/opt/etcd/ssl/ca.pem \\ -etcd-certfile=/opt/etcd/ssl/etcd.pem \\ -etcd-keyfile=/opt/etcd/ssl/etcd-key.pem\u0026quot; EOF 配置系统systemd启动文件\ncat \u0026gt; flanneld.service \u0026lt;\u0026lt;EOF [Unit] Description=Flanneld overlay address etcd agent After=network-online.target network.target Before=docker.service [Service] Type=notify EnvironmentFile=/opt/kubernetes/config/flanneld.conf ExecStart=/opt/kubernetes/bin/flanneld --ip-masq $FLANNEL_OPTIONS ExecStartPost=/opt/kubernetes/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/subnet.env Restart=on-failure [Install] WantedBy=multi-user.target EOF 配置Docker的系统文件，指定子网（/usr/lib/systemd/system/docker.service）\n[Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com BindsTo=containerd.service After=network-online.target firewalld.service containerd.service Wants=network-online.target Requires=docker.socket [Service] Type=notify EnvironmentFile=/run/flannel/subnet.env ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS ExecReload=/bin/kill -s HUP $MAINPID TimeoutSec=0 RestartSec=2 Restart=always StartLimitBurst=3 StartLimitInterval=60s LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity TasksMax=infinity Delegate=yes KillMode=process [Install] WantedBy=multi-user.target 将配置文件拷贝到另外主机\ncp /opt/kubernetes/flanneld.service /usr/lib/systemd/system/ scp -r /opt/kubernetes/ 10.1.10.129:/opt/ scp -r /opt/kubernetes/ 10.1.10.130:/opt/ scp /usr/lib/systemd/system/{docker,flanneld}.service 10.1.10.129:/usr/lib/systemd/system/ scp /usr/lib/systemd/system/{docker,flanneld}.service 10.1.10.130:/usr/lib/systemd/system/ 启动flannel和重启docker\nsystemctl daemon-reload \u0026amp;\u0026amp; systemctl enable flanneld \u0026amp;\u0026amp; systemctl start flanneld systemctl restart docker 检查docker是否使用了flannel网络：\n# ps -ef | grep docker root 10201 1 0 11:08 ? 00:00:00 /usr/bin/dockerd --bip=172.17.69.1/24 --ip-masq=false --mtu=1450 起一个容器测试网络连通性是否正确\n# docker run -it --name node02 --rm busybox /bin/sh / # ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 7: eth0@if8: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1450 qdisc noqueue link/ether 02:42:ac:11:50:02 brd ff:ff:ff:ff:ff:ff inet 172.17.80.2/24 brd 172.17.80.255 scope global eth0 valid_lft forever preferred_lft forever / # ping 10.1.10.128 -c 1 PING 10.1.10.128 (10.1.10.128): 56 data bytes 64 bytes from 10.1.10.128: seq=0 ttl=63 time=0.802 ms --- 10.1.10.128 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.802/0.802/0.802 ms / # ping 10.1.10.129 -c 1 PING 10.1.10.129 (10.1.10.129): 56 data bytes 64 bytes from 10.1.10.129: seq=0 ttl=63 time=0.515 ms --- 10.1.10.129 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.515/0.515/0.515 ms / # ping 10.1.10.130 -c 1 PING 10.1.10.130 (10.1.10.130): 56 data bytes 64 bytes from 10.1.10.130: seq=0 ttl=64 time=0.075 ms --- 10.1.10.130 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.075/0.075/0.075 ms / # ping 172.17.7.2 -c 1 PING 172.17.7.2 (172.17.7.2): 56 data bytes 64 bytes from 172.17.7.2: seq=0 ttl=62 time=0.884 ms --- 172.17.7.2 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.884/0.884/0.884 ms 安装mater组件 下载地址： https://dl.k8s.io/v1.16.4/kubernetes-server-linux-amd64.tar.gz\nmkdir /root/kubernetes/ssl/kubernetes -p （1）、解压安装压缩文件\ntar xf kubernetes-server-linux-amd64.tar.gz （2）、将我们需要的二进制文件拷贝到我们部署目录中\ncp kubernetes/server/bin/{kube-apiserver,kubectlkube-scheduler,kube-controller-manager} /opt/kubernetes/bin/ scp kubernetes/server/bin/{kubelet,kube-proxy} 10.1.10.129:/opt/kubernetes/bin/ scp kubernetes/server/bin/{kubelet,kube-proxy} 10.1.10.130:/opt/kubernetes/bin/ （3）、将其加入环境变量\necho \u0026quot;PATH=/opt/kubernetes/bin/:$PATH\u0026quot; \u0026gt;\u0026gt; /etc/profile source /etc/profile （4）、将我们所需的证书和密钥拷贝到部署目录中\n 由于我们master也准备当Node使用，所以我们将所有证书都拷贝到部署证书目录\n cp /root/kubernetes/ssl/kubernetes/*.pem /opt/kubernetes/ssl/ 生成证书 创建CA证书 （1）、新建CA配置文件（ca-csr.json）\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/ca-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF CN CommonName,kube-apiserver从证书中提取该字段作为请求的用户名(User Name)，浏览器使用该字段验证网站是否合法 O Organization,kube-apiserver 从证书中提取该字段作为请求用户和所属组(Group) kube-apiserver将提取的User、Group作为RBAC授权的用户和标识 （2）、新建CA配置文件（ca-config.json）\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/ca-config.json \u0026lt;\u0026lt;EOF { \u0026quot;signing\u0026quot;: { \u0026quot;default\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;87600h\u0026quot; }, \u0026quot;profiles\u0026quot;: { \u0026quot;kubernetes\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;87600h\u0026quot;, \u0026quot;usages\u0026quot;: [ \u0026quot;signing\u0026quot;, \u0026quot;key encipherment\u0026quot;, \u0026quot;server auth\u0026quot;, \u0026quot;client auth\u0026quot; ] } } } } EOF signing 表示该证书可用于签名其它证书，生成的ca.pem证书找中CA=TRUE server auth 表示client可以用该证书对server提供的证书进行验证 client auth 表示server可以用该证书对client提供的证书进行验证 （3）、生成CA证书\n# cfssl gencert -initca ca-csr.json | cfssljson -bare ca - 创建apiserver证书 （1）、新建apiserver证书文件\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/apiserver-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;10.254.0.1\u0026quot;, \u0026quot;127.0.0.1\u0026quot;, \u0026quot;10.1.10.128\u0026quot;, \u0026quot;10.1.10.129\u0026quot;, \u0026quot;10.1.10.130\u0026quot;, \u0026quot;kubernetes\u0026quot;, \u0026quot;kubernetes.default\u0026quot;, \u0026quot;kubernetes.default.svc\u0026quot;, \u0026quot;kubernetes.default.svc.cluster\u0026quot;, \u0026quot;kubernetes.default.svc.cluster.local\u0026quot; ], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF （2）、生成证书\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes apiserver-csr.json | cfssljson -bare apiserver 创建 Kubernetes webhook 证书配置文件 （1）、创建证书文件\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/aggregator-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;aggregator\u0026quot;, \u0026quot;hosts\u0026quot;: [\u0026quot;\u0026quot;], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF （2）、生成证书文件\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes aggregator-csr.json | cfssljson -bare aggregator 创建 Kubernetes admin 证书配置文件 （1）、创建证书文件\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/admin-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;hosts\u0026quot;: [\u0026quot;\u0026quot;], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;system:masters\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF （2）、生成证书和私钥\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin 创建kube-scheduler 证书配置文件 （1）、创建证书文件\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/kube-scheduler-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;system:kube-scheduler\u0026quot;, \u0026quot;hosts\u0026quot;: [\u0026quot;\u0026quot;], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;system:kube-scheduler\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF （2）、生成证书文件和私钥\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler 生成kube-controller-manager证书配置文件 （1）、创建证书文件\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/kube-controller-manager-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;system:kube-controller-manager\u0026quot;, \u0026quot;hosts\u0026quot;: [\u0026quot;\u0026quot;], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;system:kube-controller-manager\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF （2）、生成证书和私钥\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager 创建flannel 证书配置文件 （1）、创建证书文件\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/flannel-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;flannel\u0026quot;, \u0026quot;hosts\u0026quot;: [\u0026quot;\u0026quot;], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;system:masters\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF （2）、生成证书和私钥\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes flannel-csr.json | cfssljson -bare flannel 创建kube-proxy证书 （1）、创建证书文件\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/kube-proxy-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;system:kube-proxy\u0026quot;, \u0026quot;hosts\u0026quot;: [], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;system:masters\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF （2）、生成证书文件\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy 创建 kubernetes-dashboard证书配置文件 （1）、创建证书文件\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/dashboard-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;dashboard\u0026quot;, \u0026quot;hosts\u0026quot;: [\u0026quot;\u0026quot;], \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF （2）、生成证书文件和私钥\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes dashboard-csr.json | cfssljson -bare dashboard 创建metrics-server 证书配置文件 （1）、创建证书文件\ncat \u0026gt; /root/kubernetes/ssl/kubernetes/metrics-server-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;metrics-server\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Chongqing\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;System\u0026quot; } ] } EOF （2）、生成证书和私钥\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes metrics-server-csr.json | cfssljson -bare metrics-server 创建kubeconfig配置文件  在/root/kubernetes/kubeconfig目录下创建这些文件\n （1）、设置kube-apiserver环境变量\nexport KUBE_APISERVER=\u0026quot;https://10.1.10.128:6443\u0026quot; 创建admin kubeconfig # 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=../ssl/kubernetes/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=admin.kubeconfig # 设置客户端认证参数 kubectl config set-credentials admin \\ --client-certificate=../ssl/kubernetes/admin.pem \\ --client-key=../ssl/kubernetes/admin-key.pem \\ --embed-certs=true \\ --kubeconfig=admin.kubeconfig # 设置上下文参数 kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin \\ --namespace=kube-system \\ --kubeconfig=admin.kubeconfig # 设置默认上下文 kubectl config use-context kubernetes --kubeconfig=admin.kubeconfig 创建kube-scheduler kubeconfig # 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=../ssl/kubernetes/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=kube-scheduler.kubeconfig # 设置客户端认证参数 kubectl config set-credentials system:kube-scheduler \\ --client-certificate=../ssl/kubernetes/kube-scheduler.pem \\ --embed-certs=true \\ --client-key=../ssl/kubernetes/kube-scheduler-key.pem \\ --kubeconfig=kube-scheduler.kubeconfig # 设置上下文参数 kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=kube-scheduler.kubeconfig # 设置默认上下文 kubectl config use-context kubernetes --kubeconfig=kube-scheduler.kubeconfig 创建kube-controller-manager kubeconfig # 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=../ssl/kubernetes/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=kube-controller-manager.kubeconfig # 设置客户端认证参数 kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=../ssl/kubernetes/kube-controller-manager.pem \\ --embed-certs=true \\ --client-key=../ssl/kubernetes/kube-controller-manager-key.pem \\ --kubeconfig=kube-controller-manager.kubeconfig # 设置上下文参数 kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=kube-controller-manager.kubeconfig # 设置默认上下文 kubectl config use-context kubernetes --kubeconfig=kube-controller-manager.kubeconfig 创建bootstrap kubeconfig # 生成TOKEN export TOKEN_ID=$(head -c 6 /dev/urandom | md5sum | head -c 6) export TOKEN_SECRET=$(head -c 16 /dev/urandom | md5sum | head -c 16) export BOOTSTRAP_TOKEN=${TOKEN_ID}.${TOKEN_SECRET} # 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=../ssl/kubernetes/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=bootstrap.kubeconfig # 设置客户端认证参数 kubectl config set-credentials system:bootstrap:${TOKEN_ID} \\ --token=${BOOTSTRAP_TOKEN} \\ --kubeconfig=bootstrap.kubeconfig # 设置上下文参数 kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:bootstrap:${TOKEN_ID} \\ --kubeconfig=bootstrap.kubeconfig # 设置默认上下文 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig  BOOTSTRAP_TOKEN=0a22e7.4b91472175b8aaab\n 创建flannel kubeconfig # 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=../ssl/kubernetes/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=kubeconfig.conf # 设置客户端认证参数 kubectl config set-credentials flannel \\ --client-certificate=../ssl/kubernetes/flannel.pem \\ --client-key=../ssl/kubernetes/flannel-key.pem \\ --embed-certs=true \\ --kubeconfig=kubeconfig.conf # 设置上下文参数 kubectl config set-context default \\ --cluster=kubernetes \\ --user=flannel \\ --kubeconfig=kubeconfig.conf # 设置默认上下文 kubectl config use-context default --kubeconfig=kubeconfig.conf 创建kube-proxy kubeconfig # 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=../ssl/kubernetes/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=kube-proxy.kubeconfig # 设置客户端认证参数 kubectl config set-credentials system:kube-proxy \\ --client-certificate=../ssl/kubernetes/kube-proxy.pem \\ --client-key=../ssl/kubernetes/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 设置上下文参数 kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 设置默认上下文 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 创建组件配置文件 创建kube-apiserver配置文件 （1）、创建主配置文件\ncat \u0026gt; /opt/kubernetes/config/kube-apiserver.conf \u0026lt;\u0026lt;EOF KUBE_APISERVER_OPTS=\u0026quot;--logtostderr=false \\\\ --bind-address=10.1.10.128 \\\\ --advertise-address=10.1.10.128 \\\\ --secure-port=6443 \\\\ --insecure-port=0 \\\\ --service-cluster-ip-range=10.254.0.0/16 \\\\ --service-node-port-range=20000-40000 \\\\ --etcd-cafile=/opt/etcd/ssl/etcd-ca.pem \\\\ --etcd-certfile=/opt/etcd/ssl/etcd-server.pem \\\\ --etcd-keyfile=/opt/etcd/ssl/etcd-server-key.pem \\\\ --etcd-prefix=/registry \\\\ --etcd-servers=https://10.1.10.128:2379,https://10.1.10.129:2379,https://10.1.10.130:2379 \\\\ --client-ca-file=/opt/kubernetes/ssl/ca.pem \\\\ --tls-cert-file=/opt/kubernetes/ssl/apiserver.pem\\\\ --tls-private-key-file=/opt/kubernetes/ssl/apiserver-key.pem \\\\ --kubelet-client-certificate=/opt/kubernetes/ssl/apiserver.pem \\\\ --kubelet-client-key=/opt/kubernetes/ssl/apiserver-key.pem \\\\ --service-account-key-file=/opt/kubernetes/ssl/ca.pem \\\\ --requestheader-client-ca-file=/opt/kubernetes/ssl/ca.pem \\\\ --proxy-client-cert-file=/opt/kubernetes/ssl/aggregator.pem \\\\ --proxy-client-key-file=/opt/kubernetes/ssl/aggregator-key.pem \\\\ --requestheader-allowed-names=aggregator \\\\ --requestheader-group-headers=X-Remote-Group \\\\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\\\ --requestheader-username-headers=X-Remote-User \\\\ --enable-aggregator-routing=true \\\\ --anonymous-auth=false \\\\ --allow-privileged=true \\\\ --experimental-encryption-provider-config=/opt/kubernetes/config/encryption-config.yaml \\\\ --enable-admission-plugins=DefaultStorageClass,DefaultTolerationSeconds,LimitRanger,NamespaceExists,NamespaceLifecycle,NodeRestriction,OwnerReferencesPermissionEnforcement,PodNodeSelector,PersistentVolumeClaimResize,PodPreset,PodTolerationRestriction,ResourceQuota,ServiceAccount,StorageObjectInUseProtection MutatingAdmissionWebhook ValidatingAdmissionWebhook \\\\ --disable-admission-plugins=DenyEscalatingExec,ExtendedResourceToleration,ImagePolicyWebhook,LimitPodHardAntiAffinityTopology,NamespaceAutoProvision,Priority,EventRateLimit,PodSecurityPolicy \\\\ --cors-allowed-origins=.* \\\\ --enable-swagger-ui \\\\ --runtime-config=api/all=true \\\\ --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\\\ --authorization-mode=Node,RBAC \\\\ --apiserver-count=1 \\\\ --audit-log-maxage=30 \\\\ --audit-log-maxbackup=3 \\\\ --audit-log-maxsize=100 \\\\ --kubelet-https \\\\ --event-ttl=1h \\\\ --feature-gates=RotateKubeletServerCertificate=true,RotateKubeletClientCertificate=true \\\\ --enable-bootstrap-token-auth=true \\\\ --audit-log-path=/var/log/kubernetes/api-server-audit.log \\\\ --alsologtostderr=true \\\\ --log-dir=/var/log/kubernetes \\\\ --v=2 \\\\ --endpoint-reconciler-type=lease \\\\ --max-mutating-requests-inflight=100 \\\\ --max-requests-inflight=500 \\\\ --target-ram-mb=6000\u0026quot; EOF （2）、创建encryption-config.yaml\nexport ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64) cat \u0026gt; /opt/kubernetes/config/encryption-config.yaml \u0026lt;\u0026lt;EOF kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {} EOF 创建kube-controller-manager配置文件 cat \u0026gt; /opt/kubernetes/config/kube-controller-manager.conf \u0026lt;\u0026lt;EOF KUBE_CONTROLLER_MANAGER_OPTS=\u0026quot;--logtostderr=false \\\\ --leader-elect=true \\\\ --address=0.0.0.0 \\\\ --service-cluster-ip-range=10.254.0.0/16 \\\\ --cluster-cidr=172.20.0.0/16 \\\\ --node-cidr-mask-size=24 \\\\ --cluster-name=kubernetes \\\\ --allocate-node-cidrs=true \\\\ --kubeconfig=/opt/kubernetes/config/kube-controller-manager.kubeconfig \\\\ --authentication-kubeconfig=/opt/kubernetes/config/kube-controller-manager.kubeconfig \\\\ --authorization-kubeconfig=/opt/kubernetes/config/kube-controller-manager.kubeconfig \\\\ --use-service-account-credentials=true \\\\ --client-ca-file=/opt/kubernetes/ssl/ca.pem \\\\ --requestheader-client-ca-file=/opt/kubernetes/ssl/ca.pem \\\\ --node-monitor-grace-period=40s \\\\ --node-monitor-period=5s \\\\ --pod-eviction-timeout=5m0s \\\\ --terminated-pod-gc-threshold=50 \\\\ --alsologtostderr=true \\\\ --cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\\\ --cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\ --deployment-controller-sync-period=10s \\\\ --experimental-cluster-signing-duration=86700h0m0s \\\\ --enable-garbage-collector=true \\\\ --root-ca-file=/opt/kubernetes/ssl/ca.pem \\\\ --service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\ --feature-gates=RotateKubeletServerCertificate=true,RotateKubeletClientCertificate=true \\\\ --controllers=*,bootstrapsigner,tokencleaner \\\\ --horizontal-pod-autoscaler-use-rest-clients=true \\\\ --horizontal-pod-autoscaler-sync-period=10s \\\\ --tls-cert-file=/opt/kubernetes/ssl/kube-controller-manager.pem \\\\ --tls-private-key-file=/opt/kubernetes/ssl/kube-controller-manager-key.pem \\\\ --kube-api-qps=100 \\\\ --kube-api-burst=100 \\\\ --log-dir=/var/log/kubernetes \\\\ --v=2\u0026quot; EOF 创建kube-scheduler配置文件 cat \u0026gt; /opt/kubernetes/config/kube-scheduler.conf \u0026lt;\u0026lt;EOF KUBE_SCHEDULER_OPTS=\u0026quot; \\\\ --logtostderr=false \\\\ --address=0.0.0.0 \\\\ --leader-elect=true \\\\ --kubeconfig=/opt/kubernetes/config/kube-scheduler.kubeconfig \\\\ --authentication-kubeconfig=/opt/kubernetes/config/kube-scheduler.kubeconfig \\\\ --authorization-kubeconfig=/opt/kubernetes/config/kube-scheduler.kubeconfig \\\\ --alsologtostderr=true \\\\ --kube-api-qps=100 \\\\ --kube-api-burst=100 \\\\ --log-dir=/var/log/kubernetes \\\\ --v=2\u0026quot; EOF 创建kubelet配置文件  在node节点上创建\n cat \u0026gt; /opt/kubernetes/config/kubelet.conf \u0026lt;\u0026lt;EOF KUBELET_OPTS=\u0026quot;--logtostderr=true \\\\ --v=4 \\\\ --network-plugin=cni \\\\ --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin \\\\ --hostname-override=10.1.10.129 \\\\ --kubeconfig=/opt/kubernetes/config/kubelet.kubeconfig \\\\ --bootstrap-kubeconfig=/opt/kubernetes/config/bootstrap.kubeconfig \\\\ --config=/opt/kubernetes/config/kubelet.yaml \\\\ --cert-dir=/opt/kubernetes/ssl \\\\ --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/rookieops/pause-amd64:3.0\u0026quot; EOF  address: 节点IP，不同节点需要更改\nnode-ip：节点IP，不同节点需要更改\nhostname-override：节点hostname，也可以配置节点IP，不同节点需要更改\nhealthz-bind-address：节点IP，不同节点需要更改\n\u0026ndash;hostname-override 在集群中显示的主机名，其他节点需要更改\n\u0026ndash;kubeconfig 指定kubeconfig文件位置，会自动生成\n\u0026ndash;bootstrap-kubeconfig 指定刚才生成的bootstrap.kubeconfig文件\n\u0026ndash;cert-dir 颁发证书存放位置\n\u0026ndash;pod-infra-container-image 管理Pod网络的镜像\n 创建kubelet.yaml配置文件\ncat \u0026gt; /opt/kubernetes/config/kubelet.yaml \u0026lt;\u0026lt;EOF kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 address: 10.1.10.129 port: 10250 readOnlyPort: 10255 cgroupDriver: cgroupfs clusterDNS: [\u0026quot;10.254.0.2\u0026quot;] clusterDomain: cluster.local. failSwapOn: false authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /opt/kubernetes/ssl/ca.pem authorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30s evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5% maxOpenFiles: 1000000 maxPods: 110 EOF  不同节点需要修改的地方为IP\n 创建kube-proxy配置文件 cat \u0026gt; /opt/kubernetes/config/kube-proxy.conf \u0026lt;\u0026lt;EOF KUBE_PROXY_OPTS=\u0026quot;--logtostderr=false \\\\ --v=2 \\\\ --feature-gates=SupportIPVSProxyMode=true \\\\ --masquerade-all=true \\\\ --proxy-mode=ipvs \\\\ --ipvs-min-sync-period=5s \\\\ --ipvs-sync-period=5s \\\\ --ipvs-scheduler=rr \\\\ --cluster-cidr=172.20.0.0/16 \\\\ --log-dir=/var/log/kubernetes \\\\ --kubeconfig=/opt/kubernetes/config/kube-proxy.kubeconfig\u0026quot; EOF 创建组件systemd启动文件 创建kube-apiserver启动文件 cat \u0026gt; /usr/lib/systemd/system/kube-apiserver.service \u0026lt;\u0026lt;EOF [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes [Service] EnvironmentFile=-/opt/kubernetes/config/kube-apiserver.conf ExecStart=/opt/kubernetes/bin/kube-apiserver \\$KUBE_APISERVER_OPTS Restart=on-failure RestartSec=10 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target EOF 创建kube-controller-manager启动文件 cat \u0026gt; /usr/lib/systemd/system/kube-controller-manager.service \u0026lt;\u0026lt;EOF [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/kubernetes/kubernetes [Service] EnvironmentFile=-/opt/kubernetes/config/kube-controller-manager.conf ExecStart=/opt/kubernetes/bin/kube-controller-manager \\$KUBE_CONTROLLER_MANAGER_OPTS Restart=on-failure [Install] WantedBy=multi-user.target EOF 创建kube-scheduler启动文件 cat \u0026gt; /usr/lib/systemd/system/kube-scheduler.service \u0026lt;\u0026lt;EOF [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/kubernetes/kubernetes [Service] EnvironmentFile=-/opt/kubernetes/config/kube-scheduler.conf ExecStart=/opt/kubernetes/bin/kube-scheduler \\$KUBE_SCHEDULER_OPTS Restart=on-failure [Install] WantedBy=multi-user.target EOF 创建kubelet启动文件  在需要部署的Node上创建\n cat \u0026gt; /usr/lib/systemd/system/kubelet.service \u0026lt;\u0026lt;EOF [Unit] Description=Kubernetes Kubelet After=docker.service Requires=docker.service [Service] EnvironmentFile=/opt/kubernetes/config/kubelet.conf ExecStart=/opt/kubernetes/bin/kubelet \\$KUBELET_OPTS Restart=on-failure KillMode=process [Install] WantedBy=multi-user.target EOF 创建kube-proxy启动文件  在需要部署的Node上创建\n cat \u0026gt; /usr/lib/systemd/system/kube-proxy.service \u0026lt;\u0026lt;EOF [Unit] Description=Kubernetes Proxy After=network.target [Service] EnvironmentFile=-/opt/kubernetes/config/kube-proxy.conf ExecStart=/opt/kubernetes/bin/kube-proxy \\$KUBE_PROXY_OPTS Restart=on-failure [Install] WantedBy=multi-user.target EOF 启动组件 master组件  由于我们master也准备当Node使用，所以我们将所有证书都拷贝到部署证书目录\n cp /root/kubernetes/ssl/kubernetes/*.pem /opt/kubernetes/ssl/ （1）、将我们创建的kubeconfig配置文件也拷贝到部署目录\ncp /root/kubernetes/kubeconfig/* /opt/kubernetes/config/ （2）、创建日志目录，并启动kube-apiserver\nmkdir /var/log/kubernetes systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable kube-apiserver \u0026amp;\u0026amp; systemctl start kube-apiserver （3）、复制kubeconfig文件到~/.kube/\nmv ~/.kube/config{,.old} cp /opt/kubernetes/config/admin.kubeconfig ~/.kube/config （4）、查看状态\nsystemctl status kube-apiserver # kubectl cluster-info Kubernetes master is running at https://10.1.10.128:6443 （5）、启动kube-controller-manager\nsystemctl daemon-reload \u0026amp;\u0026amp; systemctl enable kube-controller-manager \u0026amp;\u0026amp; systemctl start kube-controller-manager （6）、启动kube-scheduler\nsystemctl daemon-reload \u0026amp;\u0026amp; systemctl enable kube-scheduler \u0026amp;\u0026amp; systemctl start kube-scheduler （7）、查看集群状态\n# kubectl get cs -o=go-template='{{printf \u0026quot;|NAME|STATUS|MESSAGE|\\n\u0026quot;}}{{range .items}}{{$name := .metadata.name}}{{range .conditions}}{{printf \u0026quot;|%s|%s|%s|\\n\u0026quot; $name .status .message}}{{end}}{{end}}' |NAME|STATUS|MESSAGE| |scheduler|True|ok| |controller-manager|True|ok| |etcd-2|True|{\u0026quot;health\u0026quot;:\u0026quot;true\u0026quot;}| |etcd-0|True|{\u0026quot;health\u0026quot;:\u0026quot;true\u0026quot;}| |etcd-1|True|{\u0026quot;health\u0026quot;:\u0026quot;true\u0026quot;}| # kubectl get all --all-namespaces NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.254.0.1 \u0026lt;none\u0026gt; 443/TCP 8m26s （8）、授权访问kube-apiserver\n授予 kubernetes API 的权限 kubectl create clusterrolebinding controller-node-clusterrolebing --clusterrole=system:kube-controller-manager --user=system:kube-controller-manager kubectl create clusterrolebinding scheduler-node-clusterrolebing --clusterrole=system:kube-scheduler --user=system:kube-scheduler kubectl create clusterrolebinding controller-manager:system:auth-delegator --user system:kube-controller-manager --clusterrole system:auth-delegator 授予 kubernetes 证书访问 kubelet API 的权限 kubectl create clusterrolebinding --user system:serviceaccount:kube-system:default kube-system-cluster-admin --clusterrole cluster-admin kubectl create clusterrolebinding kubelet-node-clusterbinding --clusterrole=system:node --group=system:nodes kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes （9）、配置kubectl自动补全\nyum install -y bash-completion source /usr/share/bash-completion/bash_completion source \u0026lt;(kubectl completion bash) echo \u0026quot;source \u0026lt;(kubectl completion bash)\u0026quot; \u0026gt;\u0026gt; ~/.bashrc  如果要更改默认得namespace，可以使用如下命令\n kubectl config set-context --current --namespace={{namespace}} node组件  在master上部署bootstrap secret，脚本可以放置任意位置，我习惯放于/root/manifests下。另外TOKEN_ID和TOKEN_SECRET是我们在创建bootstrap kubeconfig生成的，在做那一步的时候以防万一应该记录下来。\n cat \u0026lt;\u0026lt; EOF | tee bootstrap.secret.yaml apiVersion: v1 kind: Secret metadata: # Name MUST be of form \u0026quot;bootstrap-token-\u0026lt;token id\u0026gt;\u0026quot; name: bootstrap-token-${TOKEN_ID} namespace: kube-system # Type MUST be 'bootstrap.kubernetes.io/token' type: bootstrap.kubernetes.io/token stringData: # Human readable description. Optional. description: \u0026quot;The default bootstrap token generated by 'kubelet '.\u0026quot; # Token ID and secret. Required. token-id: ${TOKEN_ID} token-secret: ${TOKEN_SECRET} # Allowed usages. usage-bootstrap-authentication: \u0026quot;true\u0026quot; usage-bootstrap-signing: \u0026quot;true\u0026quot; # Extra groups to authenticate the token as. Must start with \u0026quot;system:bootstrappers:\u0026quot; auth-extra-groups: system:bootstrappers:worker,system:bootstrappers:ingress --- # A ClusterRole which instructs the CSR approver to approve a node requesting a # serving cert matching its client cert. kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver rules: - apiGroups: [\u0026quot;certificates.k8s.io\u0026quot;] resources: [\u0026quot;certificatesigningrequests/selfnodeserver\u0026quot;] verbs: [\u0026quot;create\u0026quot;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026quot;true\u0026quot; labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kubernetes-to-kubelet rules: - apiGroups: - \u0026quot;\u0026quot; resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \u0026quot;*\u0026quot; --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:kubernetes namespace: \u0026quot;\u0026quot; roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kubernetes-to-kubelet subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kubernetes EOF  然后创建资源\n # 创建资源 kubectl create -f bootstrap.secret.yaml ### 查看创建的token kubeadm token list # 允许 system:bootstrappers 组用户创建 CSR 请求 kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers # 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求 kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers # 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求 kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes # 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求 kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes （1）、在Node节点创建我们需要的目录\nmkdir /opt/kubernetes/{bin,config,ssl} -p （2）、将node节点需要的二进制文件拷贝过去\ncd /root/kubernetes/install/kubernetes/server/bin scp kubelet kube-proxy 10.1.10.129:/opt/kubernetes/bin/ scp kubelet kube-proxy 10.1.10.130:/opt/kubernetes/bin/ （3）、将kubeconfig文件拷贝到Node节点上\ncd /root/kubernetes/kubeconfig scp * 10.1.10.129:/opt/kubernetes/config/ scp * 10.1.10.130:/opt/kubernetes/config/ （4）、将证书拷贝到Node节点上\n 只拷贝需要的，我这里仅仅是为了方便~~\n cd /root/kubernetes/ssl/kubernetes scp *.pem 10.1.10.129:/opt/kubernetes/ssl/ scp *.pem 10.1.10.130:/opt/kubernetes/ssl/ （5）、启动kubelet\nsystemctl daemon-reload \u0026amp;\u0026amp; systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet （6）、启动kube-proxy\nsystemctl daemon-reload \u0026amp;\u0026amp; systemctl enable kube-proxy \u0026amp;\u0026amp; systemctl start kube-proxy （7）、在master上查看\nkubectl get node NAME STATUS ROLES AGE VERSION node01-k8s NotReady \u0026lt;none\u0026gt; 72m v1.16.4 node02-k8s NotReady \u0026lt;none\u0026gt; 5m12s v1.16.4  之所以是NotReady，是因为我们还没有部署网络\n 安装组件 部署Flannel kubernetes提供一个CNI接口，它可以和任何支持CNI的网络插件对接，所以我们这里不直接部署Flannel，改成部署cni，然后将flannel部署在集群中。\n 使用CNI插件时，需要做三个配置：\n kubelet启动参数中networkPlugin设置为cni 在/etc/cni/net.d中增加cni的配置文件，配置文件中可以指定需要使用的cni组件及参数 将需要用到的cni组件（二进制可执行文件）放到/opt/cni/bin目录下   （1）、确保配置中开启了cni，如下\nKUBELET_OPTS=\u0026quot;--logtostderr=true \\ --v=4 \\ --network-plugin=cni \\ --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin \\ --hostname-override=10.1.10.128 \\ --kubeconfig=/opt/kubernetes/config/kubelet.kubeconfig \\ --bootstrap-kubeconfig=/opt/kubernetes/config/bootstrap.kubeconfig \\ --config=/opt/kubernetes/config/kubelet.config \\ --cert-dir=/opt/kubernetes/ssl \\ --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/rookieops/pause-amd64:3.0\u0026quot; （2）、下载cni文件\n下载地址：https://github.com/containernetworking/plugins/releases/download/v0.8.3/cni-plugins-linux-amd64-v0.8.3.tgz\n（3）、创建需要的目录\nmkdir /opt/cni/bin /etc/cni/net.d -p （4）、解压压缩包到安装目录/opt/cni/bin\ntar xf cni-plugins-linux-amd64-v0.8.3.tgz -C /opt/cni/bin/ （5）、将其拷贝到另外的节点\nscp -r /opt/cni/bin/* 10.1.10.129:/opt/cni/bin/ scp -r /opt/cni/bin/* 10.1.10.130:/opt/cni/bin/ （6）、配置kube-flannel YAML清单文件（kube-flannel.yaml）\n 下载地址：https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml\n --- apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: psp.flannel.unprivileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default spec: privileged: false volumes: - configMap - secret - emptyDir - hostPath allowedHostPaths: - pathPrefix: \u0026quot;/etc/cni/net.d\u0026quot; - pathPrefix: \u0026quot;/etc/kube-flannel\u0026quot; - pathPrefix: \u0026quot;/run/flannel\u0026quot; readOnlyRootFilesystem: false # Users and groups runAsUser: rule: RunAsAny supplementalGroups: rule: RunAsAny fsGroup: rule: RunAsAny # Privilege Escalation allowPrivilegeEscalation: false defaultAllowPrivilegeEscalation: false # Capabilities allowedCapabilities: ['NET_ADMIN'] defaultAddCapabilities: [] requiredDropCapabilities: [] # Host namespaces hostPID: false hostIPC: false hostNetwork: true hostPorts: - min: 0 max: 65535 # SELinux seLinux: # SELinux is unsed in CaaSP rule: 'RunAsAny' --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel rules: - apiGroups: ['extensions'] resources: ['podsecuritypolicies'] verbs: ['use'] resourceNames: ['psp.flannel.unprivileged'] - apiGroups: - \u0026quot;\u0026quot; resources: - pods verbs: - get - apiGroups: - \u0026quot;\u0026quot; resources: - nodes verbs: - list - watch - apiGroups: - \u0026quot;\u0026quot; resources: - nodes/status verbs: - patch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: flannel namespace: kube-system --- kind: ConfigMap apiVersion: v1 metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flannel data: cni-conf.json: | { \u0026quot;cniVersion\u0026quot;: \u0026quot;0.2.0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;cbr0\u0026quot;, \u0026quot;plugins\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;flannel\u0026quot;, \u0026quot;delegate\u0026quot;: { \u0026quot;hairpinMode\u0026quot;: true, \u0026quot;isDefaultGateway\u0026quot;: true } }, { \u0026quot;type\u0026quot;: \u0026quot;portmap\u0026quot;, \u0026quot;capabilities\u0026quot;: { \u0026quot;portMappings\u0026quot;: true } } ] } net-conf.json: | { \u0026quot;Network\u0026quot;: \u0026quot;172.20.0.0/16\u0026quot;, \u0026quot;Backend\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;vxlan\u0026quot; } } --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-flannel-ds-amd64 namespace: kube-system labels: tier: node app: flannel spec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/os operator: In values: - linux - key: beta.kubernetes.io/arch operator: In values: - amd64 hostNetwork: true tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: registry.cn-hangzhou.aliyuncs.com/rookieops/flannel:v0.11.0-amd64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: registry.cn-hangzhou.aliyuncs.com/rookieops/flannel:v0.11.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \u0026quot;100m\u0026quot; memory: \u0026quot;50Mi\u0026quot; limits: cpu: \u0026quot;100m\u0026quot; memory: \u0026quot;50Mi\u0026quot; securityContext: privileged: false capabilities: add: [\u0026quot;NET_ADMIN\u0026quot;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg （7）、生成资源清单\nkubectl apply -f kube-flannel.yaml （8）、查看集群状态\n# kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE kube-flannel-ds-amd64-2qkcb 1/1 Running 0 85s kube-flannel-ds-amd64-7nzj5 1/1 Running 0 85s # kubectl get node NAME STATUS ROLES AGE VERSION node01-k8s Ready \u0026lt;none\u0026gt; 104m v1.16.4 node02-k8s Ready \u0026lt;none\u0026gt; 37m v1.16.4  可以看到集群状态已经变为ready\n （9）、用一个demo文件测试一下\napiVersion: v1 kind: Pod metadata: name: pod-demo spec: containers: - name: test-ng image: nginx 查看是否能成功分配IP\n# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-flannel-ds-amd64-2qkcb 1/1 Running 0 5m36s 10.1.10.129 node01-k8s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-flannel-ds-amd64-7nzj5 1/1 Running 0 5m36s 10.1.10.130 node02-k8s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod-demo 1/1 Running 0 55s 172.20.1.2 node02-k8s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  测试正常\n 部署core dns YAML清单如下\napiVersion: v1 kind: ServiceAccount metadata: name: coredns namespace: kube-system labels: kubernetes.io/cluster-service: \u0026quot;true\u0026quot; addonmanager.kubernetes.io/mode: Reconcile --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: kubernetes.io/bootstrapping: rbac-defaults addonmanager.kubernetes.io/mode: Reconcile name: system:coredns rules: - apiGroups: - \u0026quot;\u0026quot; resources: - endpoints - services - pods - namespaces verbs: - list - watch - apiGroups: - \u0026quot;\u0026quot; resources: - nodes verbs: - get --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026quot;true\u0026quot; labels: kubernetes.io/bootstrapping: rbac-defaults addonmanager.kubernetes.io/mode: EnsureExists name: system:coredns roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:coredns subjects: - kind: ServiceAccount name: coredns namespace: kube-system --- apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system labels: addonmanager.kubernetes.io/mode: EnsureExists data: Corefile: | .:53 { errors health kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure upstream /etc/resolv.conf fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . /etc/resolv.conf cache 30 reload loadbalance } --- apiVersion: apps/v1 kind: Deployment metadata: name: coredns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: \u0026quot;true\u0026quot; addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: \u0026quot;CoreDNS\u0026quot; spec: # replicas: not specified here: # 1. In order to make Addon Manager do not reconcile this replicas parameter. # 2. Default is 1. # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on. strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 selector: matchLabels: k8s-app: kube-dns template: metadata: labels: k8s-app: kube-dns annotations: seccomp.security.alpha.kubernetes.io/pod: 'docker/default' spec: priorityClassName: system-cluster-critical serviceAccountName: coredns tolerations: - key: \u0026quot;CriticalAddonsOnly\u0026quot; operator: \u0026quot;Exists\u0026quot; nodeSelector: beta.kubernetes.io/os: linux containers: - name: coredns image: coredns/coredns imagePullPolicy: Always resources: limits: memory: 170Mi requests: cpu: 100m memory: 70Mi args: [ \u0026quot;-conf\u0026quot;, \u0026quot;/etc/coredns/Corefile\u0026quot; ] volumeMounts: - name: config-volume mountPath: /etc/coredns readOnly: true ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP livenessProbe: httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: httpGet: path: /health port: 8080 scheme: HTTP securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - all readOnlyRootFilesystem: true dnsPolicy: Default volumes: - name: config-volume configMap: name: coredns items: - key: Corefile path: Corefile --- apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system annotations: prometheus.io/port: \u0026quot;9153\u0026quot; prometheus.io/scrape: \u0026quot;true\u0026quot; labels: k8s-app: kube-dns kubernetes.io/cluster-service: \u0026quot;true\u0026quot; addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: \u0026quot;CoreDNS\u0026quot; spec: selector: k8s-app: kube-dns clusterIP: 10.254.0.2 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP - name: metrics port: 9153 protocol: TCP 测试：\n# 安装测试攻击 yum install bind-utils-y # 测试百度，要在Node节点测试，因为我们master没有安装网络 # dig @10.254.0.2 www.baidu.com ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.11.4-P2-RedHat-9.11.4-9.P2.el7 \u0026lt;\u0026lt;\u0026gt;\u0026gt; @10.254.0.2 www.baidu.com ; (1 server found) ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 24278 ;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;www.baidu.com. IN A ;; ANSWER SECTION: www.baidu.com. 30 IN CNAME www.a.shifen.com. www.a.shifen.com. 30 IN A 112.80.248.75 www.a.shifen.com. 30 IN A 112.80.248.76 ;; Query time: 54 msec ;; SERVER: 10.254.0.2#53(10.254.0.2) ;; WHEN: Sat Dec 28 23:40:43 CST 2019 ;; MSG SIZE rcvd: 149  返回解析正常\n 部署Traefik Ingress （1）、创建RBAC认证配置清单（traefik-rbac.yaml）\n--- apiVersion: v1 kind: ServiceAccount metadata: name: traefik-ingress-controller namespace: kube-system --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: traefik-ingress-controller rules: - apiGroups: - \u0026quot;\u0026quot; resources: - services - endpoints - secrets verbs: - get - list - watch - apiGroups: - extensions resources: - ingresses verbs: - get - list - watch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: traefik-ingress-controller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik-ingress-controller subjects: - kind: ServiceAccount name: traefik-ingress-controller namespace: kube-system （2）、创建traefik配置清单（traefik.yaml）\n--- kind: Deployment apiVersion: apps/v1 metadata: name: traefik-ingress-controller namespace: kube-system labels: k8s-app: traefik-ingress-lb spec: replicas: 1 selector: matchLabels: k8s-app: traefik-ingress-lb template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 60 # tolerations: # - operator: \u0026quot;Exists\u0026quot; # nodeSelector: # kubernetes.io/hostname: master containers: - image: traefik:v1.7.17 name: traefik-ingress-lb ports: - name: http containerPort: 80 - name: admin containerPort: 8080 args: - --api - --kubernetes - --logLevel=INFO --- kind: Service apiVersion: v1 metadata: name: traefik-ingress-service namespace: kube-system spec: selector: k8s-app: traefik-ingress-lb ports: - protocol: TCP port: 80 name: web nodePort: 38000 - protocol: TCP port: 8080 nodePort: 38080 name: admin type: NodePort （3）、创建配置清单\nkubectl apply -f traefik-rbac.yaml kubectl apply -g traefik.yaml （4）、查看结果\nkubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE coredns-9d5b6bdb6-mpwht 1/1 Running 0 22h kube-flannel-ds-amd64-2qkcb 1/1 Running 0 22h kube-flannel-ds-amd64-7nzj5 1/1 Running 0 22h pod-demo 1/1 Running 0 22h traefik-ingress-controller-7758594f89-lwf2t 1/1 Running 0 41s # kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.254.0.2 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 22h traefik-ingress-service NodePort 10.254.33.90 \u0026lt;none\u0026gt; 80:38000/TCP,8080:38080/TCP 3m33s 我们可以通过http://10.1.10.129:38080 来查看Dashboard，如下\n部署Dashboard （1）、部署，直接是官方部署文档\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml （2）、配置Ingress或者将service类型改为NodePort，我这里改为NodePort\nkubectl edit svc -n kubernetes-dashboard kubernetes-dashboard （3）、然后我们在浏览器访问\n# kubectl get svc -n kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE dashboard-metrics-scraper ClusterIP 10.254.224.240 \u0026lt;none\u0026gt; 8000/TCP 2m28s kubernetes-dashboard NodePort 10.254.82.50 \u0026lt;none\u0026gt; 443:28330/TCP 2m28s （4）、创建一个admin token\n# 创建sa kubectl create sa dashboard-admin -n kube-system # 授权 kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin # 获取token ADMIN_SECRET=$(kubectl get secrets -n kube-system | grep dashboard-admin | awk '{print $1}') # 获取dashboard kubeconfig使用token的值 DASHBOARD_LOGIN_TOKEN=$(kubectl describe secret -n kube-system ${ADMIN_SECRET} | grep -E '^token' | awk '{print $2}') echo ${DASHBOARD_LOGIN_TOKEN} （5）、创建dashboard kubeconfig\n 还是在我们统一的Kubeconfig目录下创建/root/kubernetes/kubeconfig\n # 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=../ssl/kubernetes/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=dashboard.kubeconfig # 设置客户端认证参数，使用上面创建的 Token kubectl config set-credentials dashboard_user \\ --token=${DASHBOARD_LOGIN_TOKEN} \\ --kubeconfig=dashboard.kubeconfig # 设置上下文参数 kubectl config set-context default \\ --cluster=kubernetes \\ --user=dashboard_user \\ --kubeconfig=dashboard.kubeconfig # 设置默认上下文 kubectl config use-context default --kubeconfig=dashboard.kubeconfig 然后下载dashboard.kubeconfig，在登录的时候上传即可进入主界面，如下\n部署Metrics Server  github：https://github.com/kubernetes-sigs/metrics-server\n稳定版：https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/metrics-server\n （1）、下载YAML清单\nfor file in auth-delegator.yaml auth-reader.yaml metrics-apiservice.yaml metrics-server-deployment.yaml metrics-server-service.yaml resource-reader.yaml;do wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/metrics-server/${file}; done （2）、修改metrics-server-deployment.yaml配置清单，如下\napiVersion: v1 kind: ServiceAccount metadata: name: metrics-server namespace: kube-system labels: kubernetes.io/cluster-service: \u0026quot;true\u0026quot; addonmanager.kubernetes.io/mode: Reconcile --- apiVersion: v1 kind: ConfigMap metadata: name: metrics-server-config namespace: kube-system labels: kubernetes.io/cluster-service: \u0026quot;true\u0026quot; addonmanager.kubernetes.io/mode: EnsureExists data: NannyConfiguration: |- apiVersion: nannyconfig/v1alpha1 kind: NannyConfiguration --- apiVersion: apps/v1 kind: Deployment metadata: name: metrics-server-v0.3.6 namespace: kube-system labels: k8s-app: metrics-server kubernetes.io/cluster-service: \u0026quot;true\u0026quot; addonmanager.kubernetes.io/mode: Reconcile version: v0.3.6 spec: selector: matchLabels: k8s-app: metrics-server version: v0.3.6 template: metadata: name: metrics-server labels: k8s-app: metrics-server version: v0.3.6 annotations: seccomp.security.alpha.kubernetes.io/pod: 'docker/default' spec: priorityClassName: system-cluster-critical serviceAccountName: metrics-server nodeSelector: kubernetes.io/os: linux containers: - name: metrics-server image: registry.cn-hangzhou.aliyuncs.com/rookieops/metrics-server-amd64:v0.3.6 command: - /metrics-server - --metric-resolution=30s - --kubelet-insecure-tls # These are needed for GKE, which doesn't support secure communication yet. # Remove these lines for non-GKE clusters, and when GKE supports token-based auth. # - --deprecated-kubelet-completely-insecure=true - --kubelet-port=10250 - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP ports: - containerPort: 443 name: https protocol: TCP - name: metrics-server-nanny image: registry.cn-hangzhou.aliyuncs.com/rookieops/addon-resizer:1.8.6 resources: limits: cpu: 100m memory: 300Mi requests: cpu: 100m memory: 300Mi env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: metrics-server-config-volume mountPath: /etc/config command: - /pod_nanny - --config-dir=/etc/config - --cpu=100m - --extra-cpu=0.5m - --memory=100Mi - --extra-memory=50Mi - --threshold=5 - --deployment=metrics-server-v0.3.6 - --container=metrics-server - --poll-period=300000 - --estimator=exponential # Specifies the smallest cluster (defined in number of nodes) # resources will be scaled to. # - --minClusterSize=2 volumes: - name: metrics-server-config-volume configMap: name: metrics-server-config （3）、修改resource-reader.yaml如下\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:metrics-server labels: kubernetes.io/cluster-service: \u0026quot;true\u0026quot; addonmanager.kubernetes.io/mode: Reconcile rules: - apiGroups: - \u0026quot;\u0026quot; resources: - pods - nodes - namespaces - nodes/stats verbs: - get - list - watch - apiGroups: - \u0026quot;apps\u0026quot; resources: - deployments verbs: - get - list - update - watch - apiGroups: - \u0026quot;extensions\u0026quot; resources: - deployments verbs: - get - list - update - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:metrics-server labels: kubernetes.io/cluster-service: \u0026quot;true\u0026quot; addonmanager.kubernetes.io/mode: Reconcile roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:metrics-server subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system （4）、然后创建 配置清单\nfor file in auth-delegator.yaml auth-reader.yaml metrics-apiservice.yaml metrics-server-deployment.yaml metrics-server-service.yaml resource-reader.yaml;do kubectl apply -f ${file};done （5）、查看\n# kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% master-k8s 195m 19% 1147Mi 66% node01-k8s 117m 11% 885Mi 51% node02-k8s 117m 11% 945Mi 54%  如果出现error: metrics not available yet，重启kubelet（至少我是这样）\n ","description":"纯二进制安装Kubernetes集群","id":3,"section":"posts","tags":["kubernetes","docker"],"title":"二进制安装Kubernetes集群","uri":"http://qiaokebaba.github.io/posts/binary-install-kubernetes/"},{"content":"乔克叔叔，运维一级建造师，擅长Linux、Kubernetes、Docker、Prometheus等。\n","description":"Everythig For Ops","id":4,"section":"","tags":null,"title":"About Me","uri":"http://qiaokebaba.github.io/about/"},{"content":"友情链接\n我的朋友们！\n","description":"My Friends","id":5,"section":"","tags":null,"title":"Friends","uri":"http://qiaokebaba.github.io/friends/"}]